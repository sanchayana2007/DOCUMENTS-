Python Basics:
================================================================================================================
code practice:
http://www.cse.msu.edu/~cse231/PracticeOfComputingUsingPython/

http://showmedo.com/learningpaths/12/view#id5  (BEST1)
http://www.saltycrane.com/blog/tag/python/

Installation:
+++++++++++++++++
C:\Python33

Edit the path and add the python in the path to launch it fromany place 

Python is an Interpretted language
++++++++++++++++++++++++++++++++++
Interpreted : A high level language run and executed by an Interpreter(a program which converts the high-level language to machine code and 
then executing) on the go; It processes the program a little at a time.

python is an interpreted language but it's not interpreted to the machine code, instead to some intermediate code(like byte-code or IL).
That bytecode is either interpreted (note that there's a difference, both in theory and in practical performance, between interpreting directly
 and first compiling to some intermediate representation and interpret that), as with the reference implementation (CPython),

Python and LUA are handled quite differently—interpreted rather than compiled. When you run a Python program 
(a file with an extension of .PY), it is not compiled, it is run. You could insert syntax errors into a function
in Python and unless that function is called, Python will never complain about the errors!
Process of interpreting :
--------------------------
1) There are four steps that python takes when you hit return: lexing, parsing, compiling, and interpreting.
2) Lexing is breaking the line of code you just typed into tokens.
3) The parser takes those tokens and generates a structure that shows their relationship to each other
 (in this case, an Abstract Syntax Tree). 
4) The compiler then takes the AST and turns it into one (or more) code objects.
Finally, the interpreter takes each code object executes the code it represents.



# Funny syntax error example

# Bad function!
def ErrorProne():

    printgobblegobble("Hello there!")

print("See, nothing bad happened. You worry too much!")

There is no function called printgobblegobble() in Python or in this program, so that should have generated an error! 
Here is the output:


See, nothing bad happened. You worry too much!


Python 3.x:
class MyClass(object): = new-style class
class MyClass: = new-style class (implicitly inherits from object)

Python 2.x:
class MyClass(object): = new-style class
class MyClass: = OLD-STYLE CLASS

python 3.x and 2.x
======================================
1. The print function
==================================================================================================================
Python 2
==================================================================================================================|
print 'Python', python_version()			|
print 'Hello, World!'
print('Hello, World!')
print "text", ; print 'print more text on the same line'
Python 2.7.6
Hello, World!
Hello, World!
text print more text on the same line
==================================================================================================================
==================================================================================================================
Python 3
==================================================================================================================|
print('Python', python_version())
print('Hello, World!')

print("some text,", end="")
print(' print more text on the same line')
Python 3.4.1
Hello, World!
some text, print more text on the same line
======================================
2. The Division
==================================================================================================================
Python 2
==================================================================================================================|
print '3 / 2 =', 3 / 2
3 / 2 = 1
==================================================================================================================
Python 3
==================================================================================================================
print '3 / 2 =', 3 / 2
3 / 2 = 1.5
3. Unicode
================================================================
Python 2 has ASCII str() types, separate unicode(), but no byte type.
Now, in Python 3, we finally have Unicode (utf-8) strings, and 2 byte classes: byte and bytearray
==================================================================================================================
Python 2
==================================================================================================================|
print type(unicode('this is like a python3 str type'))
<type 'unicode'>
print type(b'byte type does not exist')
<type 'str'>


==================================================================================================================
Python 3
==================================================================================================================|

print('Python', python_version())
print('strings are now utf-8 \u03BCnico\u0394é!')
Python 3.4.1
strings are now utf-8 μnicoΔé!
print('Python', python_version(), end="")
print(' has', type(b' bytes for storing data'))
Python 3.4.1 has <class 'bytes'>

4. xrange
================================================================
The usage of xrange() is very popular in Python 2.x for creating an iterable object, e.g., in a for-loop or list/set-dictionary-comprehension.\ The behavior was quite similar to a generator (i.e., "lazy evaluation"), but here the xrange-iterable is not exhaustible - meaning, you could iterate over it infinitely.

Thanks to its "lazy-evaluation", the advantage of the regular range() is that xrange() is generally faster if you have to iterate over it only once (e.g., in a for-loop). However, in contrast to 1-time iterations, it is not recommended if you repeat the iteration multiple times, since the generation happens every time from scratch!

In Python 3, the range() was implemented like the xrange() function so that a dedicated xrange() function does not exist anymore (xrange() raises a NameError in Python 3).

5. __contains__ 

 is that range got a "new" __contains__ method in Python 3.x (thanks to Yuchen Ying, who pointed this out). The __contains__ method can speedup "look-ups" in Python 3.x range significantly for integer and Boolean types.
================================================================
 "proofs" that the __contain__ method wasn't added to Python 2.x yet:

print('Python', python_version())
range.__contains__
Python 3.4.1
<slot wrapper '__contains__' of 'range' objects>
print 'Python', python_version()
range.__contains__
Python 2.7.7
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
 in ()
      1 print 'Python', python_version()
----> 2 range.__contains__

AttributeError: 'builtin_function_or_method' object has no attribute '__contains__'




Raising exceptions
--------------------------
Python 3.4.1
raise IOError, "file error"

raise IOError, "file error"
                     ^
SyntaxError: invalid syntax

raise IOError("file error")

Handling exceptions
======================
Also the handling of exceptions has slightly changed in Python 3. In Python 3 we have to use the "as" keyword now   
try:
    let_us_cause_a_NameError
except NameError as err:
    print(err, '--> our error message'
  next in generator

Only in 3.x 
next(my_generator) is ok 
my_generator.next() WILL RESULT in error 

Parsing user inputs via input()
==============================================
[back to top]
Fortunately, the input() function was fixed in Python 3 so that it always stores the user inputs as str objects. In order to avoid the dangerous behavior in Python 2 to read in other types than strings, we have to use raw_input() instead.











New style objects have a different object model to classic objects, and some things won't work properly with old style objects, for instance,
 super(), @property and descrpnt
Explanation:
Class :
-----------------------------
The classes and the derived thing 
> subclasing the built iin types
> Acessing methods from the super class
> Slots 
> Meta-programing


The __prepare__ method
======================================
This method is called before the class body is executed and it must return a dictionary-like object that's used as the local namespace for all the code from the class body. It was added in Python 3.0, see PEP-3115.

If your __prepare__ returns an object x then this:

class Class(metaclass=Meta):
    a = 1
    b = 2
    c = 3
Will make the following changes to x:

x['a'] = 1
x['b'] = 2
x['c'] = 3
This x object needs to look like a dictionary. Note that this x object will end up as an argument to Metaclass.__new__ and if it's not an instance of dict you need to convert it before calling super().__new__. [21]

So basically when an object is created 

1. Metaclass {__Call__} is called off (__call__ will call Class.__new__.)
2. __new__ (and if it returned an instance of Class it will also call Class.__init__ on it)
3. __init__


Creating a class
1. Metaclass.__prepare__ just returns the namespace object (a dictionary-like object
2. Metaclass.__new__ returns the Class object.


>subclasing the built in types :
======================================
And allowed programmers to subclass the built-in types such as list, tuple, or dict. So every time a class
that behaves almost like one of the built-in types needs to be implemented, the best
practice is to subtype it.

How to get the methods in a class :
 import inspect
 inspect.getmembers(OptionParser, predicate=inspect.ismethod)
[([('__init__', <unbound method OptionParser.__init__>),
   
 ('add_option', <unbound method OptionParser.add_option>),
 ('add_option_group', <unbound method OptionParser.add_option_group>),
 ('add_options', <unbound method OptionParser.add_options>),
 
All classes have built in methods and user defined functions
The dir(class/instance) will list it out , we can seee __call__ , __imfunc__ are auto written fpr amy class
'__call__', '__class__', '__delattr__', '__dict__', '__doc__', '__get__',
 '__getattribute__', '__hash__', '__init__', '__module__', '__name__',
 '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__',
 '__str__', 'func_closure', 'func_code', 'func_defaults', 'func_dict',
 'func_doc', 'func_globals', 'func_name']
 
Lets describe this Methods :
> __repr__() and __str__() methods
**************************************************************************************************
 • Generally, the str() method representation of an object is commonly 
expected to be more friendly to humans. This is built by an object's __str__() method.
• The repr() method representation is often going to be more technical,
perhaps even a complete Python expression to rebuild the object.
• The print() function will use str() to prepare an object for printing.
• The format() method of a string can also access these methods. When we
use {!r} or {!s} formatting, we're requesting __repr__() or __str__(),
respectively.
The following is a simple

collection with both __str__() and __repr__() methods:
class Hand:
	def __init__( self, dealer_card, *cards ):
		self.dealer_card= dealer_card
		self.cards= list(cards)
	def __str__( self ):
		return ", ".join( map(str, self.cards) )
	def __repr__( self ):
		return "{__class__.__name__}({dealer_card!r}, {_cards_str})".
	format(	__class__=self.__class__,	_cards_str=", ".join( map(repr, self.cards) ),**self.__dict__ )
The __str__() method is a simple recipe, as follows:
1. Map str() to each item in the collection. This will create an iterator over the
resulting string values.
2. Use ", ".join() to merge all the item strings into a single, long string.
difference between __str__ and __repr__ 
-----------------------------------------:
log(INFO, "I am in the weird function and a is", a, "and b is", b, "but I got a null C — using default", default_c)
But you have to do the last step — make sure every object you implement has a useful repr, so code like that can just work.
This is why the “eval” thing comes up: if you have enough information so eval(repr(c))==c, that means you know everything 
there is to know about c. If that’s easy enough, at least in a fuzzy way, do it. If not, make sure you have enough information about c
 anyway. I usually use an eval-like format: "MyClass(this=%r,that=%r)" % (self.this,self.that). It does not mean that you can actually 
 construct MyClass, or that those are the right constructor arguments — but it is a useful form to express “this is everything you need
 to know about this instance”.

Note: I used %r above, not %s. You always want to use repr() [or %r formatting character, equivalently] inside __repr__ 
implementation, or you’re defeating the goal of repr. You want to be able to differentiate MyClass(3) and MyClass("3").

The goal of __str__ is to be readable

Specifically, it is not intended to be unambiguous — notice that str(3)==str("3"). Likewise, if you implement an IP abstraction, having the 

str of it look like 192.168.1.1 is just fine. When implementing a date/time abstraction, the str can be "2010/4/12 15:35:22", etc. 
The goal is to represent it in a way that a user, not a programmer, would want to read it. Chop off useless digits, pretend to be some other class
 — as long is it supports readability, it is an improvement


> __format__()
**************************************************************************************************
The __format__() method is used by string.format() as well as the format()
built-in function. Both of these interfaces are used to get presentable string versions
of a given object.
The following are the two ways in which arguments will be presented
to __format__():
• someobject.__format__(""): This happens when the application
does format(someobject) or something equivalent to "{0}".
format(someobject). In these cases, a zero-length string specification was
provided. This should produce a default format.
• someobject.__format__(specification): This happens when the
application does format(someobject, specification) or something
equivalent to "{0:specification}".format(someobject).

__hash__()
**************************************************************************************************
 The built-in hash() function invokes the __hash__() method of a given object.
This hash is a calculation which reduces a (potentially complex) value to a small
integer value. Ideally, a hash reflects all the bits of the source value. Other hash
calculations—often used for cryptographic purposes—can produce very large values.

Deciding what to hash
Not every object should provide a hash value. Specifically, if we're creating a class of
stateful, mutable objects, the class should never return a hash value. The definition of
__hash__ should be None.
Immutable objects, on the other hand, might sensibly return a hash value so that
the object can be used as the key in a dictionary or a member of a set. In this case,
the hash value needs to parallel the way the test for equality works. It's bad to have
objects that claim to be equal and have different hash values. The reverse—objects
with the same hash that are actually not equal—is acceptable

There are three use cases for defining equality tests and hash values via the
__eq__() and __hash__() method functions:

Mutable objects: These are stateful objects that can be modified internally.
We have one design choice:
°° Define __eq__() but set __hash__ to None. These cannot be used as
dict keys or items in sets.

__bool__()
**************************************************************************************************
if some_object:
process( some_object )
Under the hood, this is the job of the bool() built-in function. This function depends
on the __bool__() method of a given object.
The default __bool__() method returns True. We can see this with the
following code:
 x = object()
 bool(x)
True


__bytes__()
**************************************************************************************************
In the most common situation, an application can create a string representation, and
the built-in encoding capabilities of the Python IO classes will be used to transform
the string into bytes.

 comparison operator methods
**************************************************************************************************
• x<y calls x.__lt__(y)
• x<=y calls x.__le__(y)
• x==y calls x.__eq__(y)
• x!=y calls x.__ne__(y)
• x>y calls x.__gt__(y)
• x>=y calls x.__ge__(y)

for example a Implementation of comparison for objects of
the same class
We'll look at a simple same-class comparison by looking at a more complete

BlackJackCard class:
class BlackJackCard:

def __init__( self, rank, suit, hard, soft ):
	self.rank= rank
	self.suit= suit
	self.hard= hard
	self.soft= soft
	
def __lt__( self, other ):
	if not isinstance( other, BlackJackCard ): 
		return NotImplemented
	return self.rank < other.rank
	
def __le__( self, other ):
	try:
		return self.rank <= other.rank
	except AttributeError:
		return NotImplemented
		
def __gt__( self, other ):
	for objects
	
Now lets try the same thing of mixed classes
We'll use the BlackJackCard class as an example to see what happens when we
attempt comparisons where the two operands are from different classes.
The following is a Card instance that we can compare against the int values:
 two = card21( 2, '♣' )
 two < 2
Traceback (most recent call last):
File "<stdin>", line 1, in <module>
TypeError: unorderable types: Number21Card() < int()


**************************************************************************************************

For a function dir output gives the same as above 
The main use is to  inherit the base class for our use suppose the dict is inherited for inheritance 
and update it for a Distict key

class DistinctDict(dict):
	def __setitem__(self,key,value):
		try:
			value_index = self.values.index(value)
			existing_key = self.keys()[value_index]
			if existing_key != key:
				raise DistictError("")
		except valueError:
			pass
		super(distinctdict, self).__setitem__(key, value)
		
 my = distinctdict()
 my['key'] = 'value'
 my['other_key'] = 'value'

Super :
=====================
The super is called by the sub classes to update the  base class calls 
class Mama(object):
	def says(self):
		"please Go and read"
class Sister(Mama):
	def says(self):
		super(Sister,self).says()
		
This shows thw usw of te supewr class 

But there are several places when super cannot and should not be used 
For diamond inheritancs 

class A:
 mtd m
 
class B(A):
	pass 

class C(A):
	mtd m
 
class D(B,C):

d=D  
d.m # calls the Cmthd


Explanation:
-------------------------------------
The crucial difference between resolution order for legacy vs new-style classes comes when the same ancestor class occurs more
than once in the "naive", depth-first approach -- e.g., consider a "diamond inheritance" case:

>>> class A: x = 'a'
... 
>>> class B(A): pass
... 
>>> class C(A): x = 'c'
... 
>>> class D(B, C): pass
... 
>>> D.x
'a'
here, legacy-style, the resolution order is D - B - A - C - A : so when looking up D.x, A is the first base in resolution order to solve it, 
thereby hiding the definition in C. 

While:

>>> class A(object): x = 'a'
... 
>>> class B(A): pass
... 
>>> class C(A): x = 'c'
... 
>>> class D(B, C): pass
... 
>>> D.x
'c'
>>> 
here, new-style, the order is:

>>> D.__mro__
(<class '__main__.D'>, <class '__main__.B'>, <class '__main__.C'>, 
    <class '__main__.A'>, <type 'object'>)

with A forced to come in resolution order only once and after all of its subclasses, so that overrides
(i.e., C's override of member x) actually work sensibly.

It's one of the reasons that old-style classes should be avoided: multiple inheritance with "diamond-like" patterns just
doesn't work sensibly with them, while it does with new-style


Heterogeneous Arguments
Another issue with super usage is argument passing in initialization. How can a
class call its base class __init__ code if it doesn't have the same signature? This
leads to the following problem:
 class BaseBase(object):
    def __init__(self):
    print 'basebase'
    super(BaseBase, self).__init__()
   
 class Base1(BaseBase):
    def __init__(self):
    print 'base1'
    super(Base1, self).__init__()
   
 class Base2(BaseBase):
    def __init__(self, arg):
    print 'base2'
    super(Base2, self).__init__()
   
 class MyClass(Base1 , Base2):
    def __init__(self, arg):
    print 'my base'
    super(MyClass, self).__init__(arg)
   
m = MyClass(10)
my base
Traceback (most recent call last):
File "<stdin>", line 1, in <module>
File "<stdin>", line 4, in __init__
TypeError: __init__() takes exactly 1 argument (2 given)


Heterogeneous Arguments
Another issue with super usage is argument passing in initialization. How can a
class call its base class __init__ code if it doesn't have the same signature? This
leads to the following problem:
 class BaseBase(object):
 def __init__(self):
	 print 'basebase'
	super(BaseBase, self).__init__()

 class Base1(BaseBase):
 def __init__(self):
	 print 'base1'
	super(Base1, self).__init__()

 class Base2(BaseBase):
 def __init__(self, arg):
 	print 'base2'
 	super(Base2, self).__init__()

 class MyClass(Base1 , Base2):
def __init__(self, arg):
 	print 'my base'
 	super(MyClass, self).__init__(arg)
.
 m = MyClass(10)
my base
Traceback (most recent call last):
File "<stdin>", line 1, in <module>
File "<stdin>", line 4, in __init__
TypeError: __init__() takes exactly 1 argument (2 given)
One solution would be to use *args and **kw magic, so that all constructors pass
along all the parameters even if they do not use them:
 class BaseBase(object):
    def __init__(self, *args, **kw):
    print 'basebase'
    super(BaseBase, self).__init__(*args, **kw)
   
 class Base1(BaseBase):
    def

getattr function :
+++++++++++++++++++++++++++++++++++++=+++++++
>>> class Person():
    	name = 'Victor'
    	def say(self, what):
       		print(self.name, what)

>>> getattr(Person, 'name')
'Victor'
>>> attr_name = 'name'
>>> person = Person()
>>> getattr(person, attr_name)
'Victor'
>>> getattr(person, 'say')('Hello')
Victor Hello

difference between __getattr__ and __getattribute__ 
----------------------------------------------------------
A key difference between __getattr__ and __getattribute__ is that __getattr__ is only invoked if the attribute wasn't found the usual ways. It's good for implementing a fallback for missing attributes, and is probably the one of two you want.

__getattribute__ is invoked before looking at the actual attributes on the object, and so can be tricky to implement correctly. You can end up in infinite recursions very easily.
class Foo(object):
    def __getattr__(self, attr):
        print "looking up", attr
        value = 42
        self.__dict__[attr] = value
        return value

f = Foo()
print f.x 
#output >>> looking up x 42

f.x = 3
print f.x 
#output >>> 3
---------------------------------------------------------
Order of referring the methods :

There are many ways to access/change the value of self.foo:

1) direct access a.foo
2) inner dict a.__dict__['foo']
3) get and set a.__get__ and a.__set__,of course there two are pre-defined methods.
4) getattribute a.__getattribute__
5) __getattr__ and __setattr__

Class attribute lookup:
---------------------------------------------

class Test(object):
	self.foo
	
	
The match will happen in the following order 
   		(Metaclass:__getatribute__('foo'))
                           |
            (yes)  ---  <foo in metaclass dict>  ---  (no)
              |                         		 |
              |			   <foo in class dict>		
        metaclass dict        (no)			(yes)
        has set n get           |                        |
              |              
              |                 |                        |
              |              <foo in metaclass dict>     foo in class dict has get
              |			|			yes			no
              |		Raise exception			|		 	|
	return metaclas					
	dict foo bar				return dict_foo_get		return Metaclass:__dict_foo

Magic method lookup
---------------------------------------------
For magic methods the lookup is done on the class, directly in the big struct with the slots: [12]

Does the object's class have a slot for that magic method (roughly object->ob_type->tp_<magicmethod> in C code)? If yes, use it. If it's NULL then the operation is not supported.





OOPS in python :
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Here is an example:


class Bug(object):
    legs = 0
    distance = 0

    def __init__(self, name, legs):
        self.name = name
        self.legs = legs

    def Walk(self):
        self.distance += 1

    def ToString(self):
        return self.name + " has " + str(self.legs) + " legs" + \
            " and taken " + str(self.distance) + " steps."

-----------------------------------------------------------------
Polymorphism:
The term polymorph means “many forms” or “many shapes”, so polymorphism is the ability to take many forms or shapes.
In the context of a class, this means we can use methods with many shapes—that is, many different sets of parameters.
In Python, we can use optional parameters to make a method more versatile. The constructor of our new Bug class can 
be transformed with the use of optional parameters like so:


    def __init__(self, name="Bug", legs=6):
        self.name = name
        self.legs = legs


--------------------------------------------------------------------------------------
Data Hiding (Encapsulation):

Private Variables:
class P:
   def __init__(self, name, alias):
      self.name = name       # public
      self.__alias = alias   # private

   def who(self):
      print('name  : ', self.name)
      print('alias : ', self.__alias)

>>> x = P()	  
>>> x.name
'Alex'
>>> x.alias
Traceback (most recent call last):
  File "", line 1, in 
AttributeError: P instance has no attribute 'alias'
	  
x.__alias
Traceback (most recent call last):
  File "", line 1, in 
AttributeError: P instance has no attribute '__alias'

But here is a magic want. One underscore('_') with the class name will do magic:

>>> x._P__alias
'amen'

access or change the distance variable (which we would assume is private, even though it isn’t):


    def GetDistance(self):
       return p_distance

    def SetDistance(self, value):
        p_distance = value

From a data hiding point of view, you could rename distance to p_distance (making it appear to be a private variable), 
and then access it using these two methods. That is, if data hiding is important in your program.

Implementing a Private Function (Workaround method)
-------------------------------------------------------------
import re
import inspect

class MyClass :

    def __init__(self) :
        pass

    def private_function ( self ) :
        try :
            function_call = inspect.stack()[1][4][0].strip()

            # See if the function_call has "self." in the begining
            # For the function called from the inside the class = self.private-function() 
            # if the function is called outside the class wd an object then it will be obj.private-function() 
            matched = re.match( '^self\.', function_call )
            if not matched :
                print 'This is Private Function, Go Away'
                return
        except :
            print 'This is Private Function, Go Away'
            return

        # This is the real Function, only accessible inside class #
        print 'Hey, Welcome in to function'

    def public_function ( self ) :
        # i can call private function from inside the class
        self.private_function()

### End ###
--------------------------------------------------------------------------------------
Inheritance:

Python supports inheritance of base classes. When a class is defined, the base class is included in parentheses:


class Car(Vehicle):

In addition, Python supports multiple inheritance; that is, more than one parent or base class can be inherited from
in a child class. For example:


class Car(Body,Engine,Suspension,Interior):

As long as the variables and methods in each parent class do not conflict with each other, the new child class can 
access them all without incident. But, if there are any conflicts, the conflicted variable or method is used from the 
parent that comes first in the inheritance ordering.


Use of super() to resolve any confuiosns in polymorphism
class Point():
    x = 0.0
    y = 0.0

    def __init__(self, x, y):
        self.x = x
        self.y = y
        print("Point constructor")



class Circle(Point):
    radius = 0.0

    def __init__(self, x, y, radius):
        super().__init__(x,y)
        self.radius = radius
 
 c = Circle(100,100,50)
 
 Output :
 
 Point constructor
Circle constructor
--------------------------------------------------------------------------------------
Polymorphism:

overloading :
1) Operator overlaoding 

class Book:
    title = ''
    pages = 0

    def __init__(self, title='', pages=0):
        self.title = title
        self.pages = pages

    def __str__(self):
        return self.title
        
      
   a1 = book(Mybook1,100)
   a2 = book(Mybook2,100)
   
   Mybook1 + Mybook2 = 200 
   
   + implementation:
   ====================
    So if the first item doesn’t know how to add itself to 0, Python fails. But before it fails, Python tries to do a revered add with the operators.
    
    def __add__(self, other):
    return self.pages + other.pages
    
    We have to override __radd__, or “reverse add”.

def __radd__(self, other):
    return self.pages + other.pages
    
    Hash overideing for making a object as dictionary key :
    =================================================================================================================
   class MyThing:
    def __init__(self,name,location,length):
        self.name = name
        self.location = location
        self.length = length

    def __hash__(self):
        return hash((self.name, self.location))

    def __eq__(self, other):
        return (self.name, self.location) == (other.name, other.location) 
    
    
    
hashable
An object is hashable if it has a hash value which never changes during its lifetime (it needs a __hash__() method), and can be compared to other 
objects (it needs an __eq__() method). Hashable objects which compare equal must have the same hash value.

Hashability makes an object usable as a dictionary key and a set member, because these data structures use the hash value internally.

All of Python’s immutable built-in objects are hashable, while no mutable containers (such as lists or dictionaries) are. Objects which are instances
of user-defined classes are hashable by default; they all compare unequal (except with themselves), and their hash value is derived from their id().

--------------------------------------------------------------------------------------
Object declaration :

A = Classname(22,22)

Calling the function is same as C++ 

A.fumc("dewded",12)

--------------------------------------------------------------------------------------

 Descriptors and properties:
===========================
For python as there is no private keyword in the classes the name managling comes handy 
Suppose we have a class with an atribute vale 

class Myclass(object):
	secret_value = 2
	
>> dir(MyClass)
['_MyClass__secret_value','__class__', '__delattr__',]
>> instance_of._MyClass__secret_value

This is provided to avoid name collision under inheritance, as the attribute is
renamed with the class name as a prefix. It is not a real lock, since the attribute can
be accessed through its composed name. This feature could be used to protect
the access of some attributes, but in practice, "__" should never be used.

_ in private 
----------------------------
When an attribute is not public, the convention to use is a "_" prefix. This does not call any
mangling algorithm, but just documents the attribute as being a private element of
the class and is the prevailing style.

Descriptors:
=======================
A descriptor lets you customize what should be done when you refer to an attribute
on an object. 
The descriptor classes are based on three special methods they have to implement:
1) __set__: This is called whenever the attribute is set
2) __get__: This is called whenever the attribute is read
3) __delete__: This is called when del is invoked on the attribute.

Lets Implenment each one of them 
class A(object):
# lets have a class with getter , setters 
    def __init__(self):
        self.value = ''

    def __get__(self, instance, owner):
        return self.value

    def __set__(self, instance, value):
         self.value = value

# Now we will use this class in another class as an Instance 
class Myclass(object):
    atr = A()

obj = Myclass()
#Set some value using the instance it will call the setter associated with the instance
obj.atr = '1st string'
# This will call the get method of the class
print (obj.atr)
>> '1st string'
# Add a new atribute on the fly with the instance
obj.new_atr = '2nd string'
print (obj.__dict__)
>> {'new_atr': '2nd string'}   # this will update the dict with new value
Another way to add an atribute 
obj.__dict__['new_atr'] = 'New Str'
print (obj.__dict__)
>>{'new_ano_atr': 'New Ano  Str', 'new_atr': 'New Str'}


Properties:
=======================


class Parrot(object):
    def __init__(self):
        self._voltage = 100000

    @property
    def voltage(self):
        """Get the current voltage."""
        return self._voltage
        
A property object has getter, setter, and deleter methods usable as decorators that create a copy of the property with the
 corresponding accessor function set to the decorated function
class C(object):
    def __init__(self):
        self._x = None

    @property
    def x(self):
        """I'm the 'x' property."""
        return self._x

    @x.setter
    def x(self, value):
        self._x = value

    @x.deleter
    def x(self):
        del self._x
        
 Take a look at two basic design patterns for properties       
• Eager calculation: In this design pattern, when we set a value via a property,
other attributes are also computed
• Lazy calculation: In this design
common features of the Hand object into an abstract superclass, as follows:
class Hand:
	def __str__( self ):
		return ", ".join( map(str, self.card) )
		
	def __repr__( self ):
		return "{__class__.__name__}({dealer_card!r}, {_cards_str})".
			format(__class__=self.__class__,_cards_str=", ".join( map(repr, self.card) ),**self.__dict__ )
			
In the preceding code, we defined just some string representation methods and
nothing else.
The following is a subclass of Hand, where total is a lazy property that is computed
only when needed:

class Hand_Lazy(Hand):
	def __init__( self, dealer_card, *cards ):
		self.dealer_card= dealer_card
		self._cards= list(cards)
	@property
	def total( self ):
		delta_soft = max(c.soft-c.hard for c in self._cards)
		hard_total = sum(c.hard for c in self._cards)
			if hard_total+delta_soft <= 21: 
				return hard_total+delta_soft
		return hard_total
		
	@property
	def card( self ):
		return self._cards
		
	@card.setter
	def card( self, aCard ):
		self._cards.append( aCard )
		
	@card.deleter
	def card( self ):
		self._cards.pop(-1)
	
The Hand_Lazy class initializes a Hand object with a list of the Cards object.
The total property is a method that computes the total only when requested.

Eagerly computed properties:
 class Hand_Eager(Hand):
	def __init__( self, dealer_card, *cards ):
		self.dealer_card= dealer_card
		self.total= 0
		self._delta_soft= 0
		self._hard_total= 0
		self._cards= list()
		for c in cards:
			self.card = c
			
	@property
	def card( self ):
		return self._cards
	@card.setter
	def card( self, aCard ):
		self._cards.append(aCard)
		self._delta_soft = max(aCard.soft-aCard.hard,self._delta_soft)
		self._hard_total += aCard.hard
		self._set_total()
	@card.deleter
	def card( self ):
		removed= self._cards.pop(-1)
		self._hard_total -= removed.hard
		# Issue: was this the only ace?
		self._delta_soft = max( c.soft-c.hard for c in self._cards)
		self._set_total()
	def _set_total( self ):
		if self._hard_total+self._delta_soft <= 21:
			self.total= self._hard_total+self._delta_soft
		else:
			self.total= self._hard_total
In this case, each time a card is added, the total attribute is updated.
		
Introspection Descriptor:
==============================
The introspection Descriptor is needed when a class is doing introspection over tehir atribute 
A property API class that render a redible documentation over the public methods of 
__author__ = 'Sanchayan'

class API(object):
    def print_values(self,obj):
        def _print_values(mtdname):
            # Rejects all the  __XX__  mthds
            if mtdname.startswith('_'):
                return ''
            val = getattr(obj,mtdname)
            doc = val.__doc__
         #   print(doc)
            return '%s:%s'%(mtdname,doc)
        res = [_print_values(ent) for ent in dir(obj)]
        #print(res)
        return '\n'.join([ent for ent in res if ent != ''])

    def __get__(self, instance, owner):
        #Get gets the call from API
        if instance is not None:
            print('instnce')
            self.print_values(instance)
        else:
            print(self.print_values(owner))



class Myclass(object):
    # doc set as object of API
    __doc__ = API()

    def __init__(self):
        self.a =2

    def addby1(self):
        """Method is add by 1"""
        return self.a +1
    def addby2(self):
        """Method is add by 2"""
        return self.a +2

if __name__ == '__main__':
    #Lets call the Doc of the class
    Myclass.__doc__
   
>>
   addby1:Method is add by 1
addby2:Method is add by 2
	
	
Meta descriptor :
======================
perform a task.This can be useful, To lower the quantity of code needed to use a class
that provides steps.

Slots:
========================
It allows you to set a static attribute list for a given class with the __slots__ attribute,
and skip the creation of the __dict__ list in each instance of the class.
> If you intend to save memory space for classes with very atributes
> Design classes whose signature needs to be frozen

class Frozen(object):
	__slots__=['ice','cream']
	
>> a = Frozen()
>> a.__dir__ # not found 

Meta-programming:
=====================================================
The new-style classes brought the ability to change classes' and objects' definitions
on the fly, through two special methods: __new__ and __metaclass__.

Meta programing 
=============================

Meta-programming
The new-style classes brought the ability to change classes' and objects' definitions
on the fly, through two special methods: __new__ and __metaclass__.

> The  __new__ Method
------------------------
the new method will be invoked when the class is instantiated 
class MyClass(object):
	def __new__(cls):
		print '_new_called'
		return object.__new__(cls)## This _new_ mth will returm a object.new(Classname)
	##This will asign the memory for the class 
	
	
	def __init__(self):
		print "init mthd"
		self.a =1 
	#The intialisation will be taken care by the init methods 
	
	
>> a = MyClass()
_new_called'
init mthd
** So new can be used to make changes to class before the object /after object creation is taking place 
Now lets inherit the class 
class Newclass(Myclass0:
	pass 

a= Newclass()
_new_called'
init mthd
My other  class 
	

Uses:  Network socket or database initializations, for instance, should be controlled in
__new__ rather than in __init__. It tells us when the initialization must be done for
the class to work and when it might be derived.

__metaclass__ Method:
=================================
Metaclasses give the ability to interact when a class object is created in memory
through its factory. They act like __new__ but at the class level. The built-in type
type is the built-in base factory.


Creating classes dynamically
***********************************************************
Since classes are objects, you can create them on the fly, like any object.

First, you can create a class in a function using class:

   def choose_class(name):
          if name == 'foo':
              class Foo(object):
                  pass
              return Foo # return the class, not an instance
          else:
              class Bar(object):
                  pass
              return Bar
          
   MyClass = choose_class('foo') 
   print(MyClass) # the function returns a class, not an instance
  <class '__main__.Foo'>
   print(MyClass()) # you can create an object from this class
  <__main__.Foo object at 0x89c6d4c>

Use of type in class:
****************************************
 print(type(ObjectCreator))
<type 'type'>
 print(type(ObjectCreator()))
<class '__main__.ObjectCreator'>

type works this way:

  type(name of the class, 
       tuple of the parent class (for inheritance, can be empty), 
       dictionary containing attributes names and values)

	   
	   
	   MyShinyClass = type('MyShinyClass', (), {}) # returns a class object
	  

What are metaclasses?
************************
Metaclasses are the 'stuff' that creates classes.
metaclasses are what create these objects. They are the classes' classes, you can picture them this way:

  MyClass = MetaClass()
  MyObject = MyClass()
You've seen that type lets you do something like this:

  MyClass = type('MyClass', (), {})
It's because the function type is in fact a metaclass. type is the metaclass Python uses to create all classes behind the scenes.

__class___ Atribute :
**************************
Class gives the class name of the type is followed up with 
name = 'bob'
   name.__class__
  <type 'str'>
 
c = Foo()
c.__class__
>><class '__main__.Foo'>


now what is __class__.__class__ ie class of class which is metaclass
c.__class__.__class__
>><type 'type'>
  
The __metaclass__ attribute:
***************************************
You can add a __metaclass__ attribute when you write a class:

class Foo(object):
  __metaclass__ = something   
  [...]
If you do so, Python will use the metaclass to create the class Foo.

Careful, it's tricky.

You write class Foo(object) first, but the class object Foo is not created in memory yet.
Python will look for __metaclass__ in the class definition. If it finds it, it will use it 
to create the object class Foo. If it doesn't, it will use type to create the class.

Why the hell would you use metaclasses?
Now the big question. Why would you use some obscure error prone feature?

Well, usually you don't:

Metaclasses are deeper magic than 99% of users should ever worry about. If you wonder whether you need them, you don't
 (the people who actually need them know with certainty that they need them, and don't need an explanation about why).
It allows you to define something like this:

  class Person(models.Model):
    name = models.CharField(max_length=30)
    age = models.IntegerField()
But if you do this:

  guy = Person(name='bob', age='35')
  print(guy.age)
It won't return an IntegerField object. It will return an int, and can even take it directly from the database.

This is possible because models.Model defines __metaclass__ and it uses some magic that will turn the Person you 
just defined with simple statements into a complex hook to a database field.

Django makes something complex look simple by exposing a simple API and using metaclasses, recreating code from
 this API to do the real job behind the scenes.
 


variables and values are REFERENCES
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Everything is  reference of an Object except the variables user declares
f you're used to most traditional languages, you have a mental model of what happens in the following sequence:

a = 1
a = 2
You believe that a is a memory location that stores the value 1, then is updated to store the value 2. 
That's not how things work in Python. Rather, a starts as a reference to an object with the value 1,
then gets reassigned as a reference to an object with the value 2. Those two objects may continue to 
coexist even though a doesn't refer to the first one anymore; in fact they may be shared by any number
of other references within the program.

Value passing in a function is non mutable :
---------------------------------------------
When you call a function with a parameter, a new reference is created that refers to the object passed in. 
This is separate from the reference that was used in the function call, so there's no way to update that reference
 and make it refer to a new object. In your example:

    self.variable = 'Original'
    self.Change(self.variable)

def Change(self, var):
    var = 'Changed'
self.variable is a reference to the string object 'Original'. When you call Change you create a second reference
 var to the object. Inside the function you reassign the reference var to a different string object 'Changed',
 but the reference self.variable is separate and does not change.
 
 What is Happening here in terms of C++ terminology is
 ****************************************
 int& xRef = x;
	int& zRef = z;
	zRef = xRef; // Assigns values, not references
 ***************************************
 So over here var and 'changed' are two seperate references and now var value is equal to the 'changed' reference
 value.
 
The only way around this is to pass a mutable object. Because both references refer to the same object, any 
changes to the object are reflected in both places.

    self.variable = ['Original']
    self.Change(self.variable)

def Change(self, var):
    var[0] = 'Changed'

Now in this condition as var is a reference we are assigning the addresesss refered by var[0] to 'Changed' value 
So this willl refelect on self.variable also


Python Associations:
=================================================================================================================

The associations for python are 
1) tuple :
------------------
 It is a Immutable List and its fast in retriving and all it can be termed as an array
 A tuple is a sequence of immutable Python objects. Tuples are sequences, just like lists. The only difference is that 
 
 1) tuples can't be changed i.e., tuples are immutable 
 2) tuples use parentheses and lists use square brackets.

 tup1 = ('physics', 'chemistry', 1997, 2000);
 del tup; // delete tuples
	Empty and single tuples :
============================
	empty tuple is written as two parentheses containing nothing:
	tup1 = ();

To write a tuple containing a single value you have to include a comma, even though there is only one value:
	tup1 = (50,);
Like string indices, tuple indices start at 0, and tuples can be sliced, concatenated and so o
 
 
Updating Tuples: 
****************
# So let's create a new tuple as follows if you wana add something to tup1 make tup2 and concatenate in tup3
	tup3 = tup1 + tup2;
		print tup3;
 
Because tuples are sequences, indexing and slicing work the same way for tuples as they do for strings. Assuming following input:

	L = ('spam', 'Spam', 'SPAM!')
 

Python Expression	Results	 Description
	L[2]	'SPAM!'	Offsets start at zero
	L[-2]	'Spam'	Negative: count from the right
	L[1:]	['Spam', 'SPAM!']	Slicing fetches sections

 
2)List : Comprehension

---------------
The list is a most versatile datatype available in Python  Good thing about a list is that items in a list need not all have the same type.
Creating a list is putting different comma-separated values between squere brackets. For example:

list1 = ['physics', 'chemistry', 1997, 2000];
Basic List Operations:
Lists respond to the + and * operators much like strings; they mean concatenation and repetition here too, except that the result is a new list, not a string.

In fact, lists respond to all of the general sequence operations we used on strings in the prior chapter.

Python Expression	Results	 Description
----------------------------------------------
len([1, 2, 3])	3	Length
[1, 2, 3] + [4, 5, 6]	[1, 2, 3, 4, 5, 6]	Concatenation
['Hi!'] * 4	['Hi!', 'Hi!', 'Hi!', 'Hi!']	Repetition
3 in [1, 2, 3]	True	Membership
for x in [1, 2, 3]: print x,	1 2 3	Iteration

list[:-1]  Gives the list except the last one element Splicing


Unique Python use of list:
--------------------------

1. Filter a number range
[x for x in range(1,1000) if x % 3 == 0 or x % 5 == 0]


2. square up a list 
a = [1,2,3]
b= [ i **2 for  i in a]

2. Reverse a list 
g = "abcd"
g[::-1]
>> dcba

It starts from the end and prints every element 
g[::-2]
>> db
It starts from the end and prints every next element  element

Use of enumerate
-------------------------------------
refactored in a list comprehension like this:
 def _treatment(pos, element):
    return '%d: %s' % (pos, element)
   
 seq = ["one", "two", "three"]
 [_treatment(i, el) for i, el in enumerate(seq)]
['0: one', '1: two', '2: three']


 
3) Dictionaries :
-------------------
A dictionary is mutable and is another container type that can store any number of Python objects,
including other container types. Dictionaries consist of pairs (called items) of keys and their
corresponding values.

Python dictionaries are also known as associative arrays or hash tables. The general syntax of a
dictionary is as follows:

dict = {'Alice': '2341', 'Beth': '9102', 'Cecil': '3258'}
You can create dictionary in the following way as well:

dict1 = { 'abc': 456 };
dict2 = { 'abc': 123, 98.6: 37 };
dict['Age'] = 8; # update existing entry
dict['School'] = "DPS School"; # Add new ent
Each key is separated from its value by a colon (:), the items are separated by commas, and the whole
thing is enclosed in curly braces. An empty dictionary without any items is written with just two 
curly braces, like this: {}.

Accessing Values in Dictionary:
*****************************8**
print "dict['Name']: ", dict['Name'];
print "dict['Age']: ", dict['Age'];

Delete Dictionary Elements:
*******************************
del dict['Name']; # remove entry with key 'Name'
dict.clear();     # remove all entries in dict
del dict ;        # delete entire dictionary

Key properties :
***************************
a) More than one entry per key not allowed. Which means no duplicate key is allowed. When duplicate keys 
encountered during assignment, the last assignment wins. Following is a simple example:



4) Sets :
The sets module provides classes for constructing and manipulating unordered collections of unique elements.
Common uses include membership testing, removing duplicates from a sequence, and computing standard math
operations on sets such as intersection, union, difference, and symmetric difference

 from sets import Set
 engineers = Set(['John', 'Jane', 'Jack', 'Janice'])
 programmers = Set(['Jack', 'Sam', 'Susan', 'Janice'])
 managers = Set(['Jane', 'Jack', 'Susan', 'Zack'])
 employees = engineers | programmers | managers           # union
 engineering_management = engineers & managers            # intersection
 fulltime_management = managers - engineers - programmers # difference
 engineers.add('Marvin')                                  # add element

Spatialnet use of set 
# scan for referenced poles (strip off the 'ST' from start of ID as not present in test db)
		referenced_pole_ids = set([staging_ent.attributes["POLE_ID"][2:]
								for staging_ent in layer_entities_map.get("Drops",[]) 
								if staging_ent.attributes.get("POLE_ID")])
here a set of unique pole ids referenced from a Drops can be drawn up 


Functions:
===================================================================================================
In python the functions are defined with def

def fun(a,b):
	c = a+b
	return c
d = fun(2,3)

default Function definition is here
	def printinfo( name, age = 35 ):

Variable-length arguments:
	def printinfo( arg1, *vartuple ):
		for var in vartuple:
      print var
	return;
	
multiple argument 
***************************************************************************************************
def manyArgs(*arg):
  print "I was called with", len(arg), "arguments:", arg

 manyArgs(1)
I was called with 1 arguments: (1,)
 manyArgs(1, 2,3)
I was called with 3 arguments: (1, 2, 3)	

j_txt = _("jobs: ") 

Single Lone Underscore (_)
==================================================
This is typically used in 3 cases:

In the interpreter: The _ name points to the result of the last executed statement in an interactive interpreter session.
 This was first done by the standard CPython interpreter, and others have followed too.

 _
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
NameError: name '_' is not defined
 42
 _
42
 'alright!' if _ else ':('
'alright!'
 _
'alright!'
As a name: This is somewhat related to the previous point. _ is used as a throw-away name. This will allow the next person reading your code to know that, by convention, a certain name is assigned but not intended to be used. 
For instance, you may not be interested in the actual value of a loop counter:

n = 42
for _ in range(n):
    do_something()

So overe here we dont care what number the loop is currently at we ajuts need to do something for 42 times and this saves 
n extra variable 
i18n: One may also see _ being used as a function. In that case, it is often the name used for the function that does internationalisation and 
localisation string translation lookups. This seems to have originated from and follow the corresponding C convention. For instance, as seen in
 the Django documentation for translation, you may have:

from django.utils.translation import ugettext as _
from django.http import HttpResponse

def my_view(request):
    output = _("Welcome to my site.")
    return HttpResponse(output)
The second and third purposes can conflict, so one should avoid using _ as a throw-away name in any code block that also uses it for i18n lookup and translation.
Difference between _, __ and __xx__ in Python
==================================================
When learning Python many people don't really understand why so much underlines in the beginning of the methods, sometimes even in the end like __this__! 
I've already had to explain it so many times, it's time to document it.

One underline in the beginning
Python doesn't have real private methods, so one underline in the beginning of a method or attribute means you shouldn't access this method, because it's not part of the API. It's very common when using properties:

class BaseForm(StrAndUnicode):
       
    
    def _get_errors(self):
        "Returns an ErrorDict for the data provided for the form"
        if self._errors is None:
            self.full_clean()
        return self._errors
    
    errors = property(_get_errors)
This snippet was taken from django source code (django/forms/forms.py). This means errors is a property, and it's part of the API, but the method this property calls, _get_errors, is "private", so you shouldn't access it.

Two underlines in the beginning
This one causes a lot of confusion. It should not be used to mark a method as private, the goal here is to avoid your method to be overridden by a subclass. Let's see an example:

class A(object):
    def __method(self):
        print "I'm a method in A"
    
    def method(self):
        self.__method()
     
a = A()
a.method()
The output here is

$ python example.py 
I'm a method in A
Fine, as we expected. Now let's subclass A and customize __method

class B(A):
    def __method(self):
        print "I'm a method in B"

b = B()
b.method()
and now the output is   

$ python example.py
I'm a method in A
as you can see, A.method() didn't call B.__method() as we could expect. Actually this is the correct behavior for __.
 So when you create a method starting with __ you're saying that you don't want anybody to override it, it will be accessible just from inside the own class.

How python does it? Simple, it just renames the method. Take a look:

a = A()
a._A__method()  # never use this!! please!
$ python example.py
I'm a method in A
If you try to access a.__method() it won't work either, as I said, __method is just accessible inside the class itself.

Two underlines in the beginning and in the end
When you see a method like __this__, the rule is simple: don't call it. Why? Because it means it's a method python calls,
 not you. Take a look:

 name = "igor"
 name.__len__()
4
 len(name)
4

 number = 10
 number.__add__(20)
30
 number + 20
30
There is always an operator or native function that calls these magic methods. The idea here is to give you the ability 
to override operators in your own classes. Sometimes it's just a hook python calls in specific situations. __init__(), 
for example, is called when the object is created so you can initialize it. __new__() is called to build the instance, and so on   

Here's an example:

class CrazyNumber(object):
    
    def __init__(self, n):
        self.n = n
    
    def __add__(self, other):
        return self.n - other.n
    
    def __sub__(self, other):
        return self.n + other.n
    
    def __str__(self):
        return str(self.n)


num = CrazyNumber(10)
print num           # 10
print num + 5       # 5
print num - 20      # 30
Another example:

class Room(object):

    def __init__(self):
        self.people = []

    def add(self, person):
        self.people.append(person)

    def __len__(self):
        return len(self.people)

room = Room()
room.add("Igor")
print len(room)     # 1
The documentation covers all these special methods.

Conclusion
Use _one_underline to mark you methods as not part of the API. Use __two_underlines__ when you're creating 
objects to look like native python objects or you wan't to customize behavior in specific situations. And 
don't use __just_to_underlines, unless you really know what you're doing!






	
Magic functions:
===================================================================================================
These are teh set of functions always surounded by the Underscores
__class__ =  
__name__ = 

 __init__ is not the first thing to get called. Actually, it's a method called __new__, which
 actually creates the instance, then passes any arguments at creation on to the initializer. At the other end of the object's 
 lifespan, there's __del__. Let's take a closer look at these 3 magic methods:

__new__(cls, [...)
__new__ is the first method to get called in an object's instantiation. It takes the class, then any other arguments that it 
will pass along to __init__. __new__ is used fairly rarely, but it does have its purposes, particularly when subclassing an 
immutable type like a tuple or a string.
I don't want to go in to too much detail on __new__ because it's not too useful, but it is covered in great detail in the 
Python docs.
__init__(self, [...)
The initializer for the class. It gets passed whatever the primary constructor was called with (so, for example, if we called

x = SomeClass(10, 'foo'),
__init__ would get passed 10 and 'foo' as arguments. __init__ is almost universally used in Python class definitions.
__del__(self)
If __new__ and __init__ formed the constructor of the object, __del__ is the destructor. It doesn't implement behavior for the
statement del x (so that code would not translate to x.__del__()). Rather, it defines behavior for when an object is garbage 
collected. It can be quite useful for objects that might require extra cleanup upon deletion, like sockets or file objects. 


Lambda Functions
===================================================================================================
Lamda functions are not bound to a name they just use a construct called lambda.
* Lamda function dont have  a return statment it will always have a line which is returned.
* Lamda can directly used without the use assigning it to a variable.

lambda x,y: x + y

this is a sum function 

lambda Functions:
==================================================
Python supports the creation of anonymous functions (i.e. functions that are not bound to a name) at runtime, 
diffrence between two functions
 def f (x): return x**2
    
 print f(8)
64
 
 g = lambda x: x**2
 
 print g(8)
64


 def make_incrementor (n): return lambda x: x + n
 
 f = make_incrementor(2)
 g = make_incrementor(6)
 
 print f(42), g(42)
44 48
 
 print make_incrementor(22)(33)
55
The above code defines a function "make_inrementor" that creates an anonymous function on the fly and returns it. The returned function increments its argument by the value that was specified when it was created.



No Multiline Lambda in Python: Why not?
===================================================
Look at the following:

map(multilambda x:
      y=x+1
      return y
   , [1,2,3])
Is this a lambda returning (y, [1,2,3]) (thus map only gets one parameter, resulting in an error)?
 Or does it return y? Or is it a syntax error, because the comma on the new line is misplaced? 
 How would Python know what you want?

Within the parens, indentation doesn't matter to python, so you can't unambiguously work with multilines.

This is just a simple one, there's probably more examples.



This is mostly used as argument of other functional concepts in python on a list of values 
g = [2, 18, 9, 22, 17, 24, 8, 12, 27]

1. filter

Filter seperates the list passed by the lamda then elements in a  new array 
 filter(lambda x: x % 3 == 0,g)
 
 >> [18, 9, 24, 12, 27]
 
 
 
2. map

map will do a set of operations on each of the elements and output it

 map(lambda x: x * 2 + 10, foo)

>> [14, 46, 28, 54, 44, 58, 26, 34, 64]

3. reduce

reduce will do a operation in all the elements at a once and output a result

reduce(lambda x, y: x + y, foo)
>> 139


Modules:
=================================
Creating a module is as simple as creating a file ,and Import it to use it 

Creating a folder Module will include a __init__.py file to make it a Module 


Importing and using of Module methods:
-----------------------------------------
File_mod/File_handler.py    A folder containing a file  having a class test

1) To import this file we 

from File_mod import File_handler

		or
import File_mod.File_handler

2) To Import test class inside the file 

 import File_mod.File_handler.test


Use of if (__name__) to check the Running module is Main or Imported:
--------------------------------------------------------------------
When your script is run by passing it as a command to the Python interpreter,

python myscript.py
all of the code that is at indentation level 0 gets executed.
__name__ is a built-in variable which evaluate to the name of the current module. However, if a module is being 
run directly (as in myscript.py above), then __name__ instead is set to the string "__main__". 
Thus, you can test whether your script is being run directly or being imported by something else by testing

# file one.py
def func():
    print("func() in one.py")

print("top-level in one.py")

if __name__ == "__main__":
    print("one.py is being run directly")
else:
    print("one.py is being imported into another module")

# file two.py
import one

print("top-level in two.py")
one.func()

if __name__ == "__main__":
    print("two.py is being run directly")
else:
    print("two.py is being imported into another module")
Now, if you invoke the interpreter as

python one.py
The output will be

top-level in one.py
one.py is being run directly
If you run two.py instead:

python two.py
You get

top-level in one.py
one.py is being imported into another module
top-level in two.py
func() in one.py
two.py is being run directly
Thus, when module one gets loaded, its __name__ equals "one" instead of __main__.



Global Variables:
=================================
Global vs. Local variables:
Variables that are defined inside a function body have a local scope, and those defined outside have a global scope.

This means that local variables can be accessed only inside the function in which they are declared, whereas global variables can be accessed 
throughout the program body by all functions. When you call a function, the variables declared inside it are brought into scope.
 Following is a simple example:

#!/usr/bin/python

total = 0; # This is global variable.
# Function definition is here
def sum( arg1, arg2 ):
   # Add both the parameters and return them."
   total = arg1 + arg2; # Here total is local variable.
   print "Inside the function local total : ", total
   return total;

# Now you can call sum function
sum( 10, 20 );
print "Outside the function global total : ", total 
When the above code is executed, it produces the following result:

Inside the function local total :  30
Outside the function global total :  0
Another way to that is by using 

rd global is only useful to change or create global variables in a local context, although creating global variables is seldom considered a good solution.

def bob():
    me = "locally defined"    # Defined only in local context
    print me

bob()
print me     # Asking for a global variable
The above will give you:

locally defined
Traceback (most recent call last):
  File "file.py", line 9, in <module>
    print me
NameError: name 'me' is not defined
While if you use the global statement, the variable will become available "outside" the scope of the function, effectively becoming a global variable.

def bob():
    global me
    me = "locally defined"   # Defined locally but declared as global
    print me

bob()
print me     # Asking for a global variable
So the above code will give you:

locally defined
locally defined


class and objects:
=============================================================================================================
Class declaration in python

class a:
     
    

new way of class declaration in Python

class a(object):
// calling the instance of the python
x = a()
now if we print the object 

print a
<__main__.ObjectCreator object at 0x8974f2c>

Classes are objects too.As soon as you use the keyword class, Python executes it and creates an OBJECT
This object (the class) is itself capable of creating objects (the instances), and this is why it's a class.

But still, it's an object, and therefore:
i)you can assign it to a variable
	b =a 
ii)you can copy it
	c(a)

iii)you can add attributes to it
	a.var = 'test'

iv)you can pass it as a function parameter
	print a

OOSP pythonic Way :
=======================================================
virtual clases:
---------------------

When abstractmethod() is applied in combination with other method descriptors, it should be applied as the innermost decorator,
 as shown in the following usage examples:

import abc

class Shape(object):
    __metaclass__ = abc.ABCMeta

    @abc.abstractmethod
    def method_to_implement(self, input):
        """Method documentation"""
        return

    @property
    @abc.abstractmethod
    def my_abstract_property(self):
           
    @my_abstract_property.setter
    @abc.abstractmethod
    def my_abstract_property(self, val):
           

    @abc.abstractmethod
    def _get_x(self):
           
    @abc.abstractmethod
    def _set_x(self, val):
           
    x = property(_get_x, _set_x)
If your class is already using a metaclass, derive it from ABCMeta rather than type and you can continue to use your own metaclass.
Alternative of Abstarct methods:
--------------------------------
A cheap alternative (and the best practice before the abc module was introduced) would be to have all your abstract methods just
raise an exception (NotImplementedError is a good one) so that classes derived from it would have to override that method to be useful.
Before this te duck typing concept was der 
Accessing members of both sequence type objects and mapping type objects is done by using the __getitem__ method of these objects.

a = [0,1 2,3]
print a[0]
0
b = {'a': 0, 'b': 1}
print b['a']
0
is exactly the same as :

a =  [0,1 2, 3]
print list.__getitem__(a, 0)
0
b = {'a': 0, 'b': 1}
print dict._getitem__(b, 'a')
0
In the first example we use normal Python syntax. In the second example we do what the first example is doing under the hood. In order to set
 members we would use the __setitem__ method instead of __getitem__.
 
 

Python methods are always virtual
like Ignacio said yet Somehow class inheritance may be a better approach to implement what you want.

class Animal:
    def __init__(self,name,legs):
        self.name = name
        self.legs = legs

    def getLegs(self):
        return "{0} has {1} legs".format(self.name, self.legs)

    def says(self):
        return "I am an unknown animal"

class Dog(Animal): # <Dog inherits from Animal here (all methods as well)

    def says(self): # <Called instead of Animal says method
        return "I am a dog named {0}".format(self.name)
The principle of duck typing says that you shouldn't care what type of object you have - just whether or not you can do
 the required action with your object.
 For this reason the isinstance keyword is frowned upon.
    def somethingOnlyADogCanDo(self):
        return "be loyal"

formless = Animal("Animal", 0)
rover = Dog("Rover", 4) #<calls initialization method from animal

print(formless.says()) # <calls animal say method

print(rover.says()) #<calls Dog says method
print(rover.getLegs()) #<calls getLegs method from animal class
Results should be:

I am an unknown animal
I am a dog named Rover
Rover has 4 legs


Static method :
--------------------------------------

class MyClass(object):
    @staticmethod
    def the_static_method(x):
        print x

MyClass.the_static_method(2) # outputs 2

Class vs static methods in Python:
===================================
class to do so but that will spread the code related to class, to out of the class. This can cause a future code

def get_no_of_instances(cls_obj):
    return cls_obj.no_inst
 
class Kls(object):
    no_inst = 0
 
    def __init__(self):
        Kls.no_inst = Kls.no_inst + 1
 
ik1 = Kls()
ik2 = Kls()
 
print(get_no_of_instances(Kls))
Gives us the following output:

1
2

we can create a method in a class, using @classmethod
-------------------------------------------------------------------

class Kls(object):
    no_inst = 0
 
    def __init__(self):
        Kls.no_inst = Kls.no_inst + 1
 
    @classmethod
    def get_no_of_instance(cls_obj):
        return cls_obj.no_inst
 
ik1 = Kls()
ik2 = Kls()
 
print ik1.get_no_of_instance()
print Kls.get_no_of_instance()
We get the following output:
2
2

Python @staticmethod
----------------------------------------------------------------------
Often there is some functionality that relates to the class, but does not need the class or any instance(s) to do some work.
 Perhaps something like setting environmental variables, changing an attribute in another class, etc. In these situation we
 can also use a function, however doing so also spreads the interrelated code .
 
  use a @staticmethod, we can place all code in the relevant place.
  
IND = 'ON'
 
class Kls(object):
    def __init__(self, data):
        self.data = data
 
    @staticmethod
    def checkind():
        return (IND == 'ON')  ## this is a global variable value which is intialised over here 
 
    def do_reset(self):
        if self.checkind():
            print('Reset done for:', self.data)
 
    def set_db(self):
        if self.checkind():
            self.db = 'New db connection'
        print('DB connection made for: ', self.data)
 
ik1 = Kls(12)
ik1.do_reset()
ik1.set_db()

Reset done for: 12
DB connection made for: 12



How @staticmethod and @classmethod are different.
class Kls(object):
    def __init__(self, data):
        self.data = data
 
    def printd(self):
        print(self.data)
 
    @staticmethod
        def smethod(*arg):
            print('Static:', arg)
 
    @classmethod
        def cmethod(*arg):
            print('Class:', arg)
>>> ik = Kls(23)
>>> ik.printd()
23
>>> ik.smethod()
Static: ()
>>> ik.cmethod()
Class: (<class '__main__.Kls'>,)
>>> Kls.printd()
TypeError: unbound method printd() must be called with Kls instance as first argument (got nothing instead)
>>> Kls.smethod()
Static: ()
>>> Kls.cmethod()
 
 
 
Magical methods :
===============================================================================

http://www.rafekettler.com/magicmethods.html#intro

Yeild Keyword :
====================================================================================================================
Iterables

When you create a list, you can read its items one by one, and it's called iteration:

 mylist = [1, 2, 3]
 for i in mylist:
       print(i)
1
2
3
Mylist is an iterable. When you use a list comprehension, you create a list, and so an iterable:

 mylist = [x*x for x in range(3)]
 for i in mylist:
       print(i)
0
1
4
Everything you can use "for    in..." on is an iterable: lists, strings, files   
 These iterables are handy because you can read them as much as you wish, but you store all the values in memory and 
 it's not always what you want when you have a lot of values.

Generators

Generators are iterators, but you can only iterate over them once. It's because they do not store all the values in memory,
they generate the values on the fly:

 mygenerator = (x*x for x in range(3))
 for i in mygenerator:
       print(i)
0
1
4
It is just the same except you used () instead of []. BUT, you can not perform for i in mygenerator a second time since generators can only
be used once: they calculate 0, then forget about it and calculate 1, and end calculating 4, one by one.

for i in mygenerator:
       print(i)
Actually this outputs nothing	   
	   
Yield
=============
Yield is a keyword that is used like return, except the function will return a generator.

 def createGenerator():
       mylist = range(3)
       for i in mylist:
           yield i*i
   
 mygenerator = createGenerator() # create a generator
 print(mygenerator) # mygenerator is an object!
<generator object createGenerator at 0xb7555c34>
 for i in mygenerator:
        print(i)
0
1
4
Here it's a useless example, but it's handy when you know your function will return a huge set of values that
 you will only need to read once.

To master yield, you must understand that when you call the function, the code you have written in the function
 body does not run. The function only returns the generator object, this is a bit tricky :-)

Then, your code will be run each time the for uses the generator.

Now the hard part:

The first time the for calls the generator object created from your function, it will run the code in your function 
from the beginning until it hits yield, then it'll return the first value of the loop. Then, each other call will
 run the loop you have written in the function one more time, and return the next value, until there is no value to return.

The generator is considered empty once the function runs but does not hit yield anymore. It can be because the loop 
had come to an end, or because you do not satisfy a "if/else" anymore.

Iterators:
-----------------------------------------------------------
An iterator is nothing more than a container object that implements the iterator
protocol

i = iter([1,2,3,4])
>> i.next()
1
>> i.next()
2
    like that 
the way this is implemented is by using two methods 
a) next, which returns the next item of the container
b) __iter__, which returns the iterator itself

Now we can write our own iterators in the same way just have to implement the __iter__ method 
and next() this next name can be different its not a mandate

class myiter(object):
	def __init__(self,step):
		self.step=step
	def next(self):
		if step ==0:
			raise Exception("Ended")
		step -=1
		return self.step
	def __iter__(self):
		"""return the the iteratoe itself"""
		return self
		
>>i =  myiter(5)
>> for el in MyIterator(4):
    print el

Generators :
-------------------------------------------------------------
generators provide an elegant way to write simple and efficient
code for functions that return a list of elements. Based on the yield directive, they
allow you to pause a function and return an intermediate result. The function saves
its execution context and can be resumed later if necessary.

Ex:
def fibonacci():
	a, b = 0, 1
	while True:
		yield b
		a, b = b, a + b
fib = fibonacci()
[fib.next() for i in range(10)]
[3, 5, 8, 13, 21, 34, 55, 89, 144, 233]

This function returns a generator object, a special iterator, which knows how to
save the execution context. It can be called indefinitely, yielding the next element of
the suite each time.

*** when to use 
generators should be considered every time you deal with a
function that returns a sequence or works in a loop. Returning the elements one at a
time can improve the overall performance, when they are passed to another function
for further work.

One step up again it looks like we will be using yeild as expression and 

but makes yield return the value passed. The function can,
therefore, change its behavior depending on the client code. Two other functions
were added to complete this behavior: throw and close. They raise an error into
the generator:
throw allows the client code to send any kind of exception to be raised.
close acts in the same way, but raises a specific exception: GeneratorExit.
In that case, the generator function must raise GeneratorExit again, or
StopIteration.
Therefore, a typical template for a generator would look like the following:
 def my_generator():
    try:
		yield 'something'
		except ValueError:
		yield 'dealing with the exception'
    finally:
		print "ok let's clean"
   
 gen = my_generator()
 gen.next()
'something'
 gen.throw(ValueError('mean mean mean'))
'dealing with the exception'
 gen.close()
ok let's clean
 gen.next()
Traceback (most recent
The itertools Module
================================================================================
When iterators were added in Python, a new module was provided to implement
common patterns.

1) islice: 
------------------
One can use islice every time to extract data located in a particular position in a
stream.

itertools.islice(iterable, start, stop[, step])
# islice('ABCDEFG', 2) --> A B
# islice('ABCDEFG', 2, 4) --> C D
# islice('ABCDEFG', 2, None) --> C D E F G
# islice('ABCDEFG', 0, None, 2) --> A C E G

Practicle uses:
This can be a file in a special format using records for instance, or a stream
that presents data encapsulated with metadata, like a SOAP envelope, for example.
In that case, islice can be seen as a window that slides over each line of data

2) tee:
--------------------
An iterator consumes the sequence it works with. There is no turning back. tee
provides a pattern to run several iterators over a sequence. This helps us to run over
the data again, if provided with the information of the first run

 import itertools
 def with_head(iterable, headsize=1):
    a, b = itertools.tee(iterable)
    return list(itertools.islice(a, headsize)), b
   
 with_head(seq)
([1], <itertools.tee object at 0x100c698>)
 with_head(seq, 4)
([1, 2, 3, 4], <itertools.tee object at 0x100c670>)
In this function, if two iterators are generated with tee, then the first one is used
with islice to get the first headsize elements of the iteration, and return them as
a flat list. The second element returned is a fresh iterator that can be used to perform
work over the whole sequence.

3) groupby: The uniq Iterator
------------------------------------
 It is able to group the duplicate elements from an iterator, as long as they are adjacent. A function can
be given to the function for it to compare the elements
Just a few lines are necessary with groupby to obtain RLE:
 from itertools import groupby
 def compress(data):
    return ((len(list(group)), name)
    for name, group in groupby(data))
   
 def decompress(data):
    return (car * size for size, car in data)
   
 list(compress('get uuuuuuuuuuuuuuuuuup'))
[(1, 'g'), (1, 'e'), (1, 't'), (1, ' '),
(18, 'u'), (1, 'p')]
 compressed = compress('get uuuuuuuuuuuuuuuuuup')
 ''.join(decompress(compressed))
'get uuuuuuuuuuuuuuuuuup

These function are used to make utilities for a ode libarary 
 # [k for k, g in groupby('AAAABBBCCDAABBB')] --> A B C D A B
 # [list(g) for k, g in groupby('AAAABBBCCD')] --> AAAA BBB CC D
 
Others are:
------------------------------
Infinite Iterators:
# count(10) --> 10 11 12 13 14    
# count(2.5, 0.5) -> 2.5 3.0 3.5    
# cycle('ABCD') --> A B C D A B C D A B C D    
# repeat(10, 3) --> 10 10 10
Iterators terminating on the shortest input sequence:
# chain('ABC', 'DEF') --> A B C D E F
# compress('ABCDEF', [1,0,1,0,1,1]) --> A C E F
#ifilter(lambda x: x%2, range(10)) --> 1 3 5 7 9
# imap(pow, (2,3,10), (5,2,3)) --> 32 9 1000   (2P5,)
# izip_longest('ABCD', 'xy', fillvalue='-') --> Ax By C- D-
Combinatoric generators:
# product('ABCD', 'xy') --> Ax Ay Bx By Cx Cy Dx Dy
# product(range(2), repeat=3) --> 000 001 010 011 100 101 110 111
# permutations('ABCD', 2) --> AB AC AD BA BC BD CA CB CD DA DB DC
# permutations(range(3)) --> 012 021 102 120 201 210
# combinations('ABCD', 2) --> AB AC AD BC BD CD
# combinations(range(4), 3) --> 012 013 023 123


Greedy vs. Non-Greedy:
================================

Exception Handling:
==============================
Type of Errors:

 10 * (1/0)
Traceback (most recent call last):
  File "<stdin>", line 1, in ?
ZeroDivisionError: integer division or modulo by zero
 4 + spam*3
Traceback (most recent call last):
  File "<stdin>", line 1, in ?
NameError: name 'spam' is not defined
 '2' + 2
Traceback (most recent call last):
  File "<stdin>", line 1, in ?
TypeError: cannot concatenate 'str' and 'int' objects

How the for statement works?
Any time you use for to iterate over an iterable (basically, all sequence types and anything that defines __iter__() or __getitem__()),
 it needs to know when to stop iterating. Take a look at the code below:

words = ['exceptions', 'are', 'useful']
for word in words:
    print(word)
How does for know when it's reached the last element in words and should stop trying to get more items? The answer may surprise you:
 the list raises a StopIteration exception.
In fact, all iterables follow this pattern. When a for statement is first evaluated, it calls iter() on the object being iterated over.\
 This creates an iterator for the object, capable of returning the contents of the object in sequence. For the call to iter() to succeed,
 the object must either support the iteration protocol (by defining __iter__()) or the sequence protocol (by defining __getitem__()).

As it happens, both the __iter__() and __getitem__() functions are required to raise an exception when the items to iterate over are exhausted.
 __iter__() raises the StopIteration exception, as discussed earlier, and __getitem__() raises the IndexError exception. This is how for knows when to stop
 __iter__() raises the StopIteration exception, as discussed earlier, and __getitem__() raises the IndexError exception. This is how for knows when to stop




Raising the exceptions :
-------------------------------------------------------------------------------
def _check_parents_equal(self, other):
        if not self._parents_equal(other):
            raise TypeError(
                'This operation is valid only for enum values of the same type')
                
                


catching it exception passing it for another try:

def __getitem__(cls, other):
        try:
            _check_parents_equal(self, other)
        except TypeError:
            pass
        try:
            return cls._enums_by_str[name]
        except KeyError:
            return cls._enums_by_int[name]


 Memory managment in Python:
 ===================================================================================
 
 private heap
-----------------------------------------
| all Python objects and data structures |
------------------------------------------
				|
		(Python memory manager) # this controls the heap`
				
				
Steps for Allocations:
1. At the lowest Level a raw memory allocator ensures enough room in the private heap 
for storing all Python-related data by interacting with the memory manager od OS.
2. several object-specific allocators operate on the same heap and implement distinct memory management(lke for int / dict etc)
The default raw memory block allocator uses the following functions: malloc(), calloc(), realloc() and free(); call malloc(1)

class Resource:
    def __init__(self, name):
        self.handle = allocate_resource(name)
    def __del__(self):
        if self.handle:
            self.close()
    def close(self):
        release_resource(self.handle)
        self.handle = None
    ...

for name in big_list:
    x = Resource(name)
    do something with x
In current releases of CPython, each new assignment to x inside the loop will release the previously allocated resource. 
Using GC, this is not guaranteed. If you want to write code that will work with any Python implementation, you should explicitly
close the resource; this will work regardless of GC:

for name in big_list:
    x = Resource()
    do something with x
    x.close()


Python heap is performed by the interpreter itself and that the user has no control over it, even if she regularly manipulates
object pointers to memory blocks inside that heap.
			The allocation of heap space for Python objects and other internal buffers is performed on demand by the Python memory manager through
			
Decorators:
=================================
Python's functions are objects
To understand decorators, you must first understand that functions are objects in Python. 
This has important consequences. 

    
Functions references:
===========================
functions are objects and therefore can be assigned to a variable can be defined in another function.
Well, that means that a function can return another function :-) Have a look:

def getTalk(type="shout"):

    # We define functions on the fly
    def shout(word="yes"):
        return word.capitalize()+"!"

    def whisper(word="yes") :
        return word.lower()+"...";

    # Then we return one of them
    if type == "shout":
        # We don't use "()", we are not calling the function,
        # we are returning the function object
        return shout  
    else:
        return whisper

# How do you use this strange beast?

# Get the function and assign it to a variable
	talk = getTalk()      

# You can see that "talk" is here a function object:
	print talk
#outputs : <function shout at 0xb7ea817c>

# The object is the one returned by the function:
	print talk()
#outputs : Yes!

# And you can even use it directly if you feel wild:
	print getTalk("whisper")()
#outputs : yes   
But wait, there is more. If you can return a function, then you can pass one as a parameter:

def doSomethingBefore(func): 
    print "I do something before then I call the function you gave me"
    print func()

	doSomethingBefore(scream)
#outputs: 
#I do something before then I call the function you gave me
#Yes!
Well, you just have everything needed to understand decorators. You see, decorators are wrappers which means that they let you execute code before and after the function they decorate without the need to modify the function itself.


Passing Multiple Arguments to a Function at a shot :
==============================================================================================
def test_var_args_call(arg1, arg2, arg3):
    print "arg1:", arg1
    print "arg2:", arg2
    print "arg3:", arg3


Passing as a Tuple: Just create a tuple with the arguments and Pass it in the definations 

args = ("two", 3)
test_var_args_call(1, *args)

passing as a Dictionary:  will Similarly just keep the names same of the keys with the args in the function 

kwargs = {"arg3": 3, "arg2": "two"}
test_var_args_call(1, **kwargs)


Handcrafted decorators
==================================================================================================
How you would do it manually:

# A decorator is a function that expects ANOTHER function as parameter
def my_shiny_new_decorator(a_function_to_decorate):

    # Inside, the decorator defines a function on the fly: the wrapper.
    # This function is going to be wrapped around the original function
    
    def the_wrapper_around_the_original_function():

        print "Before the function runs"

        # Call the function here (using parentheses) Actual function call
        a_function_to_decorate()

        print "After the function runs"

    # At this point, "a_function_to_decorate" HAS NEVER BEEN EXECUTED.
    # We return the wrapper function we have just created.
    return the_wrapper_around_the_original_function

# Now imagine you create a function you don't want to ever touch again.
def a_stand_alone_function():
    print "I am a stand alone function, don't you dare modify me"

a_stand_alone_function() 
#outputs: I am a stand alone function, don't you dare modify me

Well, you can decorate it to extend its behavior. Just pass it to the decorator, it will wrap it dynamically in 
 any code you want and return you a new function ready to be used:

a_stand_alone_function_decorated = my_shiny_new_decorator(a_stand_alone_function)
a_stand_alone_function_decorated()


#outputs:
 Before the function runs
 I am a stand alone function, don't you dare modify me
 After the function runs
Now, you probably want that every time you call a_stand_alone_function,
a_stand_alone_function_decorated is called instead. That's easy, just overwrite a_stand_alone_function with 
the function returned by my_shiny_new_decorator:


Decorators demystified
The previous example, using the decorator syntax Now to reuse the above decorator created above and 

@my_shiny_new_decorator
def another_stand_alone_function():
    print "Leave me alone"

another_stand_alone_function()  
#outputs:  
 Before the function runs
 Leave me alone
 After the function runs
 
Yes, that's all, it's that simple. @decorator is just a shortcut to:

another_stand_alone_function = my_shiny_new_decorator(another_stand_alone_function)
Decorators are just a pythonic variant of the decorator design pattern. 
There are several classic design patterns embedded in Python to ease development, like iterators.
Of course, you can cumulate decorators:

def bread(func):
    def wrapper():
        print "</''''''\>"
        func()
        print "<\______/>"
    return wrapper

def ingredients(func):
    def wrapper():
        print "#tomatoes#"
        func()
        print "~salad~"
    return wrapper

def sandwich(food="--ham--"):
    print food

sandwich()
#outputs: --ham--
sandwich = bread(ingredients(sandwich))
sandwich()
#outputs:
#</''''''\>
# #tomatoes#
# --ham--
# ~salad~
#<\______/>
Using the Python decorator syntax:

@bread
@ingredients
def sandwich(food="--ham--"):
    print food

sandwich()
#outputs:
#</''''''\>
# #tomatoes#
# --ham--
# ~salad~
#<\______/>
The order you set the decorators MATTERS:

@ingredients
@bread
def strange_sandwich(food="--ham--"):
    print food

strange_sandwich()
#outputs:
##tomatoes#
#</''''''\>
# --ham--
#<\______/>
# ~salad~
Eventually answering the question
As a conclusion, you can easily see how to answer the question:

# The decorator to make it bold
def makebold(fn):
    # The new function the decorator returns
    def wrapper():
        # Insertion of some code before and after
        return "<b>" + fn() + "</b>"
    return wrapper

# The decorator to make it italic
def makeitalic(fn):
    # The new function the decorator returns
    def wrapper():
        # Insertion of some code before and after
        return "<i>" + fn() + "</i>"
    return wrapper

@makebold
@makeitalic
def say():
    return "hello"

print say() 
#outputs: <b><i>hello</i></b>

# This is the exact equivalent to 
def say():
    return "hello"
say = makebold(makeitalic(say))

print say() 
#outputs: <b><i>hello</i></b>
You can now just leave happy, or burn your brain a little bit more and see advanced uses of decorators.

Passing arguments to the decorated function
# It's not black magic, you just have to let the wrapper 
# pass the argument:

def a_decorator_passing_arguments(function_to_decorate):
    def a_wrapper_accepting_arguments(arg1, arg2):
        print "I got args! Look:", arg1, arg2
        function_to_decorate(arg1, arg2)
    return a_wrapper_accepting_arguments

# Since when you are calling the function returned by the decorator, you are
# calling the wrapper, passing arguments to the wrapper will let it pass them to 
# the decorated function

@a_decorator_passing_arguments
def print_full_name(first_name, last_name):
    print "My name is", first_name, last_name

print_full_name("Peter", "Venkman")
# outputs:
#I got args! Look: Peter Venkman
#My name is Peter Venkman
Decorating methods
What's great with Python is that methods and functions are really the same,
except methods expect their first parameter to be a reference to the current object (self).
It means you can build a decorator for methods the same way, just remember to take self in consideration:

def method_friendly_decorator(method_to_decorate):
    def wrapper(self, lie):
        lie = lie - 3 # very friendly, decrease age even more :-)
        return method_to_decorate(self, lie)
    return wrapper


class Lucy(object):

    def __init__(self):
        self.age = 32

    @method_friendly_decorator
    def sayYourAge(self, lie):
        print "I am %s, what did you think?" % (self.age + lie)

l = Lucy()
l.sayYourAge(-3)
#outputs: I am 26, what did you think?
Of course, if you make a very general decorator and want to apply it to any function or method, no matter
its arguments, then just use *args, **kwargs:

def a_decorator_passing_arbitrary_arguments(function_to_decorate):
    # The wrapper accepts any arguments
    def a_wrapper_accepting_arbitrary_arguments(*args, **kwargs):
        print "Do I have args?:"
        print args
        print kwargs
        # Then you unpack the arguments, here *args, **kwargs
        # If you are not familiar with unpacking, check:
        # http://www.saltycrane.com/blog/2008/01/how-to-use-args-and-kwargs-in-python/
        function_to_decorate(*args, **kwargs)
    return a_wrapper_accepting_arbitrary_arguments

@a_decorator_passing_arbitrary_arguments
def function_with_no_argument():
    print "Python is cool, no argument here."

function_with_no_argument()
#outputs
#Do I have args?:
#()
#{}
#Python is cool, no argument here.

@a_decorator_passing_arbitrary_arguments
def function_with_arguments(a, b, c):
    print a, b, c

function_with_arguments(1,2,3)
#outputs
#Do I have args?:
#(1, 2, 3)
#{}
#1 2 3 

@a_decorator_passing_arbitrary_arguments
def function_with_named_arguments(a, b, c, platypus="Why not ?"):
    print "Do %s, %s and %s like platypus? %s" %\
    (a, b, c, platypus)

function_with_named_arguments("Bill", "Linus", "Steve", platypus="Indeed!")
#outputs
#Do I have args ? :
#('Bill', 'Linus', 'Steve')
#{'platypus': 'Indeed!'}
#Do Bill, Linus and Steve like platypus? Indeed!

class Mary(object):

    def __init__(self):
        self.age = 31

    @a_decorator_passing_arbitrary_arguments
    def sayYourAge(self, lie=-3): # You can now add a default value
        print "I am %s, what did you think ?" % (self.age + lie)

m = Mary()
m.sayYourAge()
#outputs
# Do I have args?:
#(<__main__.Mary object at 0xb7d303ac>,)
#{}
#I am 28, what did you think?
Passing arguments to the decorator
Great, now what would you say about passing arguments to the decorator itself? Well this is a bit twisted because
a decorator must accept a function as an argument and therefore, you cannot pass the decorated function arguments 
directly to the decorator.

Before rushing to the solution, let's write a little reminder:

# Decorators are ORDINARY functions
def my_decorator(func):
    print "I am a ordinary function"
    def wrapper():
        print "I am function returned by the decorator"
        func()
    return wrapper

# Therefore, you can call it without any "@"

def lazy_function():
    print "zzzzzzzz"

decorated_function = my_decorator(lazy_function)
#outputs: I am a ordinary function 

# It outputs "I am a ordinary function", because that's just what you do:
# calling a function. Nothing magic.

@my_decorator
def lazy_function():
    print "zzzzzzzz"

#outputs: I am a ordinary function
It's exactly the same. "my_decorator" is called. So when you @my_decorator, you are telling Python to call 
the function 'labeled by the variable "my_decorator"'. It's important, because the label you give can point directly to the decorator 
or not! Let's start to be evil!

def decorator_maker():

    print "I make decorators! I am executed only once: "+\
          "when you make me create a decorator."

    def my_decorator(func):

        print "I am a decorator! I am executed only when you decorate a function."

        def wrapped():
            print ("I am the wrapper around the decorated function. "
                  "I am called when you call the decorated function. "
                  "As the wrapper, I return the RESULT of the decorated function.")
            return func()

        print "As the decorator, I return the wrapped function."

        return wrapped

    print "As a decorator maker, I return a decorator"
    return my_decorator

# Let's create a decorator. It's just a new function after all.
new_decorator = decorator_maker()       
#outputs:
#I make decorators! I am executed only once: when you make me create a decorator.
#As a decorator maker, I return a decorator

# Then we decorate the function

def decorated_function():
    print "I am the decorated function."

decorated_function = new_decorator(decorated_function)
#outputs:
#I am a decorator! I am executed only when you decorate a function.
#As the decorator, I return the wrapped function

# Let's call the function:
decorated_function()
#outputs:
#I am the wrapper around the decorated function. I am called when you call the decorated function.
#As the wrapper, I return the RESULT of the decorated function.
#I am the decorated function.
No surprise here. Let's do EXACTLY the same thing, but skipping intermediate variables:

def decorated_function():
    print "I am the decorated function."
decorated_function = decorator_maker()(decorated_function)
#outputs:
#I make decorators! I am executed only once: when you make me create a decorator.
#As a decorator maker, I return a decorator
#I am a decorator! I am executed only when you decorate a function.
#As the decorator, I return the wrapped function.

# Finally:
decorated_function()    
#outputs:
#I am the wrapper around the decorated function. I am called when you call the decorated function.
#As the wrapper, I return the RESULT of the decorated function.
#I am the decorated function.
Let's make it AGAIN, even shorter:

@decorator_maker()
def decorated_function():
    print "I am the decorated function."
#outputs:
#I make decorators! I am executed only once: when you make me create a decorator.
#As a decorator maker, I return a decorator
#I am a decorator! I am executed only when you decorate a function.
#As the decorator, I return the wrapped function.

#Eventually: 
decorated_function()    
#outputs:
#I am the wrapper around the decorated function. I am called when you call the decorated function.
#As the wrapper, I return the RESULT of the decorated function.
#I am the decorated function.
Hey, did you see that? We used a function call with the "@" syntax :-)

So back to decorators with arguments. If we can use functions to generate the decorator on the fly, we can pass arguments to that function, right?

def decorator_maker_with_arguments(decorator_arg1, decorator_arg2):

    print "I make decorators! And I accept arguments:", decorator_arg1, decorator_arg2

    def my_decorator(func):
        # The ability to pass arguments here is a gift from closures.
        print "I am the decorator. Somehow you passed me arguments:", decorator_arg1, decorator_arg2

        # Don't confuse decorator arguments and function arguments!
        def wrapped(function_arg1, function_arg2) :
            print ("I am the wrapper around the decorated function.\n"
                  "I can access all the variables\n"
                  "\t- from the decorator: {0} {1}\n"
                  "\t- from the function call: {2} {3}\n"
                  "Then I can pass them to the decorated function"
                  .format(decorator_arg1, decorator_arg2,
                          function_arg1, function_arg2))
            return func(function_arg1, function_arg2)

        return wrapped

    return my_decorator

@decorator_maker_with_arguments("Leonard", "Sheldon")
def decorated_function_with_arguments(function_arg1, function_arg2):
    print ("I am the decorated function and only knows about my arguments: {0}"
           " {1}".format(function_arg1, function_arg2))

decorated_function_with_arguments("Rajesh", "Howard")
#outputs:
#I make decorators! And I accept arguments: Leonard Sheldon
#I am the decorator. Somehow you passed me arguments: Leonard Sheldon
#I am the wrapper around the decorated function. 
#I can access all the variables 
#   - from the decorator: Leonard Sheldon 
#   - from the function call: Rajesh Howard 
#Then I can pass them to the decorated function
#I am the decorated function and only knows about my arguments: Rajesh Howard

A decorator to decorate a decorator OK, as a bonus, 

def decorator_with_args(decorator_to_enhance):
    """ 
    This function is supposed to be used as a decorator.
    It must decorate an other function, that is intended to be used as a decorator.
    Take a cup of coffee.
    It will allow any decorator to accept an arbitrary number of arguments,
    saving you the headache to remember how to do that every time.
    """

    # We use the same trick we did to pass arguments
    def decorator_maker(*args, **kwargs):

        # We create on the fly a decorator that accepts only a function
        # but keeps the passed arguments from the maker.
        def decorator_wrapper(func):

            # We return the result of the original decorator, which, after all, 
            # IS JUST AN ORDINARY FUNCTION (which returns a function).
            # Only pitfall: the decorator must have this specific signature or it won't work:
            return decorator_to_enhance(func, *args, **kwargs)

        return decorator_wrapper

    return decorator_maker
It can be used as follows:

# You create the function you will use as a decorator. And stick a decorator on it :-)
# Don't forget, the signature is "decorator(func, *args, **kwargs)"
@decorator_with_args 
def decorated_decorator(func, *args, **kwargs): 
    def wrapper(function_arg1, function_arg2):
        print "Decorated with", args, kwargs
        return func(function_arg1, function_arg2)
    return wrapper

# Then you decorate the functions you wish with your brand new decorated decorator.

@decorated_decorator(42, 404, 1024)
def decorated_function(function_arg1, function_arg2):
    print "Hello", function_arg1, function_arg2

decorated_function("Universe and", "everything")
#outputs:
#Decorated with (42, 404, 1024) {}
#Hello Universe and everything

# Whoooot!
I know, the last time you had this feeling, it was after listening a guy saying: "before understanding recursion, you must first understand recursion". But now, don't you feel good about mastering this?

Best practices while using decorators
====================================================================================
They are new as of Python 2.4, so be sure that's what your code is running on.
Decorators slow down the function call. Keep that in mind.
You can not un-decorate a function. There are hacks to create decorators that can be removed but nobody uses them.
 So once a function is decorated, it's done. For all the code.
Decorators wrap functions, which can make them hard to debug.
Python 2.5 solves this last issue by providing the functools module including functools.wraps that copies the name, module and docstring
 of any wrapped function to it's wrapper. Fun fact, functools.wraps is a decorator :-)

 # "functools" can help for that

import functools

def bar(func):
    # We say that "wrapper", is wrapping "func"
    # and the magic begins
    @functools.wraps(func)
    def wrapper():
        print "bar"
        return func()
    return wrapper

@bar
def foo():
    print "foo"

print foo.__name__
#outputs: foo
How can the decorators be useful?
===========================================
Now the big question: what can I use decorators for? Seem cool and powerful, but a practical example would be great. Well, there are 1000 possibilities.
Classic uses are extending a function behavior from an external lib (you can't modify it) or for a debug purpose (you don't want to modify it because
 it's temporary). You can use them to extends several functions with the same code without rewriting it every time, for DRY's sake. E.g.:

 Checking Execution time:
 ***************************
def benchmark(func):
    """
    A decorator that prints the time a function takes
    to execute.
    """
    import time
    def wrapper(*args, **kwargs):
        t = time.clock()
        res = func(*args, **kwargs)
        print func.__name__, time.clock()-t
        return res
    return wrapper


def logging(func):
    """
    A decorator that logs the activity of the script.
    (it actually just prints it, but it could be logging!)
    """
    def wrapper(*args, **kwargs):
        res = func(*args, **kwargs)
        print func.__name__, args, kwargs
        return res
    return wrapper


def counter(func):
    """
    A decorator that counts and prints the number of times a function has been executed
    """
    def wrapper(*args, **kwargs):
        wrapper.count = wrapper.count + 1
        res = func(*args, **kwargs)
        print "{0} has been used: {1}x".format(func.__name__, wrapper.count)
        return res
    wrapper.count = 0
    return wrapper

@counter
@benchmark
@logging
def reverse_string(string):
    return str(reversed(string))

print reverse_string("Able was I ere I saw Elba")
print reverse_string("A man, a plan, a canoe, pasta, heros, rajahs, a coloratura, maps, snipe, percale, macaroni, a gag, a banana bag, a tan, a tag, a banana bag again (or a camel), a crepe, pins, Spam, a rut, a Rolo, cash, a jar, sore hats, a peon, a canal: Panama!")

#outputs:
#reverse_string ('Able was I ere I saw Elba',) {}
#wrapper 0.0
#wrapper has been used: 1x 
#ablE was I ere I saw elbA
#reverse_string ('A man, a plan, a canoe, pasta, heros, rajahs, a coloratura, maps, snipe, percale, macaroni, a gag, a banana bag, a tan, a tag, a banana bag again (or a camel), a crepe, pins, Spam, a rut, a Rolo, cash, a jar, sore hats, a peon, a canal: Panama!',) {}
#wrapper 0.0
#wrapper has been used: 2x
#!amanaP :lanac a ,noep a ,stah eros ,raj a ,hsac ,oloR a ,tur a ,mapS ,snip ,eperc a ,)lemac a ro( niaga gab ananab a ,gat a ,nat a ,gab ananab a ,gag a ,inoracam ,elacrep ,epins ,spam ,arutaroloc a ,shajar ,soreh ,atsap ,eonac a ,nalp a ,nam A
Of course the good thing with decorators is that you can use them right away on almost anything without rewriting. DRY, I said:

@counter
@benchmark
@logging
def get_random_futurama_quote():
    import httplib
    conn = httplib.HTTPConnection("slashdot.org:80")
    conn.request("HEAD", "/index.html")
    for key, value in conn.getresponse().getheaders():
        if key.startswith("x-b") or key.startswith("x-f"):
            return value
    return "No, I'm     doesn't!"

print get_random_futurama_quote()
print get_random_futurama_quote()

#outputs:
#get_random_futurama_quote () {}
#wrapper 0.02
#wrapper has been used: 1x
#The laws of science be a harsh mistress.
#get_random_futurama_quote () {}
#wrapper 0.01
#wrapper has been used: 2x
#Curse you, merciful Poseidon!
Python itself provides several decorators: property, staticmethod, etc. Django use decorators to manage caching and view permissions. Twisted to fake inlining asynchronous functions calls. This really is a large playground.

EDIT: given the success of this answer, and people asking me to do the same with metaclasses, I did.
The common patterns for decorators are:
Argument checking
Caching
Proxy
Context provider

1. Argument checking
Checking the arguments that a function receives or returns can be useful when it is
executed in a specific context. For example, if a function is to be called through
XML-RPC, Python will not be able to directly provide its full signature as in
the statically-typed languages.(to check the datatypes of argument wen pass/ret to a function)

A decorator can be used to fixed the allowed type of user defined types fixed a class which implemnets 
a decorator
# Dict to store the 
rpc_info = {} # dictionary of function name and tuple of Arguments 
def xmlrpc(in=(),out=(type(None)): #Wrapper Bread wd default arg tuple as in/out
	#register the Signature allowed 
	func_name= inspect.stack()[1][3] # Get the function name which is calling this decorator
	rpc_info[func_name]=(in,out) # the Global dict is Update Nameand args
	
	def _check_types(elements, types):
		"""Subfunction that checks the types."""
		if len(elements) != len(types):    #
			raise TypeError('argument count is wrong')
		typed = enumerate(izip(elements, types))#(int,string)(int,string)=(int,int)(string,string)
		
		for index, couple in typed:
			arg, of_the_right_type = couple
			if isinstance(arg, of_the_right_type):
				continue
			raise TypeError('arg #%d should be %s' % \(index, of_the_right_type)
	
	#Ingredeints  wrapper sits above sandvich 
	def __xmlrpc(*args): #args = (arg1,arg2)
		#Extracting the Args input
		input_args = arg[1:]
		_check_types(input_args,in) # Lets current arg list wd the fixed type 
		# Now free to call our Sandvich function with arguments
		res = function(*args) ## The function call is the name taken from above
		
		#Check the output as  a list or Tuple 
		if not Type(res) in [list,tuple]
			chck_res= (res,)
		else:
			chck_res=  res
		_check_type(chck_res)
		return res
	return __xmlrpc
return _xmlrpc

Lets write a class that uses this decorator 
class RPC(object):
	@xmlrpc((int,int))
	def meth1(int,int):
		print "received 2 ints as arg n ret type"

	@xmlrpc((int,),(int,))
	def meth2(int):
		print "received 1 ints as arg n ret type as int"
			
lets use the class 
my =RPC()
my.meth1(1,2)  # Runs OK 
my.meth2(1,2) #Throws an error 

2. Caching :
A simple decorator but focuses on those functions whose internal state does not affect the output. Each set of
arguments can be linked to a unique result. So is a good decorator to provide on CPU intensive functions,
the idea over here is to generate a key with args and preserve it for a particular duration 

cache = {} # this maintains a dict of the keys 
def is_obsolute(entry,duration):
	# thsi shuold return the cur time diffrence wd the func entry time 
	return (time.time() - entry['time']) > duration
	
def compute_key(function,args,kw):
	key = pickle.dumps((function.func_name,args,kw))
	return hashlib.sha1(key).hexdigest()
	
def memorize(dur=10):
	def _memorize(function):
		def __memorize(*args,**kw):
			key = compute_key(function,args,kw)
			# check the key exixts or not 
			if (key in cache) and not is_obsolute(cache[key],duration):
				return cache[key]['value']
			# computing 
			result = function(*args,**kw)
			#storing the result 
			cache[key]={'value': result, 'time': time.time()}
			return result
		return __memorize
	return _memorize

@memorize()
def add_it(a,b):
	return a + b
	
add_it(2,2) # 4 

Update the dur @memorize(2)

3. Proxy :
Proxy decorators are used to tag and register functions with a global mechanism,
For the use case we try to put a 

UserPerev = {}
class User(object):
	def __init__(self,user,role)
		self.role = role
		self.user = user
		UserPerev[self.user]=self.role
		
def protect(role):
	def _protect(object):
		def __protect(*args,**krgs):
			if role is None or @protect('admin')='Admin'
					throw an exception
			return (func(*args))
		return __protect
	return _protect		
	
	
Ram = User(Ram,'admin')

class Myprojcts():
		@protect('admin')
		def waffle_recipe(self):
			print 'use tons of butter!'
			
>>user = ram
>>these_are = Myprojcts()


4. Context Provider
------------------------------------

What is Multithreading?
=============================================================================
A thread is short for a thread of execution. A programmer can split his or her work
into threads that run simultaneously and share the same memory context. 
Multi-threading will benefit from a multiprocessor or multi-core
machine and will parallelize each thread execution on each CPU, thus making the
program faster.

How Python Deals with Threads
=============================
Python uses multiple kernel-level threads that can each run any of the interpreter-level 
threads. However, all threads accessing Python objects are serialized by one global lock. 
This is done because much of the interpreter code as well as third-party C code is not
thread-safe and need to be protected.
This mechanism is called the Global Interpreter Lock (GIL)

When threads contain only pure Python code, there is no point in using threads to
speed up the program since the GIL will serialize it. However, multiple threads can
do IO operations or execute C code in certain third-party extensions parallelly.

Use of threads can be really useful in some cases. They can help in:
Building responsive interfaces
Delegating work
Building multi-user applications

Building Responsive Interfaces
Let's say you ask your system to copy files from a folder to another through a
graphical user interface. The task will possibly be pushed into the background
and the windows will be constantly refreshed by the main thread, so you get live
feedback on the operation. You will also be able to cancel the operation.

Delegating Work
If your process depends on third-party resources, threads might really speed up
everything.Let's take the case of a function that indexes files in a folder and pushes the built
indexes into a database. Depending on the type of file, the function calls a different
external program.

Starting a New Thread
=============================================================================
import thread
import time

# Define a function for the thread
def print_time( threadName, delay):
   count = 0
   while count < 5:
      time.sleep(delay)
      count += 1
      print "%s: %s" % ( threadName, time.ctime(time.time()) )

# Create two threads as follows
try:
   thread.start_new_thread( print_time, ("Thread-1", 2, ) )
   thread.start_new_thread( print_time, ("Thread-2", 4, ) )
except:
   print "Error: unable to start thread"

while 1:
   pass
   
When the above code is executed, it produces the following result −

Thread-1: Thu Jan 22 15:42:17 2009
Thread-1: Thu Jan 22 15:42:19 2009
Thread-2: Thu Jan 22 15:42:19 2009
Thread-1: Thu Jan 22 15:42:21 2009
Thread-2: Thu Jan 22 15:42:23 2009
Thread-1: Thu Jan 22 15:42:23 2009
Thread-1: Thu Jan 22 15:42:25 2009

Creating Thread Using Threading Module
=============================================================================
To implement a new thread using the threading module, you have to do the following −

Define a new subclass of the Thread class.

Override the __init__(self [,args]) method to add additional arguments.

Then, override the run(self [,args]) method to implement what the thread should do when started.

Once you have created the new Thread subclass, you can create an instance of it and then start a new thread by
invoking the start(), which in turn calls run() method.

Example
#!/usr/bin/python

import threading
import time

exitFlag = 0

class myThread (threading.Thread):
    def __init__(self, threadID, name, counter):
        threading.Thread.__init__(self)
        self.threadID = threadID
        self.name = name
        self.counter = counter
    def run(self):
        print "Starting " + self.name
        print_time(self.name, self.counter, 5)
        print "Exiting " + self.name

def print_time(threadName, delay, counter):
    while counter:
        if exitFlag:
            thread.exit()
        time.sleep(delay)
        print "%s: %s" % (threadName, time.ctime(time.time()))
        counter -= 1

# Create new threads
thread1 = myThread(1, "Thread-1", 1)
thread2 = myThread(2, "Thread-2", 2)

# Start new Threads
thread1.start()
thread2.start()

print "Exiting Main Thread"
When the above code is executed, it produces the following result −

Starting Thread-1
Starting Thread-2
Exiting Main Thread
Thread-1: Thu Mar 21 09:10:03 2013
Thread-1: Thu Mar 21 09:10:04 2013
Thread-2: Thu Mar 21 09:10:04 2013
Thread-1: Thu Mar 21 09:10:05 2013
Thread-1: Thu Mar 21 09:10:06 2013
Thread-2: Thu Mar 21 09:10:06 2013
Thread-1: Thu Mar 21 09:10:07 2013
Exiting Thread-1
Thread-2: Thu Mar 21 09:10:08 2013
Thread-2: Thu Mar 21 09:10:10 2013
Thread-2: Thu Mar 21 09:10:12 2013
Exiting Thread-2


Synchronizing Threads:
=============================================================================
The threading module provided with Python includes a simple-to-implement locking mechanism that allows you to synchronize threads.
A new lock is created by calling the Lock() method, which returns the new lock.

The acquire(blocking) method of the new lock object is used to force threads to run synchronously. 
The optional blocking parameter enables you to control whether the thread waits to acquire the lock.

If blocking is set to 0, the thread returns immediately with a 0 value if the lock cannot be acquired and with a 1 if the lock was acquired.
 If blocking is set to 1, the thread blocks and wait for the lock to be released.

The release() method of the new lock object is used to release the lock when it is no longer required.

Example
#!/usr/bin/python

import threading
import time

class myThread (threading.Thread):
    def __init__(self, threadID, name, counter):
        threading.Thread.__init__(self)
        self.threadID = threadID
        self.name = name
        self.counter = counter
    def run(self):
        print "Starting " + self.name
        # Get lock to synchronize threads
        threadLock.acquire()
        print_time(self.name, self.counter, 3)
        # Free lock to release next thread
        threadLock.release()

def print_time(threadName, delay, counter):
    while counter:
        time.sleep(delay)
        print "%s: %s" % (threadName, time.ctime(time.time()))
        counter -= 1

threadLock = threading.Lock()
threads = []

# Create new threads
thread1 = myThread(1, "Thread-1", 1)
thread2 = myThread(2, "Thread-2", 2)

# Start new Threads
thread1.start()
thread2.start()

# Add threads to thread list
threads.append(thread1)
threads.append(thread2)

# Wait for all threads to complete
for t in threads:
    t.join()
print "Exiting Main Thread"

When the above code is executed, it produces the following result −

Starting Thread-1
Starting Thread-2
Thread-1: Thu Mar 21 09:11:28 2013
Thread-1: Thu Mar 21 09:11:29 2013
Thread-1: Thu Mar 21 09:11:30 2013
Thread-2: Thu Mar 21 09:11:32 2013
Thread-2: Thu Mar 21 09:11:34 2013
Thread-2: Thu Mar 21 09:11:36 2013
Exiting Main Thread
Multithreaded Priority Queue
=============================================================================
The Queue module allows you to create a new queue object that can hold a specific number of items. There are following methods to control the Queue −

get(): The get() removes and returns an item from the queue.

put(): The put adds item to a queue.

qsize() : The qsize() returns the number of items that are currently in the queue.

empty(): The empty( ) returns True if queue is empty; otherwise, False.

full(): the full() returns True if queue is full; otherwise, False.

Example
#!/usr/bin/python

import Queue
import threading
import time

exitFlag = 0

class myThread (threading.Thread):
    def __init__(self, threadID, name, q):
        threading.Thread.__init__(self)
        self.threadID = threadID
        self.name = name
        self.q = q
    def run(self):
        print "Starting " + self.name
        process_data(self.name, self.q)
        print "Exiting " + self.name

def process_data(threadName, q):
    while not exitFlag:
        queueLock.acquire()
        if not workQueue.empty():
            data = q.get()
            queueLock.release()
            print "%s processing %s" % (threadName, data)
        else:
            queueLock.release()
        time.sleep(1)

threadList = ["Thread-1", "Thread-2", "Thread-3"]
nameList = ["One", "Two", "Three", "Four", "Five"]
queueLock = threading.Lock()
workQueue = Queue.Queue(10)
threads = []
threadID = 1

# Create new threads
for tName in threadList:
    thread = myThread(threadID, tName, workQueue)
    thread.start()
    threads.append(thread)
    threadID += 1

# Fill the queue
queueLock.acquire()
for word in nameList:
    workQueue.put(word)
queueLock.release()

# Wait for queue to empty
while not workQueue.empty():
    pass

# Notify threads it's time to exit
exitFlag = 1

# Wait for all threads to complete
for t in threads:
    t.join()
print "Exiting Main Thread"
When the above code is executed, it produces the following result −

Starting Thread-1
Starting Thread-2
Starting Thread-3
Thread-1 processing One
Thread-2 processing Two
Thread-3 processing Three
Thread-1 processing Four
Thread-2 processing Five
Exiting Thread-3
Exiting Thread-1
Exiting Thread-2
Exiting Main Thread

Simple Example
=============================================================================
Let's take a small example of an application that recursively scans a directory to
process text files. Each text file is opened and processed by an external converter.
Using threads will possibly make it faster because the indexation work can be done
simultaneously on several files.
The external converter is a small Python program that does some complex work:
#!/usr/bin/python
	for i in range(100000):
	i = str(i) + "y"*10000


In a multi-threaded solution, the main program deals with a pool of threads. Each
thread takes its work from a queue. In this use case, threads are called workers. The
queue is the shared resource where the main program adds files it has found walking
in the directory.It provides a multi-consumer, multiproducer 
FIFO queue that internally uses a deque instance and is thread-safe.
So if we want to process files that are in that queue, we just use the get method
together with task_done, which lets the Queue instance know that the task has be
finished for join to work:
   
The worker function, which will be called through a thread, takes file names from
the queue and processes them by calling the index_file function. The sleep call
simulates the process done by an external program, and makes the thread wait for
the results, and therefore unlock the GIL.
The main program can then launch workers, scan for files to be processed, and feed
the queue with them.
This is done by creating Thread instances with the worker method. The setDaemon
is necessary so that the threads automatically get shut down when the program exits.
Otherwise, the program would hang forever waiting for them to exit. This can be
manually managed but it is not useful here.
At the end of the index_files function, the join method will wait for the queue to
be fully processed.
Let's create a full script called indexer.py that runs a multithreaded version, and a
single thread to index a directory structure containing text files:


from threading import Thread
import os
import subprocess
from Queue import Queue
import logging
import time
import sys
from pbp.scripts.profiler import profile, print_stats
dirname = os.path.realpath(os.path.dirname(__file__))
CONVERTER = os.path.join(dirname, 'converter.py')
q = Queue()

def index_file(filename):
	f = open(filename)
	try:
		content = f.read()
		# process is here
		subprocess.call([CONVERTER])
		finally:
		f.close()
		
def worker():
	while True:
		index_file(q.get())
		q.task_done()
		
def index_files(files, num_workers):
	for i in range(num_workers):
		t = Thread(target=worker)
		t.setDaemon(True)
		t.start()
		for file in files:
			q.put(file)
			q.join()
			
def get_text_files(dirname):
	for root, dirs, files in os.walk(dirname):
		for file in files:
			if os.path.splitext(file)[-1] != '.txt':
				continue
			yield os.path.join(root, file)
			
@profile('process')
def process(dirname, numthreads):
	dirname = os.path.realpath(dirname)
	if numthreads > 1:
		index_files(get_text_files(dirname), numthreads)
	else:
		for f in get_text_files(dirname):
			index_file(f)
			
if __name__ == '__main__':
	process(sys.argv[1], int(sys.argv[2]))
	print_stats()
	
	
This script can be used with any directory as long as it contains text files. It takes two
parameters:
1. The name of the directory
2. The number of threads
When name of the directory is usedalone, no threads are launched and the directory
is processed in the main thread.Let's run it directory containing 36 files with 19 text files.
The directory is composed of a structure of 6 directories:
$ python2 indexer.py zc.buildout-1.0.6-py2.5.egg 1
process : 301.83 kstones, 6.821 secondes, 396 bytes
$ python indexer.py zc.buildout-1.0.6-py2.5.egg 2
---------------------------------------------------------------------------------------------
Debugging in python:
=========================

PDB:

You can use the pdb module, insert pdb.set_trace() anywhere and it will function as a breakpoint.

>>> import pdb
>>> a="a string"
>>> pdb.set_trace()
--Return--
> <stdin>(1)<module>()->None
(Pdb) p a
'a string'
(Pdb)
To continue execution use c (or cont or continue).
It is possible to execute arbitrary Python expressions using pdb. For example, if you find a mistake, you can correct the code, 
then type a type expression to have the same effect in the running code .

PUDB:

python -m pudb.run my-script.py

Pyprocessing
=============
pyprocessing provides a portable way to work with processes as if they
were threads.


Caching
========
The result of a function or a method that is expensive to run can be cached as long as:
The function is deterministic and results have the same value every time,
given the same input.
The return value of the function continues to be useful and valid for some
period of time (non-deterministic).
Good candidates for caching are usually:
Results from callables that query databases
Results from callables that render static values, like file content, web
requests, or PDF rendering
Results from deterministic callables that perform complex calculations
Global mappings that keep track of values with expiration times, like web
session objects
Some data that needs to be accessed often and quickly
Deterministic Caching:
=====================
A simple example of deterministic caching is a function that calculates a square.
Keeping track of the results allows you to speed it up:
 import random, timeit
 from pbp.scripts.profiler import profile, print_stats
 cache = {}
 def square(n):
    return n * n
   
 @profile(‘not cached’)
    def factory_calls():
    for i in xrange(100):
    square(random.randint(1, 10))
   
 def cached_factory(n):
    if n not in cache:
    cache[n] = square(n)
    return cache[n]
   
 @profile(‘cached’)
    def cached_factory_calls():
    n = [random.randint(1, 10) for i in range(100)]
    ns = [cached_factory(i) for i in n]
   
 factory_calls(); cached_factory_calls();
 print_stats()
not cached : 20.51 kstones, 0.340 secondes, 396 bytes
cached : 6.07 kstones, 0.142 secondes, 480 bytes

Testing the Python Modules :
------------------------------------------
The standard workflow is:
1. You define your own class derived from unittest.TestCase.
2. Then you fill it with functions that start with ‘test_’.
3. You run the tests by placing unittest.main() in your file, usually at the bottom.

One of the many benifits of unittest, that you’ll use when your tests get bigger than the toy examples I’m showing on this blog,
 is the use of ‘setUp’ and ‘tearDown’ functions to get your system ready for the tests.

Like the doctest introduction, I’ll run through a simple example first, then show how I’m using unittest for testing markdown.py.

unittest example

Using the same unnecessary_math.py module that I wrote in the
doctest intro, here’s some example test
code to test my ‘multiply’ function.

test_um_unittest.py:

 
import unittest
from unnecessary_math import multiply

class TestUM(unittest.TestCase):

    def setUp(self):
        pass

    def test_numbers_3_4(self):
        self.assertEqual( multiply(3,4), 12)

    def test_strings_a_3(self):
        self.assertEqual( multiply('a',3), 'aaa')

if __name__ == '__main__':
    unittest.main()



In this example, I’ve used assertEqual(). The unittest framework has a whole bunch of assertBlah() style functions like assertEqual().
 Once you have a reasonable reference for all of the assert functions bookmarked, working with unnittest is pretty powerful and easy.

Aside from the tests you write, most of what you need to do can be accomplished with the test fixture methods such as setUp, tearDown,
 setUpClass, tearDownClass, etc.

Running unittests

At the bottom of the test file, we have this code:


 
if __name__ == '__main__':
    unittest.main()
This allows us to run all of the test code just by running the file.
Running it with no options is the most terse, and running with a ‘-v’ is more verbose, showing which tests
ran.



> python test_um_unittest.py
..
----------------------------------------------------------------------
Ran 2 tests in 0.000s

OK
> python test_um_unittest.py -v
test_numbers_3_4 (__main__.TestUM) ... ok
test_strings_a_3 (__main__.TestUM) ... ok

----------------------------------------------------------------------
Ran 2 tests in 0.000s
OK

OPTIMISATION OF APPLICATION S
	
Time Checks for Modules :
-------------------------------------------
Write a Speed Test

def test_speed():
   import time
   start = time.time()
   #The function to be tested 
   func()
   interval =time.time() - start
   assert interval > 10
Profiling CPU Usage:
=============================================================================
Macro-profiling: Profiles the whole program while it is being used, and
generates statistics.
Micro-profiling: Measures a precise part of the program by instrumenting
it manually.

Macro-profiling:
=======================
python -m cProfile myapp.py
1212 function calls in 10.120 CPU seconds
Ordered by: standard name
ncalls tottime cumtime percall file
1 0.000 10.117 10.117 myapp.py:16(main)
400 0.004 4.077 0.010 myapp.py:3(lighter)
200 0.002 2.035 0.010 myapp.py:6(light)
2 0.005 10.117 5.058 myapp.py:9(heavy)
3 0.000 0.000 0.000 {range}
602 10.106 10.106 0.017 {time.sleep}

Micro-Profiling:
=======================
When the slow function is found, it is sometimes necessary to do more profiling
work that tests just a part of the program. This is done by manually instrumenting a
part of the code in a speed test.
For instance, the cProfile module can be used from a decorator:
 import tempfile, os, cProfile, pstats
 def profile(column='time', list=5):
	 def _profile(function):
		def __profile(*args, **kw):
			s = tempfile.mktemp()
			profiler = cProfile.Profile
			profiler.runcall(function, *args, **kw)
			profiler.dump_stats(s)
			p = pstats.Stats(s)
			p.sort_stats(column).print_stats(list)
		return __profile
 	return _profile

 from myapp import main
 @profile('time', 6)
    def main_profiled():


Profiling Memory Usage:
=======================
Another problems is memory consumption. If a program that runs starts to eat so
much memory that the system swaps, there is probably a place in your application
where too many objects are created. This is often easy to detect through classical
profiling because consuming enough memory to make a system swap involves a lot
of CPU work that can be detected.

So objects that remain in memory are:
Global objects
Objects that are still referenced in some way

Profiling Memory:
=======================
Knowing how many objects are controlled by the garbage collector and their real
size is a bit tricky.



Profiling Network Usage:
=======================
As I said earlier, an application that communicates with third-party programs such
as a database or an LDAP server can be slowed down when those applications are
slow. This can be tracked with a regular code profiling method on the application
side. But if the third-party software works fine on its own, the culprit is probably
the network.






xml handling :
=================================
file handling:
=================================
f = open('workfile', 'w')
f.read()
>> 'This is the entire file.\n'
 f.readline()
'This is the first line of the file.\n'
 f.readline()
'Second line of the file\n'
 f.readline()
''
 f = open('workfile', 'r+')
 f.write('0123456789abcdef')
 f.seek(5)     # Go to the 6th byte in the file
 f.read(1)
'5'
 f.seek(-3, 2) # Go to the 3rd byte before the end
 f.read(1)
'd'


Reg Expresions:
======================================
import re

The \w = [a-zA-Z0-9_] // All alpha neumeric stuffs and Underline


a = 'My name is ShankarDD  12'

1) Search any 2 any chars after Shankar.

re.search('Shankar\w\w',a).group()  or re.search('Shankar[a-zA-Z0-9_][a-zA-Z0-9_]',a).group()

>> ShankarDD

For any digits use the \d symbol 



2) Match a symbol at the end ^/start $ of a word
re.search('Shankar[^a-zA-Z0-9_][^a-zA-Z0-9_]',a).group()
>> ShankarDD


3) // is used to match a /


4) Finding a email ID :

str = abc@google.com
	match = re.search('([\w.-]+)@([\w.-]+)', str)


\w is match for [^a-zA-Z0-9_]
. - is also added to the email usernme search

So the () @ () will devide the search in 2 groups divided b


So the ([\w.-]+) wil separate the user name with domain 

match.group(1)= abc
match.group(2)= google.com

--------------------------------------------------------------------------------------------|
Method/Attribute|     Purpose								    |
group()		|	Return the string matched by the RE				    |
start()		|	Return the starting position of the match			    |
end()		|	Return the ending position of the match				    |
span()		|	Return a tuple containing the (start, end) positions of the match   |
--------------------------------------------------------------------------------------------|


5)  Group match :


ating White Spaces from a Line Groupp search by Split or by find all:

line = 'test is all done '

pattern_space = re.split("(?:(?:[^a-zA-Z]+')|(?:'[^a-zA-Z]+))|(?:[^a-zA-Z']+)", line)
pattern_space = re.compile("([\w][\w]*'?\w?)").findall(line)


find all will return you a tuples of all  the matching componenst


Ex:
 p = re.compile('\d+')
 p.findall('12 drummers drumming, 11 pipers piping, 10 lords a-leaping')
['12', '11', '10']


The difference between ?= and ?! is that the former requires the given expression to match and the latter requires it to not match. For example a(?=b) will match the "a" in "ab", but not the "a" in "ac". Whereas a(?!b) will match the "a" in "ac", but not the "a" in "ab".

The difference between ?: and ?= is that ?= excludes the expression from the entire match while ?: just doesn't create a capturing group. So for example a(?:b) will match the "ab" in "abc", while a(?=b) will only match the "a" in "abc". a(b) would match the "ab" in "abc" and create a capture containing the "b".
Split :
----------
Split string by the occurrences of pattern. If capturing parentheses are used in pattern, then the text 
f all groups in the pattern are also returned as part of the resulting list. If maxsplit is nonzero, at
most maxsplit splits occur, and the remainder of the string is returned as the final element of the list


Matching Versus Searching
--------------------------------------------------
Python offers two different primitive operations based on regular expressions: match checks for a match only at the beginning of the string,
 while search checks for a match anywhere in the string (this is what Perl does by default).

Example
#!/usr/bin/python
import re

line = "Cats are smarter than dogs";

matchObj = re.match( r'dogs', line, re.M|re.I)
if matchObj:
   print "match --> matchObj.group() : ", matchObj.group()
else:
   print "No match!!"

searchObj = re.search( r'dogs', line, re.M|re.I)
if searchObj:
   print "search --> searchObj.group() : ", searchObj.group()
else:
   print "Nothing found!!"
When the above code is executed, it produces the following result −

No match!!
search --> matchObj.group() :  dogs

Search and Replace:
------------------------------------------------------------
#!/usr/bin/python
import re

phone = "2004-959-559 # This is Phone Number"

# Delete Python-styleWhen the above code is executed, it produces the following result −

Phone Num :  2004-959-559
Phone Num :  2004959559



comments
num = re.sub(r'#.*$', "", phone)
print "Phone Num : ", num

# Remove anything other than digits
num = re.sub(r'\D', "", phone)    
print "Phone Num : ", num



When the above code is executed, it produces the following result −

Phone Num :  2004-959-559
Phone Num :  2004959559

Modifier	Description
re.I	Performs case-insensitive matching.
re.L	Interprets words according to the current locale. This interpretation affects the alphabetic group (\w and \W), as well as word boundary behavior (\b and \B).
re.M	Makes $ match the end of a line (not just the end of the string) and makes ^ match the start of any line (not just the start of the string).


Pattern	Description
^	Matches beginning of line.
$	Matches end of line.
.	Matches any single character except newline. Using m option allows it to match newline as well.
[...]	Matches any single character in brackets.
[^...]	Matches any single character not in brackets



The Singleton
==================================
Possibly the simplest design pattern is the singleton, which is a way to provide one and only one object of a particular type. To accomplish this, 
you must take control
 of object creation out of the hands of the programmer. One convenient way to do this is to delegate to a single instance of a private nested inner class:

# Singleton/SingletonPattern.py

class OnlyOne:
    class __OnlyOne:
         def __init__(self, arg):
            self.val = arg
        def __str__(self):
            return repr(self) + self.val
    instance = None
    def __init__(self, arg):
        if not OnlyOne.instance:
            OnlyOne.instance = OnlyOne.__OnlyOne(arg)
        else:
            OnlyOne.instance.val = arg
    def __getattr__(self, name):
        return getattr(self.instance, name)

x = OnlyOne('sausage')
print(x)
y = OnlyOne('eggs')
print(y)
z = OnlyOne('spam')
print(z)
print(x)
print(y)
print(`x`)
print(`y`)
print(`z`)
output = '''
<__main__.__OnlyOne instance at 0076B7AC>sausage
<__main__.__OnlyOne instance at 0076B7AC>eggs
<__main__.__OnlyOne instance at 0076B7AC>spam
<__main__.__OnlyOne instance at 0076B7AC>spam




things to practice:
+++++++++++++++++++++++++++++++++

1. Tuple of list 
==================
first_lst = [('-2.50', 0.49, 0.52), ('-2.00', 0.52, 0.50)]
this is not the correct way and will throw a generator error
>> print first_lst 
>> <generator object <genexpr> at 0x02261288>

convert the tuple into all floats 
first_lst=[ tuple(float(y) for y in x) for x in first_lst ]

Now the other way (x for x in y) where y is a tuple is all allowed over here 


2. lamdas list and fetching Values 
====================================
from functools import partial
lambdas = [(lambda : i for i in range(3)]
 lambdas[0]()
2
 lambdas[1]()
2

Sol: 

The important thing to be understood here is, the functions are created during the evaluation of the list 
comprehension, but the value of i will be evaluated only during the execution of the functions.

So, in your case, you have created three functions and all of them refer
 i. When those functions are
 invoked at runtime, i will have the value ..
 
 2, because that was the last value bound to i in the last
 iteration of the for loop.
 
  lambdas = [lambda i=i: i for i in range(3)]
  Use a parameter with default value to bind the current value of i to a local variable. When the lambda
  gets called without an argument, the local variable i is assigned the default value
  
  
  
3. use of Arg and karg best expalined :
==============================================
Soln: 
class Foo():
    def __init__(self, data = False, *args, **kwargs):
        print kwargs, args, data

Foo()
Foo(data = True)
Foo(data = True, data1 = "Welcome")
Foo(True, 1, data1 = "Welcome")
# Foo(True, data1 = "Welcome", 1) # This will throw an error
Output

{} () False
{} () True
{'data1': 'Welcome'} () True
{'data1': 'Welcome'} (1,) True
In this example,

We don't pass a value to data, so default value (False) is assumed
We explicitly pass a value to data, so that value is taken
We explicitly pass a value to data and another keyword argument.
We explicitly pass a value to data, positional parameter and a keyword argument.
This will throw an error because no positional parameter should occur after a keyword argument. Order is very important

4. generator(tuple) vs lsit comprehension
==================================================
[myClass().Function(things) for things in biggerThing]
Function is a method, and it builds a list. The method itself doesn't return anything, but lists get manipulated within.

Now when I change it to a generator ,

(myClass().Function(things) for things in biggerThing)

Wont work 
Soln:

1). map(myClass().Function, biggerThing) 
map(function, iterable, ...)

Apply function to every item of iterable and return a list of the results. If additional iterable arguments are passed, function must take that many arguments and is applied to the items from all iterables in parallel. If one iterable is shorter than another it is assumed to be extended with None items. If function is None, the identity function is assumed; if there are multiple arguments, map() returns a list consisting of tuples containing the corresponding items from all iterables (a kind of transpose operation). The iterable arguments may be a sequence or any iterable object; the result is always a list.


2). 
Generators are evaluated on the fly, as they are consumed. So if you never iterate over a generator, its elements are never evaluated.
So, if you did:

for _ in (myClass().Function(things) for things in biggerThing):
    pass


5.create a dictionary with list comprehension
============================================================

Soln:

d = {key: value for (key, value) in iterable}
Of course, you can use the iterable in any way you want (tuples and lists literals, generator comprehensions, list comprehensions, 
generator functions, functional composition    feel creative) as long as each element is an iterable itself of two elements:

d = {value: foo(value) for value in sequence if bar(value)}

def key_value_gen(k):
   yield chr(k+65)
   yield chr((k+13)%26+65)
d = dict(map(key_value_gen, range(26)))

6.sort a list of dictionaries by values of the dictionary in Python?
===================================================================
[{'name':'Homer', 'age':39}, {'name':'Bart', 'age':10}]
sorted by name, should become

[{'name':'Bart', 'age':10}, {'name':'Homer', 'age':39}]


sol:
newlist = sorted(list_to_be_sorted, key=lambda k: k['name']) 

7.  list to make a dictionary
===================================
a = [1,2,3,4] and I want d = {1:0, 2:0, 3:0, 4:0}

sol:
zip can be usedinsid a dict to make a key value pair
a = [1,2,3,4] and I want d = {1:0, 2:0, 3:0, 4:0}


dict(zip(x ,[0 for x in range(0,len(x))]))


8. Merging two Dictionary :
==================================================================================
def merge_two_dicts(x, y):
    '''Given two dicts, merge them into a new dict as a shallow copy.'''
    z = x.copy()
    z.update(y)
    return z
x = {'a': 1, 'b': 2}
y = {'b': 3, 'c': 4}
y will come second and its values will replace x's values, thus 'b' will point to 3 in our final result.

9.Dict of Dicts :
==============================================================================================================
class AutoVivification(dict):
    """Implementation of perl's autovivification feature."""
    def __getitem__(self, item):
        try:
            return dict.__getitem__(self, item)
        except KeyError:
            value = self[item] = type(self)()
            return value
Testing:

a = AutoVivification()

a[1][2][3] = 4
a[1][3][3] = 5
a[1][2]['test'] = 6

print a
Output:

{1: {2: {'test': 6, 3: 4}, 3: {3: 5}}}

10. Best way to check a key in a dict:
==============================================================================================================
if 'key1' in dict:
  print "blah"
else:
  print "boo"
  
  the other methods will check for trhe whole dict 
   key = i % 10
    d[key] = d.get(key, 0) + 1
<<<<<<< HEAD
	
11. counting keyword in a strring 
================================================================
def name_freq(review_str,Names_list ):

    total = 0
    start = 0
    for keyword in Names_list:
       # print(keyword)
        a= re.search(keyword,review_str,re.IGNORECASE)
        if a is not None:
            matches = re.compile(keyword,re.IGNORECASE)
            while True:
                mo = matches.search(review_str,start)
              #  print(mo)
                if mo is None:
                    break
                start = 1 + mo.start()
                total = total +1
            print(a.group(),total)
        else :
            print('No matches found for '+ keyword)

    
11. Select the string split 
==============================================================================================================
 line = 'asdf fjdk; afed, fjek,asdf,      foo'
 import re
 re.split(r'[;,\s]\s*', line)
['asdf', 'fjdk', 'afed', 'fjek', 'asdf', 'foo']


>> fields = re.split(r'(;|,|\s)\s*', line)
 fields
['asdf', ' ', 'fjdk', ';', 'afed', ',', 'fjek', ',', 'asdf', ',', 'foo']
>>>>>>> b54797adae915e70034bfb835fe66a5c02bf182f

Map:
==============================================================================================
map executes the function given as the first argument on all the elements of the iterable given as the second argument.
 If the function given takes in more than 1 arguments, then many iterables are given.  #Follow the link to know more similar functions
For eg:

1
2
3
4
5
>>>a='ayush'
>>>map(ord,a)
....  [97, 121, 117, 115, 104]
>>> print map(lambda x, y: x*y**2, [1, 2, 3], [2, 4, 1])
....  [4, 32, 3]
