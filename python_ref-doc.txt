Python Basics:
================================================================================================================
code practice:
http://www.cse.msu.edu/~cse231/PracticeOfComputingUsingPython/

http://showmedo.com/learningpaths/12/view#id5  (BEST1)
http://www.saltycrane.com/blog/tag/python/

Installation:
+++++++++++++++++
C:\Python33

Edit the path and add the python in the path to launch it from any place 

Python is an Interpretted language
++++++++++++++++++++++++++++++++++
Interpreted : A high level language run and executed by an Interpreter(a program which converts the high-level language to machine code and 
then executing) 

Python and LUA are handled quite differently—interpreted rather than compiled. When you run a Python program 
(a file with an extension of .PY), it is not compiled, it is run. You could insert syntax errors into a function
in Python and unless that function is called, Python will never complain about the errors!
what happens wen a Python program is run :
-----------------------------------------------
1. when you execute a program Python first compiles your source code (the statements in your file) into a format known
	as byte code(byte code is a lower-level,platform-independent representation of your source code which is much fast i.e Pyc files)
	3.2 > version pyc will be in same dir after run 
	3.2=< version pyc in the __pycache module
	Process of interpreting :
	--------------------------
	There are four steps that python takes when you hit return: lexing, parsing, compiling, and interpreting.
	
	Lexing is breaking the line of code you just typed into tokens(a fixed symbol for an entity.)
	
	3) The parser takes those tokens and generates a structure that shows their relationship to each other
	 (in this case, an Abstract Syntax Tree). 

2. PVM is just a big code loop that iterates through
	your byte code instructions, one by one, to carry out their operations.

1) The compiler then takes the AST and turns it into one (or more) code objects.
Finally, the interpreter takes each code object executes the code it represents.

Python is dynamically but strongly typed:
+++++++++++++++++++++++++++++++++++++++++
The fact that the two halves of that statement fit together can confuse those who come from a static language type background.
 In Python it is perfectly legal to do this :

variable = 3
variable = 'hello'
So hasn't variable just changed type ? The answer is a resounding no. variable isn't an object at all - it's a name. In the first
 statement we create an integer object with the value 3 and bind the name 'variable' to it. In the second statement we create a
 new string object with the value hello, and rebind the name 'variable' to it. If there are no other names bound to the first object
 (or more properly no references to the object - not all references are names) then it's reference count will drop to zero and it 
 will be garbage collected

Only Called is compiled:
++++++++++++++++++++++++++++++++++
# Funny syntax error example

# Bad function!
def ErrorProne():

    printgobblegobble("Hello there!")

print("See, nothing bad happened. You worry too much!")

There is no function called printgobblegobble() in Python or in this program, so that should have generated an error! 
Here is the output:


See, nothing bad happened. You worry too much!


Python 3.x:
class MyClass(object): = new-style class
class MyClass: = new-style class (implicitly inherits from object)

Python 2.x:
class MyClass(object): = new-style class
class MyClass: = OLD-STYLE CLASS

python 3.x and 2.x
======================================
1. The print function
==================================================================================================================
Python 2
==================================================================================================================|
print 'Python', python_version()			|
print 'Hello, World!'
print('Hello, World!')
print "text", ; print 'print more text on the same line'
Python 2.7.6
Hello, World!
Hello, World!
text print more text on the same line
==================================================================================================================
==================================================================================================================
Python 3
==================================================================================================================|
print('Python', python_version())
print('Hello, World!')

print("some text,", end="")
print(' print more text on the same line')
Python 3.4.1
Hello, World!
some text, print more text on the same line
======================================
2. The Division
==================================================================================================================
Python 2
==================================================================================================================|
print '3 / 2 =', 3 / 2
3 / 2 = 1
==================================================================================================================
Python 3
==================================================================================================================
print '3 / 2 =', 3 / 2
3 / 2 = 1.5
3. Unicode
================================================================
Python 2 has ASCII str() types, separate unicode(), but no byte type.
Now, in Python 3, we finally have Unicode (utf-8) strings, and 2 byte classes: byte and bytearray
==================================================================================================================
Python 2
==================================================================================================================|
print type(unicode('this is like a python3 str type'))
<type 'unicode'>
print type(b'byte type does not exist')
<type 'str'>


==================================================================================================================
Python 3
==================================================================================================================|

print('Python', python_version())
print('strings are now utf-8 \u03BCnico\u0394é!')
Python 3.4.1
strings are now utf-8 μnicoΔé!
print('Python', python_version(), end="")
print(' has', type(b' bytes for storing data'))
Python 3.4.1 has <class 'bytes'>

4. xrange
================================================================
The usage of xrange() is very popular in Python 2.x for creating an iterable object, e.g., in a for-loop or list/set-dictionary-comprehension.
The behaviour was quite similar to a generator (i.e., "lazy evaluation"), but here the xrange-iterable is 
not exhaustible - meaning, you could iterate over it infinitely.

Thanks to its "lazy-evaluation", the advantage of the regular range() is that xrange() is generally faster
 if you have to iterate over it only once (e.g., in a for-loop). However, in contrast to 1-time iterations,
 it is not recommended if you repeat the iteration multiple times, since the generation happens every time
 from scratch!

In Python 3, the range() was implemented like the xrange() function so that a dedicated xrange() function does
 not exist anymore (xrange() raises a NameError in Python 3).

5. __contains__ 

 is that range got a "new" __contains__ method in Python 3.x (thanks to Yuchen Ying, who pointed this out).
 The __contains__ 
 method can speedup "look-ups" in Python 3.x range significantly for integer and Boolean types.
================================================================
 "proofs" that the __contain__ method wasn't added to Python 2.x yet:

print('Python', python_version())
range.__contains__
Python 3.4.1
<slot wrapper '__contains__' of 'range' objects>
print 'Python', python_version()
range.__contains__
Python 2.7.7
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
 in ()
      1 print 'Python', python_version()
----> 2 range.__contains__

AttributeError: 'builtin_function_or_method' object has no attribute '__contains__'

Raising exceptions
--------------------------
Python 3.4.1 Errors are objects now 
raise IOError, "file error"
o/p:
raise IOError, "file error"                     ^
SyntaxError: invalid syntax

raise IOError("file error")

Handling exceptions
======================
Also the handling of exceptions has slightly changed in Python 3. In Python 3 we have to use the "as" keyword now   
try:
    let_us_cause_a_NameError
except NameError as err:
    print(err, '--> our error message'

next in generator
Only in 3.x 
next(my_generator) is ok 
my_generator.next() WILL RESULT in error 

Parsing user inputs via input()
==============================================

Fortunately, the input() function was fixed in Python 3 so that it always stores the user inputs as str objects.
In order to avoid the dangerous behavior in Python 2 to read in other types than strings, we have to use raw_input() instead.

New style objects have a different object model to classic objects, and some things won't work properly with
 old style objects, for instance,
 super(), @property and descrpnt
 
Explanation:
Class :
-----------------------------
The classes and the derived thing 
> subclasing the built in types
> Acessing methods from the super class
> Slots 
> Meta-programing


The __prepare__ method (Prepare_method_100)
======================================
Metaclass can be any callable type , Now in order to fully exploate metaclas functionality an extra atribute 
named __prepare__ is called during the class pre-construction .
def prepare_class(name, *bases, metaclass=None, **kwargs):
  if metaclass is None:
     metaclass = compute_default_metaclass(bases)
  prepare = getattr(metaclass, '__prepare__', None)
  if prepare is not None:
     return prepare(name, bases, **kwargs)
  else:
     return dict()


This prepare method is called before the class body is executed and it must return a dictionary-like object 
that's used as the local namespace for all the code from the class body. It was added in Python 3.0, 

If your __prepare__ returns an object x then this:

class Class(metaclass=Meta):
    a = 1
    b = 2
    c = 3
Will make the following changes to x:

x['a'] = 1
x['b'] = 2
x['c'] = 3
This x object needs to look like a dictionary. Note that this x object will end up as an argument to Metaclass.__new__ and 
if it's not an instance of dict you need to convert it before calling super().__new__. [21]

So basically when an object is created 

1. Metaclass {__Call__} is called off (__call__ will call Class.__new__.)
2. __new__ (and if it returned an instance of Class it will also call Class.__init__ on it)
3. __init__


Creating a class
1. Metaclass.__prepare__ just returns the namespace object (a dictionary-like object
2. Metaclass.__new__ returns the Class object.

Data Types: 
=====================================
Python also provides some built-in data types,here are the 6 built in types 

1. dict:(association)
*********
A dictionary is mutable and is another container type that can store any number of Python objects,
including other container types. Dictionaries consist of pairs (called items) of keys and their
corresponding values.

Python dictionaries are also known as associative arrays or hash tables. The general syntax of a
dictionary is as follows:
=======

he following examples all return a dictionary equal to {"one": 1, "two": 2, "three": 3}:

>>> a = dict(one=1, two=2, three=3)
>>> b = {'one': 1, 'two': 2, 'three': 3}
>>> c = dict(zip(['one', 'two', 'three'], [1, 2, 3]))
>>> d = dict([('two', 2), ('one', 1), ('three', 3)])
>>> e = dict({'three': 3, 'one': 1, 'two': 2})
>>> a == b == c == d == e
True
dict['Age'] = 8; # update existing entry
dict['School'] = "DPS School"; # Add new ent
Each key is separated from its value by a colon (:), the items are separated by commas, and the whole
thing is enclosed in curly braces. An empty dictionary without any items is written with just two 
curly braces, like this: {}.

Accessing Values in Dictionary:
---------------------------------
print "dict['Name']: ", dict['Name'];
print "dict['Age']: ", dict['Age'];
for k in {"x":1,"y":3}:
    print(k)

	x 
	y  or Y
		  x
Virtually all Python implementations use hash maps to allow extremely fast access to the dictionary's values by key
But the order they're stored in has nothing to do with the order that they're added. They are stored in the order
that makes it easiest to look them up. Python dictionaries work the same way 

Fetching the Keys and Values:
----------------------------
Now methods to dictionaries that return different kinds of iterators explicitly:
for key in dict.iterkeys(): or keys for 3.x

for value in dict.itervalues():or values for 3.x

for key, value in dict.iteritems() or items

Delete Dictionary Elements:
------------------------------
del dict['Name']; # remove entry with key 'Name'
dict.clear();     # remove all entries in dict
del dict ;        # delete entire dictionary

Key properties :
------------------
a) More than one entry per key not allowed. Which means no duplicate key is allowed. When duplicate keys 
encountered during assignment, the last assignment wins.
b) Keys not present when searched results in Error
b) Tuples can be used as Dict keys but not Lists 

Dictionary Update :
----------------------------
dict = {'Name': 'Zara', 'Age': 7}
dict2 = {'Sex': 'female' }

dict.update(dict2)
print "Value : %s" %  dict

----------------------------
dict[new_key] = dict[old_key]
del dict[old_key]
Or in 1 step:

dict[new_key] = dict.pop(old_key)

Dictionary Comprehensions (Only = after 3.0)
----------------------------
dict1 = {value : str(value) for value in range(3) }
>> {'1':1,'2':2,'3':3}


Lists  not as Dictionary Keys
----------------------------
That said, the simple answer to why lists cannot be used as dictionary keys is that lists do not provide a valid __hash__ method. Of course, 
the obvious question is, "Why not?"

Consider what kinds of hash functions could be provided for lists.

If lists hashed by id, this would certainly be valid given Python's definition of a hash function -- lists with different hash values 
would have different ids. But lists are containers, and most other operations on them deal with them as such. So hashing lists by their id 
instead would produce unexpected behavior such as:

Looking up different lists with the same contents would produce different results, even though comparing lists with the same contents would indicate them as equivalent.
Using a list literal in a dictionary lookup would be pointless -- it would always produce a KeyError.

Splicing :
-------------------
>>> p=[1,2,3,4,5]
>>> p[1:]  ## leave the 1st element 
[2, 3, 4, 5]
>>> p[:1]   ## Include the 1st element
[1]
>>> p[1:1] ## Include the 1st ele and Leave the 1st elem
[]
>>> p[::1] ## All elements in the order 
[1, 2, 3, 4, 5]
>>> p[-1:] ## Include the last element 
[5]
>>> p[:-1] ## Leave the last element 
[1, 2, 3, 4]
>>> p[::-1] ## reverse the list 
[5, 4, 3, 2, 1]
>>>p[::2] ## every 2nd element frm start 
[1, 3, 5]

Tuples   as Dictionary Keys
----------------------------

Note that since tuples are immutable, they do not run into the troubles of lists - they can be hashed by their contents without worries about modification. Thus,
 in Python, they provide a valid __hash__ method, and are thus usable as dictionary keys.
--------------------------------------------------------------------------------------



2. list:(collection) [Can be used as Generator and comprehnesion]
*********************
The list is a most versatile datatype available in Python  Good thing about a list is that items in a list 
need not all have the same type.
Creating a list is putting different comma-separated values between squere brackets. For example:

list1 = ['physics', 'chemistry', 1997, 2000];
Basic List Operations:
Lists respond to the + and * operators much like strings; they mean concatenation and repetition here too, except that the result is a new list, not a string.

In fact, lists respond to all of the general sequence operations we used on strings in the prior chapter.

Python Expression	Results	 Description
----------------------------------------------
len([1, 2, 3])	3	Length
[1, 2, 3] + [4, 5, 6]	[1, 2, 3, 4, 5, 6]	Concatenation
['Hi!'] * 4	['Hi!', 'Hi!', 'Hi!', 'Hi!']	Repetition
3 in [1, 2, 3]	True	Membership
for x in [1, 2, 3]: print x,	1 2 3	Iteration

list[:-1]  Gives the list except the last one element Splicing


Unique Python use of list:
--------------------------

1. Filter a number range
[x for x in range(1,1000) if x % 3 == 0 or x % 5 == 0]


2. square up a list 
a = [1,2,3]
b= [ i **2 for  i in a]

2. Reverse a list 
g = "abcd"
g[::-1]
>> dcba

It starts from the end and prints every element 
g[::-2]
>> db
It starts from the end and prints every next element  element

Use of enumerate
-------------------------------------
refactored in a list comprehension like this:
 def _treatment(pos, element):
    return '%d: %s' % (pos, element)
   
 seq = ["one", "two", "three"]
 [_treatment(i, el) for i, el in enumerate(seq)]
['0: one', '1: two', '2: three']


3. set:(unordered collection of distinct hashable objects and Mutable):
************************************************************************
Addition :
-----------
s = {'jack', 'sjoerd'}##in addition to the set constructor.
s = set(['jack', 'sjoerd'])# Put a List 
s = set(('jack', 'sjoerd')) # Put a Tuple 

Comprehensions:
{str(val) for val in range(3)}
>>{1,2,3}
 

Properties:
-----------------
1. Like other collections, sets support x in set, len(set), and for x in set. Being an unordered collection,
sets do not record element position or order of insertion. Accordingly, sets do not support indexing, slicing, 
or other sequence-like behavior.

2. The set type is mutable — the contents can be changed using methods like add() and remove(). Since it is mutable,
 it has no hash value and cannot be used as either a dictionary key or as an element of another set.

 3. Instances of set are compared to instances of frozenset based on their members.
 For example, set('abc') == frozenset('abc') returns True and so does set('abc') in set([frozenset('abc')]).

 4 . sets has but not frozen sets 
 
 The following table lists operations available for set that do not apply to immutable instances of frozenset:
------------------------------------------------------------------------------
update(other, ...)															 |
set |= other | ...															 |
Update the set, adding elements from all others.							 |
------------------------------------------------------------------------------
intersection_update(other, ...)												 |
set &= other & ...															 |
Update the set, keeping only elements found in it and all others.			 |
-----------------------------------------------------------------------------
difference_update(other, ...)												 |
set -= other | ...															 |
Update the set, removing elements found in others.
-----------------------------------------------------------------------------|
symmetric_difference_update(other)
set ^= other
Update the set, keeping only elements found in either set, but not in both.	 |
-----------------------------------------------------------------------------|
add(elem)
Add element elem to the set.												 |
-----------------------------------------------------------------------------|
remove(elem)
Remove element elem from the set. Raises KeyError if elem is not contained in|
the set.																	 |
-----------------------------------------------------------------------------|
discard(elem)
Remove element elem from the set if it is present.
-----------------------------------------------------------------------------|
pop()
Remove and return an arbitrary element from the set. Raises KeyError if the set|
 is empty.
-----------------------------------------------------------------------------
clear()																		 |
Remove all elements from the set.											 |
-----------------------------------------------------------------------------|
 
 
 
The sets  provides classes for constructing and manipulating unordered collections of unique elements.
Common uses include membership testing, removing duplicates from a sequence, and computing standard math
operations on sets such as intersection, union, difference, and symmetric difference

 from sets import Set
 engineers = Set(['John', 'Jane', 'Jack', 'Janice'])
 programmers = Set(['Jack', 'Sam', 'Susan', 'Janice'])
 managers = Set(['Jane', 'Jack', 'Susan', 'Zack'])
 employees = engineers | programmers | managers           # union
 engineering_management = engineers & managers            # intersection
 fulltime_management = managers - engineers - programmers # difference
 engineers.add('Marvin')                                  # add element

Spatialnet use of set 
# scan for referenced poles (strip off the 'ST' from start of ID as not present in test db)
		referenced_pole_ids = set([staging_ent.attributes["POLE_ID"][2:]
								for staging_ent in layer_entities_map.get("Drops",[]) 
								if staging_ent.attributes.get("POLE_ID")])
here a set of unique pole ids referenced from a Drops can be drawn up 

Combining two Iterables  (Generators): 
------------------------------------------------
>chain from Itertools: Joins two Generator to form a Third one 
itertools.chain(range(3),range(3,6))


3.frozenset:(ordered Collection Immutable)
******************************************
 The frozenset type is immutable and hashable — its contents cannot be altered after it is created;
 it can therefore be used as a dictionary key or as an element of another set.

Supports all set properties except add n remove
cities = frozenset(["Frankfurt", "Basel","Freiburg"])
>>> cities.add("Strasbourg")
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
AttributeError: 'frozenset' object has no attribute 'add'
>>> 



4. tuple:
***********
It is a Immutable List and its fast in retriving and all it can be termed as an array
 A tuple is a sequence of immutable Python objects. Tuples are sequences, just like lists. The only difference
 is that 
 
 1) tuples can't be changed i.e., tuples are immutable 
 2) tuples use parentheses and lists use square brackets.

 tup1 = ('physics', 'chemistry', 1997, 2000);
 del tup; // delete tuples
	Empty and single tuples :
============================
	empty tuple is written as two parentheses containing nothing:
	tup1 = ();

To write a tuple containing a single value you have to include a comma, even though there is only one value:
	tup1 = (50,);
Like string indices, tuple indices start at 0, and tuples can be sliced, concatenated and so o
 
 
Updating Tuples: 
****************
# So let's create a new tuple as follows if you wana add something to tup1 make tup2 and concatenate in tup3
	tup3 = tup1 + tup2;
		print tup3;
 
Because tuples are sequences, indexing and slicing work the same way for tuples as they do for strings. Assuming following input:

	L = ('spam', 'Spam', 'SPAM!')
 

Python Expression	Results	 Description
	L[2]	'SPAM!'	Offsets start at zero
	L[-2]	'Spam'	Negative: count from the right
	L[1:]	['Spam', 'SPAM!']	Slicing fetches sections



5. str class (Used to hold Unicode strings):
*********************************************

6. bytes class (holding data in  binary data buffer ):

********************************************
A byte is an 8-bit value, equivalent to an integer from 0–255, and represents the raw bit patterns stored by the 
computer or transmitted over a network.
They are very similar to strings in use and support many of the same methods. The type names are spelled as byte and bytearray respectively.
Literal byte strings are represented in quotes preceded by the letter b. Byte strings are immutable.
Bytearrays are similar, but they are mutable.

Bytes objects are immutable sequences of single bytes. 
bytearray objects are a mutable counterpart to bytes objects. There is no dedicated literal syntax for bytearray
objects, instead they are always created by calling the constructor:

Creation :
------------
	1. Creating an empty instance: bytearray()
	2. Creating a zero-filled instance with a given length: bytearray(10)
		# Create bytearray and append integers as bytes.
		values = bytearray()
		values.append(0)
		values.append(1)
		values.append(2)
		print(values)

# Delete the first element.
del values[0:1]
print(values)
	3. From an iterable of integers: bytearray(range(20))
	
	1. As bytes are immutable so they have to be created only at the constructor level
		data = bytes([10, 20, 30, 40])
	2. Python that converts string, bytearray
	----------------------------------------------------------
	# Create a bytearray from a string with ASCII encoding. This converts a key encodes a value 
	arr = bytearray("abc", "ascii")
	print(arr)
	>>bytearray(b'abc') 
	# Convert bytearray back into a string.
	result = arr.decode("ascii")
	print(result)
	>> abc
	
	
Copying existing binary data via the buffer protocol: 
----------------------------------------------------
bytearray(b'Hi!')
> The string inside the byte Array always should be ascii 
> The number in the array should be from 0-255 inclusive of that 


****************************************
Bytes objects are immutable sequences of single bytes. Since many major binary protocols are based on the ASCII text encoding,
 bytes objects offer several methods that are only valid when working with ASCII compatible data
 and are closely related to string objects in a variety of other ways.
 b'1,2,3'

 
bytes.fromhex('2Ef0 F1f2  ')
b'.\xf0\xf1\xf2'

bytearray objects are a mutable counterpart to bytes objects. There is no dedicated literal syntax for bytearray objects,
instead they are always created by calling the constructor:
>>> bytearray.fromhex('2Ef0 F1f2  ')
bytearray(b'.\xf0\xf1\xf2')

Memory Views
-------------------------
memoryview objects allow Python code to access the internal data of an object that supports the buffer protocol without copying.
data = bytearray(b'abcefg')
>>> v = memoryview(data)
>>> v.readonly
False
>>> v[0] = ord(b'z')
>>> data
bytearray(b'zbcefg')
view = memoryview(b"abc")

# Print the first element.
print(view[0])
b'a'
# String Difference with Byte arrays 
view[0] == b'a' will returm a False 

# Print the element count.
print(len(view))

# Convert to a list.
print(view.tolist())

Output

b'a'
3
[97, 98, 99]

Performance. Suppose we want to append 256 values to a list. Bytearray here is faster.
 So we both improve memory size and reduce time required with bytearray over list.
So:
Bytearray is more complex to handle. It does not support non-ASCII characters or large numeric values.
But:
In many programs where these are not required, bytearray can be used to improve speed. This benchmark 
supports this idea.


7. files :
****************************************
Reading file contents 

fobj = open("ad_lesbiam.txt")
for line in fobj:
    print line.rstrip()
fobj.close()

Writing file contents
-------------------------------------
fobj_in = open("ad_lesbiam.txt")
fobj_out = open("ad_lesbiam2.txt","w")
i = 1
for line in fobj_in:
    print line.rstrip()
    fobj_out.write(str(i) + ": " + line)
    i = i + 1
fobj_in.close()
fobj_out.close()

o/p:
1: V. ad Lesbiam 
2: 
3: VIVAMUS mea Lesbia, atque amemus,
4: rumoresque senum seueriorum
5: omnes unius aestimemus assis! 

>>>>>>> origin/master

7. Numeric Types(int,float,complex)
never compare floats for equality. It generally doesn’t make sense. Still, in
many applications (such as computational geometry), you’d very much like to do just that. Instead, you
should check whether they are approximately equal. For example, you could take the approach of
assertAlmostEqual from the unittest module:
>>> def almost_equal(x, y, places=7):
... return round(abs(x-y), places) == 0
...
>>> almost_equal(sum(0.1 for i in range(10)), 1.0)
True




Specialized data types:
------------------------------------
1. datetime — Basic date and time types:[time_types_100.py]
*****************************************
Actually there two modules onehandles time(HH:MM:SS:MS) and other datetime handles the date(YY:MM:DD)

a) time
time objects hold the time information 
import time
time.time()  #returns the current system time in ticks since 12:00am, January 1, 1970(epoch)

#To get the Local time in Ticks
time.localtime(time.time()) #
>>> time.localtime(time.time())
 time.struct_time(tm_year=2015, tm_mon=10, tm_mday=31, tm_hour=23, tm_min=40, tm_
sec=37, tm_wday=5, tm_yday=304, tm_isdst=0)
  
  So this will present the full information now  
time.asctime( time.localtime(time.time()) )
 Tue Jan 13 10:17:09 2009
A date object 

import datetime

today = datetime.date.today()
print today
print 'ctime:', today.ctime()
print 'tuple:', today.timetuple()
print 'ordinal:', today.toordinal()
print 'Year:', today.year
print 'Mon :', today.month
print 'Day :', today.day
This example prints the current date in several formats:

2013-02-21
ctime: Thu Feb 21 00:00:00 2013
tuple: time.struct_time(tm_year=2013, tm_mon=2, tm_mday=21, tm_hour=0, tm_min=0, tm_sec=0, tm_wday=3, tm_yday=52, tm_isdst=-1)
ordinal: 734920
Year: 2013
Mon : 2
Day : 21

A timedelta object represents a duration, the difference between two dates or times.

Clock utility to measure the time taken to executte a program
===============================================================
start= clock()
file1=Excel('test1.csv')
end=clock()

time: end - start 
From  3.3, time.clock() is deprecated, and it's suggested to use time.process_time() or time.perf_counter() instead.

Previously in 2.7, according to the time module docs:

time.clock()

On Unix, return the current processor time as a floating point number expressed in seconds. The precision, and in fact the very definition of the meaning of “processor time”, depends on that of the C function of the same name, but in any case, this is the function to use for benchmarking Python or timing algorithms.

On Windows, this function returns wall-clock seconds elapsed since the first call to this function, as a floating point number, based on the Win32 function QueryPerformanceCounter(). The resolution is typically better than one microsecond.

So it provides Diffrent timings as per OS


2. collections — Container datatypes:
*****************************************
import collections

-------------------------------------------------------------------------------------------------
collections.nametuple()

Named tuples assign meaning to each position in a tuple and allow for more readable, self-documenting code.
 They can be used wherever regular tuples are used, and they add the ability to access fields by name 
 instead of position index.

>>> # Basic example
>>> Point = namedtuple('Point', ['x', 'y'])
>>> p = Point(11, y=22)     # instantiate with positional or keyword arguments
>>> p[0] + p[1]             # indexable like the plain tuple (11, 22)
33
>>> x, y = p                # unpack like a regular tuple
>>> x, y
(11, 22)
>>> p.x + p.y               # fields also accessible by name
33
>>> p                       # readable __repr__ with a name=value style
Point(x=11, y=22)
Named tuples are especially useful for assigning field names to result tuples returned by the csv or sqlite3 modules:

EmployeeRecord = namedtuple('EmployeeRecord', 'name, age, title, department, paygrade')

import csv
for emp in map(EmployeeRecord._make, csv.reader(open("employees.csv", "rb"))):
    print(emp.name, emp.title)

import sqlite3
conn = sqlite3.connect('/companydata')
cursor = conn.cursor()
cursor.execute('SELECT name, age, title, department, paygrade FROM employees')
for emp in map(EmployeeRecord._make, cursor.fetchall()):
    print(emp.name, emp.title)
In addition to the methods inherited from tuples, named tuples support three additional methods and two attributes.
 To prevent conflicts with field names, the method and attribute names start with an underscore.

classmethod somenamedtuple._make(iterable)
Class method that makes a new instance from an existing sequence or iterable.

>>>
>>> t = [11, 22]
>>> Point._make(t)
Point(x=11, y=22)
somenamedtuple._asdict()
Return a new OrderedDict which maps field names to their corresponding values:

>>>
>>> p = Point(x=11, y=22)
>>> p._asdict()
OrderedDict([('x', 11), ('y', 22)])
Changed in version 3.1: Returns an OrderedDict instead of a regular dict.

somenamedtuple._replace(kwargs)
Return a new instance of the named tuple replacing specified fields with new values:

>>>
>>> p = Point(x=11, y=22)
>>> p._replace(x=33)
Point(x=33, y=22)

>>> for partnum, record in inventory.items():
...     inventory[partnum] = record._replace(price=newprices[partnum], timestamp=time.now())
somenamedtuple._source
A string with the pure Python source code used to create the named tuple class. The source makes the named tuple self-documenting. It can be printed, executed using exec(), or saved to a file and imported.

New in version 3.3.

somenamedtuple._fields
Tuple of strings listing the field names. Useful for introspection and for creating new named tuple types from existing named tuples.

>>>
>>> p._fields            # view the field names
('x', 'y')

>>> Color = namedtuple('Color', 'red green blue')
>>> Pixel = namedtuple('Pixel', Point._fields + Color._fields)
>>> Pixel(11, 22, 128, 255, 0)
Pixel(x=11, y=22, red=128, green=255, blue=0)
To retrieve a field whose name is stored in a string, use the getattr() function:

>>>
>>> getattr(p, 'x')
11
To convert a dictionary to a named tuple, use the double-star-operator (as described in Unpacking Argument Lists):

>>>
>>> d = {'x': 11, 'y': 22}
>>> Point(**d)
Point(x=11, y=22)
Since a named tuple is a regular Python class, it is easy to add or change functionality with a subclass. Here is how to add a calculated field and a fixed-width print format:

>>>
>>> class Point(namedtuple('Point', 'x y')):
    __slots__ = ()
    @property
    def hypot(self):
        return (self.x ** 2 + self.y ** 2) ** 0.5
    def __str__(self):
        return 'Point: x=%6.3f  y=%6.3f  hypot=%6.3f' % (self.x, self.y, self.hypot)
>>>
>>> for p in Point(3, 4), Point(14, 5/7):
    print(p)
Point: x= 3.000  y= 4.000  hypot= 5.000
Point: x=14.000  y= 0.714  hypot=14.018
The subclass shown above sets __slots__ to an empty tuple. This helps keep memory requirements low by preventing the creation of instance dictionaries.

Subclassing is not useful for adding new, stored fields. Instead, simply create a new named tuple type from the _fields attribute:
-------------------------------------------------------------------------------------------------
OrderedDict objects
Ordered dictionaries are just like regular dictionaries but they remember the order that items were inserted. When iterating over an ordered dictionary, the items are returned in the order their keys were first added.

class collections.OrderedDict([items])
Return an instance of a dict subclass, supporting the usual dict methods. An OrderedDict is a dict that remembers the order that keys were first inserted. If a new entry overwrites an existing entry, the original insertion position is left unchanged. Deleting an entry and reinserting it will move it to the end.

New in version 3.1.

popitem(last=True)
The popitem() method for ordered dictionaries returns and removes a (key, value) pair. The pairs are returned in LIFO order if last is true or FIFO order if false.

move_to_end(key, last=True)
Move an existing key to either end of an ordered dictionary. The item is moved to the right end if last is true (the default) or to the beginning if last is false. Raises KeyError if the key does not exist:

>>>
>>> d = OrderedDict.fromkeys('abcde')
>>> d.move_to_end('b')
>>> ''.join(d.keys())
'acdeb'
>>> d.move_to_end('b', last=False)
>>> ''.join(d.keys())
'bacde'
New in version 3.2.

In addition to the usual mapping methods, ordered dictionaries also support reverse iteration using reversed().

Equality tests between OrderedDict objects are order-sensitive and are implemented as list(od1.items())==list(od2.items()). Equality tests between OrderedDict objects and other Mapping objects are order-insensitive like regular dictionaries. This allows OrderedDict objects to be substituted anywhere a regular dictionary is used.

The OrderedDict constructor and update() method both accept keyword arguments, but their order is lost because
 Python’s function call semantics pass in keyword arguments using a regular unordered dictionary.
 OrderedDict Examples and Recipes:
 -----------------------------------
Since an ordered dictionary remembers its insertion order, it can be used in conjunction with sorting to make a sorted dictionary:

>>>
>>> # regular unsorted dictionary
>>> d = {'banana': 3, 'apple':4, 'pear': 1, 'orange': 2}

>>> # dictionary sorted by key
>>> OrderedDict(sorted(d.items(), key=lambda t: t[0]))
OrderedDict([('apple', 4), ('banana', 3), ('orange', 2), ('pear', 1)])

>>> # dictionary sorted by value
>>> OrderedDict(sorted(d.items(), key=lambda t: t[1]))
OrderedDict([('pear', 1), ('orange', 2), ('banana', 3), ('apple', 4)])

>>> # dictionary sorted by length of the key string
>>> OrderedDict(sorted(d.items(), key=lambda t: len(t[0])))
OrderedDict([('pear', 1), ('apple', 4), ('orange', 2), ('banana', 3)])
The new sorted dictionaries maintain their sort order when entries are deleted. But when new keys are added, the keys are appended to the end and the sort is not maintained.

It is also straight-forward to create an ordered dictionary variant that remembers the order the keys were last inserted. If a new entry overwrites an existing entry, the original insertion position is changed and moved to the end:

class LastUpdatedOrderedDict(OrderedDict):
    'Store items in the order the keys were last added'

    def __setitem__(self, key, value):
        if key in self:
            del self[key]
        OrderedDict.__setitem__(self, key, value)
An ordered dictionary can be combined with the Counter class so that the counter remembers the order elements are first encountered:

class OrderedCounter(Counter, OrderedDict):
    'Counter that remembers the order elements are first encountered'

    def __repr__(self):
        return '%s(%r)' % (self.__class__.__name__, OrderedDict(self))

    def __reduce__(self):
        return self.__class__, (OrderedDict(self),)

-------------------------------------------------------------------------------------------------
A ChainMap class is provided for quickly linking a number of mappings so they can be treated as a single unit. 
It is often much faster than
 creating a new dictionary and running multiple update() calls.
 
 import os, argparse

defaults = {'color': 'red', 'user': 'guest'}

parser = argparse.ArgumentParser()
parser.add_argument('-u', '--user')
parser.add_argument('-c', '--color')
namespace = parser.parse_args()
command_line_args = {k:v for k, v in vars(namespace).items() if v}

combined = ChainMap(command_line_args, os.environ, defaults)
print(combined['color'])
print(combined['user'])
Example patterns for using the ChainMap class to simulate nested contexts:

c = ChainMap()        # Create root context
d = c.new_child()     # Create nested child context
e = c.new_child()     # Child of c, independent from d
e.maps[0]             # Current context dictionary -- like Python's locals()
e.maps[-1]            # Root context -- like Python's globals()
e.parents             # Enclosing context chain -- like Python's nonlocals

d['x']                # Get first key in the chain of contexts
d['x'] = 1            # Set value in current context
del d['x']            # Delete from current context
list(d)               # All nested values
k in d                # Check all nested values
len(d)                # Number of nested values
d.items()             # All nested items
dict(d)               # Flatten into a regular dictionary
The ChainMap class only makes updates (writes and deletions) to the first mapping in the chain while lookups 
will search the full chain. However, if deep writes and deletions are desired, it is easy to make a subclass 
that updates keys found deeper in the chain:

class DeepChainMap(ChainMap):
    'Variant of ChainMap that allows direct updates to inner scopes'

    def __setitem__(self, key, value):
        for mapping in self.maps:
            if key in mapping:
                mapping[key] = value
                return
        self.maps[0][key] = value

    def __delitem__(self, key):
        for mapping in self.maps:
            if key in mapping:
                del mapping[key]
                return
        raise KeyError(key)

>>> d = DeepChainMap({'zebra': 'black'}, {'elephant': 'blue'}, {'lion': 'yellow'})
>>> d['lion'] = 'orange'         # update an existing key two levels down
>>> d['snake'] = 'red'           # new keys get added to the topmost dict
>>> del d['elephant']            # remove an existing key one level down
DeepChainMap({'zebra': 'black', 'snake': 'red'}, {}, {'lion': 'orange'})
-------------------------------------------------------------------------------------------------
Deques are a generalization of stacks and queues (the name is pronounced “deck” and is short for “double-ended queue”). Deques support thread-safe,
 memory efficient appends and pops from either side of the deque with approximately the same O(1) performance in either direction.

Though list objects support similar operations, they are optimized for fast fixed-length operations and incur O(n) memory movement costs for pop(0)
 and insert(0, v) operations which change both the size and position of the underlying data representation.

If maxlen is not specified or is None, deques may grow to an arbitrary length. Otherwise, the deque is bounded to the specified maximum length.
 Once a bounded length deque is full, when new items are added, a corresponding number of items are discarded from the opposite end. Bounded length
 deques provide functionality similar to the tail filter in Unix.
They are also useful for tracking transactions and other pools of data where only the most recent activity is of interest.


>>> from collections import deque
>>> d = deque('ghi')                 # make a new deque with three items
>>> for elem in d:                   # iterate over the deque's elements
...     print(elem.upper())
G
H
I

>>> d.append('j')                    # add a new entry to the right side
>>> d.appendleft('f')                # add a new entry to the left side
>>> d                                # show the representation of the deque
deque(['f', 'g', 'h', 'i', 'j'])

>>> d.pop()                          # return and remove the rightmost item
'j'
>>> d.popleft()                      # return and remove the leftmost item
'f'
>>> list(d)                          # list the contents of the deque
['g', 'h', 'i']
>>> d[0]                             # peek at leftmost item
'g'
>>> d[-1]                            # peek at rightmost item
'i'

>>> list(reversed(d))                # list the contents of a deque in reverse
['i', 'h', 'g']
>>> 'h' in d                         # search the deque
True
>>> d.extend('jkl')                  # add multiple elements at once
>>> d
deque(['g', 'h', 'i', 'j', 'k', 'l'])
>>> d.rotate(1)                      # right rotation
>>> d
deque(['l', 'g', 'h', 'i', 'j', 'k'])
>>> d.rotate(-1)                     # left rotation
>>> d
deque(['g', 'h', 'i', 'j', 'k', 'l'])

>>> deque(reversed(d))               # make a new deque in reverse order
deque(['l', 'k', 'j', 'i', 'h', 'g'])
>>> d.clear()                        # empty the deque
>>> d.pop()                          # cannot pop from an empty deque
Traceback (most recent call last):
    File "<pyshell#6>", line 1, in -toplevel-
        d.pop()
IndexError: pop from an empty deque

>>> d.extendleft('abc')              # extendleft() reverses the input order
>>> d
deque(['c', 'b', 'a'])
Another approach to using deques is to maintain a sequence of recently added elements by appending to the right and popping to the left:

def moving_average(iterable, n=3):
    # moving_average([40, 30, 50, 46, 39, 44]) --> 40.0 42.0 45.0 43.0
    # http://en.wikipedia.org/wiki/Moving_average
    it = iter(iterable)
    d = deque(itertools.islice(it, n-1))
    d.appendleft(0)
    s = sum(d)
    for elem in it:
        s += elem - d.popleft()
        d.append(elem)
        yield s / n
The rotate() method provides a way to implement deque slicing and deletion. 
For example, a pure Python implementation of del d[n] relies on the rotate()
 method to position elements to be popped:

def delete_nth(d, n):
    d.rotate(-n)
    d.popleft()
    d.rotate(n)
-------------------------------------------------------------------------------------
defaultdict(list):
---------------------
>>> s = [('yellow', 1), ('blue', 2), ('yellow', 3), ('blue', 4), ('red', 1)]
>>> d = defaultdict(list)
>>> for k, v in s:
...     d[k].append(v)
...
>>> d.items()
[('blue', [2, 4]), ('red', [1]), ('yellow', [1, 3])]
When keys are encountered again, the look-up proceeds normally (returning the list for that key) and the list.append() operation adds another 
value to the list. 



-------------------------------------------------------------------------------------------------
-------------------------------------------------------------------------------------------------
-------------------------------------------------------------------------------------------------



-------------------------------------------------------------------------------------------------
3. collections.abc — Abstract Base Classes for Containers
*****************************************
This provides a bunch of containers which has already implemented methods and Abstarct methods nthat needs to 
be veriddesn this are basically usewd as mixins() 
The collections module offers the following ABCs:
------------------------------------------------------------------------------------------------------------
| ABC	| Inherits from   |	Abstract Methods (Needs to overidde)|	Mixin Methods(methods already provided)	|
------------------------------------------------------------------------------------------------------------
| Set  |  Sized, Iterable,| __contains__,__iter__,__len__		| __le__, __lt__, __eq__, __ne__, __gt__
|	   |Container		  |										| __ge__, __and__, __or__, isdisjoint
------------------------------------------------------------------------------------------------------------
class ListBasedSet(collections.abc.Set): [abc.cloocetions_100.py]
     ''' Alternate set implementation favoring space over speed
         and not requiring the set elements to be hashable. '''
     def __init__(self, iterable):
         self.elements = lst = []
         for value in iterable:
             if value not in lst:
                 lst.append(value)
				 
     def __iter__(self):
         return iter(self.elements)
     def __contains__(self, value):
         return value in self.elements
     def __len__(self):
         return len(self.elements)

s1 = ListBasedSet('abcdef')
s2 = ListBasedSet('defghi')
overlap = s1 & s2            # The __and__() method is supported automatically
For using mutable Sets(Sets where Append,remove )
1> Since some set operations create new sets, the default mixin methods need a way to create new instances from an iterable.
 The class constructor is assumed to have a signature in the form ClassName(iterable). That assumption is factored-out to an
 internal classmethod called _from_iterable() which calls cls(iterable) to produce a new set.
2> To override the comparisons (presumably for speed, as the semantics are fixed), redefine __le__() and __ge__(),
3> The Set mixin provides a _hash() method to compute a hash value for the set; however, __hash__() is not defined 
because not all sets are hashable or immutable. To add set hashability using mixins, inherit from both Set() and Hashable(),
then define __hash__ = Set._hash

Now similar to this Mapping,MutableMapping,Sequence,MutableSequence,

4. heapq — Heap queue algorithm
*****************************************
5. bisect — Array bisection algorithm
*****************************************
6. array — Efficient arrays of numeric values
*****************************************
This module defines an object type which can compactly represent an array of basic values: characters, integers, floating point
 numbers.
Differnce with the list:
> the  type of objects stored in them is constrained (Same type)
> the type is mentioned at the time of creation of the object
> Any other type Appending wil result in a type Error
> If the type code  are not Natching eith the Values  then Value error
typecode (must be b, B, u, h, H, i, I, l, L, q, Q, f or d)

from array import *
my_array = array('i', [1,2,3,4,5])
for i in my_array:
... print(i)
 my_array.insert(0,0)
  c=[11,12,13]
>>> my_array.fromlist(c)
>>> my_array
array('i', [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13])
 my_array.buffer_info()
(33881712, 12)
 my_array.count(11)
	



7. weakref — Weak references
*****************************************
A weak reference to an object is not enough to keep the object alive: when the only remaining references to a referent are 
weak references, garbage collection is free to destroy the referent and reuse its memory for something else. A primary use
for weak references is to implement caches or mappings holding large objects. 
The weakref module allows the Python programmer to create weak references to objects.
Creating a weak reference
import weakref
class test(object):
	pass
o = test()
#wekref object creation 
r = weakref.ref(o)
wekref_obj = r()
if weak_ref is None:
    # referent has been garbage collected
    print "Object has been deallocated; can't frobnicate."
else:
    print "Object is still live!"
    wekref_obj.do_something_useful()
	

WeakKeyDictionary and WeakValueDictionary use weak references in their implementation, setting up callback functions on the
weak references that notify the weak dictionaries when a key or value has been reclaimed by garbage collection. Most programs
should find that using one of these weak dictionary types is all they need – it’s not usually necessary to create your own weak
references dire
obj_dict = weakref.WeakValueDictionary()
obj_dict[key] = value
This example shows how a subclass of ref can be used to store additional information about an object and affect the value that’s
returned when the referent is accessed:

import weakref

class ExtendedRef(weakref.ref):
    def __init__(self, ob, callback=None, **annotations):
        super(ExtendedRef, self).__init__(ob, callback)
        self.__counter = 0
        for k, v in annotations.iteritems():
            setattr(self, k, v)

    def __call__(self):
        """Return a pair containing the referent and the number of
        times the reference has been called.
        """
        ob = super(ExtendedRef, self).__call__()
        if ob is not None:
            self.__counter += 1
            ob = (ob, self.__counter)
        return ob
		

Need of weak references:
------------------------
So now the reference which is a direct refernce 
	child.parent = self
The loop executing this will run and the refernce counting 
mechanism is Python and after that gc module will check i the 
refernce can be removed or not .
Now the weak reference implemntation is such that it happens in two steps 
1> weakref = self
2> child.parent = weakref
Now the gc will remove the count and make it call del on the same 

replace this in above code in parent 

	child.parent= weakref.ref(self)
and in child 
class Child:
# This is an extra caution that parent ref  exists or not 
	p = self.parent()
	if p is not None:
		# process p, the Parent instance
	else:
		# the parent instance was garbage collected.

So now we can see that 
>>> p = Parent2( Child(), Child() )
>>> del p
Removing Parent2 4303253584
Removing Child 4303256464
Removing Child 4303043344

		

8. types — Dynamic type creation and names for built-in types
*****************************************
9. copy — Shallow and deep copy operations
******************************************
Assignment statements in Python do not copy objects, they create bindings between a target and an object. 
>>> x = 3
>>> y = x
Now if we change y=z the memory is changed to some other Location
For collections that are mutable or contain mutable items, a copy is sometimes needed so one can change 
one copy without changing the other.

The difference between shallow and deep copying is only relevant for compound objects 
(objects that contain other objects, like lists or class instances):
Shallow copy:
--------------------
A shallow copy constructs a new compound object and then (to the extent possible) inserts references into 
it to the objects found in the original.
Deep copy :
--------------------
A deep copy constructs a new compound object and then, recursively, inserts copies into it of the objects 
found in the original.
Two problems often exist with deep copy operations that don’t exist with shallow copy operations:

Recursive objects (compound objects that, directly or indirectly, contain a reference to themselves) may
cause a recursive loop.
Because deep copy copies everything it may copy too much, e.g., administrative data structures that should 
be shared even between copies.
>>> x = 3
>>> y = x
Example :
you assign a new value to the 0th Element of one of the two lists, there will be no side effect. Problems arise,
 if you change one of the elements of the sublist.
>>> lst1 = ['a','b',['ab','ba']]
>>> lst2 = lst1[]
>>> lst2[0] = 'c'
>>> lst2[2][1] = 'd'
>>> print(lst1)
['a', 'b', ['ab', 'd']]
As the sublist memomry location is directly assigned to lst2 so the change in 1 will update 
the next one , So the deepcopy can be used to make a seperate memory for the sublist .

from copy import deepcopy

lst1 = ['a','b',['ab','ba']]

lst2 = deepcopy(lst1)

lst2[2][1] = "d"
lst2[0] = "c"
lsr2
['c', 'b', ['ab', 'd']]
lst1
['a', 'b', ['ab', 'ba']]


10. pprint — Data pretty printer
*****************************************
11. enum — Support for enumerations
*****************************************
Subclasing the built in types :
======================================
And allowed programmers to subclass the built-in types such as list, tuple, or dict. So every time a class
that behaves almost like one of the built-in types needs to be implemented, the best
practice is to subtype it.

How to get the methods in a class :
 import inspect
 inspect.getmembers(OptionParser, predicate=inspect.ismethod)
 inspect.getmembers(OptionParser, predicate=inspect.ismethod)
[([('__init__', <unbound method OptionParser.__init__>),
   
 ('add_option', <unbound method OptionParser.add_option>),
 ('add_option_group', <unbound method OptionParser.add_option_group>),
 ('add_options', <unbound method OptionParser.add_options>),
 
All classes have built in methods and user defined functions
The dir(class/instance) will list it out , we can seee __call__ , __imfunc__ are auto written fpr amy class
'__call__', '__class__', '__delattr__', '__dict__', '__doc__', '__get__',
 '__getattribute__', '__hash__', '__init__', '__module__', '__name__',
 '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__',
 '__str__', 'func_closure', 'func_code', 'func_defaults', 'func_dict',
 'func_doc', 'func_globals', 'func_name']
 
Lets describe this Methods :
> __repr__() and __str__() methods
**************************************************************************************************
 • Generally, the str() method representation of an object is commonly 
expected to be more friendly to humans. This is built by an object's __str__() method.
• The repr() method representation is often going to be more technical,
perhaps even a complete Python expression to rebuild the object.
• The print() function will use str() to prepare an object for printing.
• The format() method of a string can also access these methods. When we
use {!r} or {!s} formatting, we're requesting __repr__() or __str__(),
respectively.
The following is a simple

collection with both __str__() and __repr__() methods:
class Hand:
	def __init__( self, dealer_card, *cards ):
		self.dealer_card= dealer_card
		self.cards= list(cards)
	def __str__( self ):
		return ", ".join( map(str, self.cards) )
	def __repr__( self ):
		return "{__class__.__name__}({dealer_card!r}, {_cards_str})".
	format(	__class__=self.__class__,	_cards_str=", ".join( map(repr, self.cards) ),
	**self.__dict__ )
The __str__() method is a simple recipe, as follows:
1. Map str() to each item in the collection. This will create an iterator over the
resulting string values.
2. Use ", ".join() to merge all the item strings into a single, long string.
difference between __str__ and __repr__ 
-----------------------------------------:
log(INFO, "I am in the weird function and a is", a, "and b is", b, "but I got a null C — using default", default_c)
But you have to do the last step — make sure every object you implement has a useful repr, so code like that can just work.
This is why the “eval” thing comes up: if you have enough information so eval(repr(c))==c, that means you know everything 
there is to know about c. If that’s easy enough, at least in a fuzzy way, do it. 
If not, make sure you have enough information about c
 anyway. I usually use an eval-like format: 
 
 "MyClass(this=%r,that=%r)" % (self.this,self.that). It does not mean that you can actually 
 construct MyClass, or that those are the right constructor arguments — but it is a useful form to express “this is everything you need
 to know about this instance”.

Note: I used %r above, not %s. You always want to use repr() [or %r formatting character, equivalently] inside __repr__ 
implementation, or you’re defeating the goal of repr. You want to be able to differentiate MyClass(3) and MyClass("3").

The goal of __str__ is to be readable

Specifically, it is not intended to be unambiguous — notice that str(3)==str("3"). Likewise, if you implement an IP abstraction, having the 

str of it look like 192.168.1.1 is just fine. When implementing a date/time abstraction, the str can be "2010/4/12 15:35:22", etc. 
The goal is to represent it in a way that a user, not a programmer, would want to read it. Chop off useless digits, pretend to be some other class
 — as long is it supports readability, it is an improvement


> __format__()
**************************************************************************************************
The __format__() method is used by string.format() as well as the format()
built-in function. Both of these interfaces are used to get presentable string versions
of a given object.
The following are the two ways in which arguments will be presented
to __format__():
• someobject.__format__(""): This happens when the application
does format(someobject) or something equivalent to "{0}".
format(someobject). In these cases, a zero-length string specification was
provided. This should produce a default format.
• someobject.__format__(specification): This happens when the
application does format(someobject, specification) or something
equivalent to "{0:specification}".format(someobject).

__hash__()
**************************************************************************************************
 The built-in hash() function invokes the __hash__() method of a given object.
This hash is a calculation which reduces a (potentially complex) value to a small
integer value. Ideally, a hash reflects all the bits of the source value. Other hash
calculations—often used for cryptographic purposes—can produce very large values.

Deciding what to hash
Not every object should provide a hash value. Specifically, if we're creating a class of
stateful, mutable objects, the class should never return a hash value. The definition of
__hash__ should be None.
Immutable objects, on the other hand, might sensibly return a hash value so that
the object can be used as the key in a dictionary or a member of a set. In this case,
the hash value needs to parallel the way the test for equality works. It's bad to have
objects that claim to be equal and have different hash values. The reverse—objects
with the same hash that are actually not equal—is acceptable

There are three use cases for defining equality tests and hash values via the
__eq__() and __hash__() method functions:

Mutable objects: These are stateful objects that can be modified internally.
We have one design choice:
°° Define __eq__() but set __hash__ to None. These cannot be used as
dict keys or items in sets.

__bool__()
**************************************************************************************************
if some_object:
	process( some_object )
Under the hood, this is the job of the bool() built-in function. This function depends
on the __bool__() method of a given object.
The default __bool__() method returns True. We can see this with the
following code:
 x = object()
 bool(x)
True

But for the objects which will having lists that is all collection instances
, we might have something as shown in the following
code snippet:
def __bool__( self ):
	return bool( self._cards )
This delegates the Boolean function to the internal _cards collection.
If we're extending a list, we might have something as follows:
def __bool__( self ):
	return super().__bool__( self )
This delegates to the superclass definition of the __bool__() function.


__del__() method.
*************************************************************************************************
The intent is to give an object a chance to do any cleanup or finalization just before
the object is removed from memory. This use case is handled much more cleanly
by context manager objects and the with statement.

> In the case where a Python object has a related OS resource, the __del__() method
is a last chance to cleanly disentangle(An open file, a mounted device, or perhaps
a child subprocess)
Why context managers are preferred ?
------------------------------------
the __del__() method describes the circumstances as precarious and provides this
additional note on exception processing: exceptions that occur during their execution
are ignored, and a warning is printed to sys.stderr instead.
For these reasons, a context manager is often preferable to implementing __del__()

Reference count and Stories :
-----------------------------
CPython implementation, objects have a reference count. The count is
incremented when the object is assigned to a variable and decremented when the
variable is removed. When the reference count is zero, the object is no longer needed
and can be destroyed. For simple objects, __del__() will be invoked and the object
will be removed.
For complex objects that have circular references among objects, the reference count
might never go to zero and __del__() can't be invoked easily.
For the reference count to be 0 in a simple object 
class Noisy:
	def __del__( self ):
		print( "Removing {0}".format(id(self)) )
We can create (and see the removal of) these objects as follows:
>>> x= Noisy()
>>> del x
Removing 4313946640

>>> ln = [ Noisy(), Noisy() ]
>>> ln2= ln[:]
>>> del ln
There's no response to this del statement.
>>> del ln2
Removing 4313920336
Removing 4313920208
The ln2 variable was a shallow copy of the ln list. The Noisy objects were referenced
in two lists. They could not be destroyed until both lists were removed, reducing the
reference counts to zero.

Circular references and garbage collection:
-------------------------------------------
Each Child instance contains a reference to the Parent class.
We'll use these two classes to examine circular references:

class parent:
	def __init__(self,*children)
		self.childern = list(children)
		for child n childern:
			child.parent = self
	def __del__( self ):
		print( "Removing {__class__.__name__} {id:d}".
			format( __class__=self.__class__, id=id(self)) )

class Child:
	def __del__( self ):
		print( "Removing {__class__.__name__} {id:d}".
			format( __class__=self.__class__, id=id(self)) )
Each Child instance has a reference to the Parent class that contains it. The reference
is created during initialization when the children are inserted into the parent's
internal collection.

>>>> p = Parent( Child(), Child() )
>>> id(p)
4313921808
>>> del p

mutual or circular references, a Parent instance and its list of Child
instances cannot be removed from the memory. This references can seen by 
>>> import gc
>>> gc.collect()
174
>>>gc.garbage

[<__main__.Parent object at 0x101213910>, <__main__.Child object at
0x101213890>, <__main__.Child object at 0x101213650>..............]


gc module :
=====================================

Manual Garbage Collection
-------------------------------
For some programs, especially long running server applications or embedded applications running on a Digi Device
 automatic garbage collection may not be sufficient. Although an application should be written to be as free of
 reference cycles as possible, it is a good idea to have a strategy for how to deal with them. Invoking the
 garbage collector manually during opportune times of program execution can be a good idea on how to handle
 memory being consumed by reference cycles.
The garbage collection can be invoked manually in the following way:
import gc
gc.collect()
gc.collect() returns the number of objects it has collected and deallocated. You can print this information in 
the following way:

import gc
collected = gc.collect()
print "Garbage collector: collected %d objects." % (collected)
If we create a few cycles, we can see manual collection work:
import sys, gc
 
def make_cycle():
    l = { }
    l[0] = l
 
def main():
    collected = gc.collect()
    print "Garbage collector: collected %d objects." % (collected)
    print "Creating cycles..."
    for i in range(10):
        make_cycle()
    collected = gc.collect()
    print "Garbage collector: collected %d objects." %(collected)
 
if __name__ == "__main__":
    ret = main()
    sys.exit(ret)
	
In general there are two recommended strategies for performing manual garbage collection: time-based and
 event-based garbage collection. Time-based garbage collection is simple: the garbage collector is called
 on a fixed time interval. Event-based garbage collection calls the garbage collector on an event. For example,
 when a user disconnects from the application or when the application is known to enter an idle state.
 
Recommendations:
------------------------
Which garbage collection technique is correct for an application? It depends. The garbage collector should be
 invoked as often as necessary to collect cyclic references without affecting vital application performance. 
 Garbage collection should be a part of your Python application design process.
Do not run garbage collection too freely, as it can take considerable time to evaluate every memory object
 within a large system. For example, one team having memory issues tried calling gc.collect() between every step
 of a complex start-up process, increasing the boot time by 20 times (2000%). Running it more than a few times
 per day - without specific design reasons - is likely a waste of device resources.
1> Run manual garbage collection after your application has completed start up and moves into steady-state operation.
 This frees potentially huge blocks of memory used to open and parse file, to build and modify object lists, and
 even code modules never to be used again. For example, one application reading XML configuration files was
 consuming about 1.5MB of temporary memory during the process. Without manual garbage collection, there is no 
 way to predict when that 1.5MB of memory will be returned to the python memory pools for reuse.
2> Run manual garbage collection after infrequently run sections of code which use and then free large blocks of 
memory. For example, consider running garbage collection after a once-per-day task which evaluates thousands of 
data points, creates an XML 'report', and then sends that report to a central office via FTP or SMTP/email. One 
application doing such daily reports was creating over 800K worth of temporary sorted lists of historical data. 
Piggy-backing gc.collect() on such daily chores has the nice side-effect of running it once per day for 'free'.
Consider manually running garbage collection either before or after timing-critical sections of code to prevent
 garbage collection from disturbing the timing. As example, an irrigation application might sit idle for 10 minutes
 , then evaluate the status of all field devices and make adjustments. Since delays during system adjustment might
 affect field device battery life, it makes sense to manually run garbage collection as the gateway is entering
 the idle period AFTER the adjustment process - or run it every sixth or tenth idle period. This insures that 
 garbage collection won't be triggered automatically during the next timing-sensitive period.


Breaking of circular references:
------------------------------------
The __del__ method wont help here bacause it will get invoked once the 
circular reference gets broken

So we can break the circularity or use a weakref reference,
which permits garbage collection.

__new__()  auto-magically a static method
**********************************************************
The __new__() method is where our code can build an uninitialized
object. This allows processing before the __init__() method is called to set the
attribute values of the object

__new__ method to Update an Ommutable :
-----------------------------------------
The __new__() method is used to extend the immutable classes where the
__init__() method can't easily be overridden

class Float_Fail(float):
	def __init__(self,value,unit):
		super().__init__(value)
		slef.unit = unit 
		
The following is what happens when we try to use this class definition:
>>> s2 = Float_Fail( 6.5, "knots" )
Traceback (most recent call last):
File "<stdin>", line 1, in <module>
TypeError: float() takes at most 1 argument (2 given)

The default implementation of __new__() simply does this:
return super().__new__( cls ). It delegates the operation to the superclass.
The work winds up getting delegated to object.__new__(), which builds a simple,
empty object of the required class.

the Arguments which is passd to new will this new 2 exceptions
>When we want to subclass an immutable class definition. We'll dig into
 that later.
> When we need to create a metaclass.as it's fundamentally different from
 creating immutable objects.  

 The following is an example class definition that shows us the proper way to
extend float:
class Float_Units( float ):
	def __new__( cls, value, unit ):
		obj= super().__new__( cls, value )
		obj.unit= unit
		return obj
In the preceding code, we set
prepare
******************************************************************
class Ordered_Attributes(type):
@classmethod
def __prepare__(metacls, name, bases, **kwds):
return collections.OrderedDict()
def __new__(cls, name, bases, namespace, **kwds):
result = super().__new__(cls, name, bases, namespace)
result._order = tuple(n for n in namespace if not
n.startswith('__'))
return result
This class extends the built-in


__bytes__()
******************************************************************
In the most common situation, an application can create a string representation, and
the built-in encoding capabilities of the Python IO classes will be used to transform
the string into bytes.

 comparison operator methods
**************************************************************************************************
• x<y calls x.__lt__(y)
• x<=y calls x.__le__(y)
• x==y calls x.__eq__(y)
• x!=y calls x.__ne__(y)
• x>y calls x.__gt__(y)
• x>=y calls x.__ge__(y)

for example a Implementation of comparison for objects of
the same class
We'll look at a simple same-class comparison by looking at a more complete

class BlackJackCard:

def __init__( self, rank, suit, hard, soft ):
	self.rank= rank
	self.suit= suit
	self.hard= hard
	self.soft= soft
	
def __lt__( self, other ):
	if not isinstance( other, BlackJackCard ): 
		return NotImplemented
	return self.rank < other.rank
	
def __lt__( self, other ):
	try:
		return self.rank <= other.rank
	except AttributeError:
		return NotImplemented

	
First, the operand on the left is checked for an operator implementation:
A<B means A.__lt__(B).
Second, the operand on the right is checked for a reversed operator
implementation: A<B means B.__gt__(A).
The rare exception to this occurs when the right operand is a subclass
of the left operand; then, the right operand is checked first to allow a
subclass to override a superclass
>>> two = BlackJackCard_p( 2, '♠' )
>>> three = BlackJackCard_p( 3, '♠' )
>>> two < three
Compare 2♠ < 3♠
True
>>> two > three
Compare 3♠ < 2♠
False
>>> two == three
False

for two > three, there's no __gt__() method defined; Python uses
three.__lt__(two) as a fallback plan.
By default, the __eq__() method is inherited from object; it compares the object
IDs; the objects participate in == and != tests as follows:
>>> two_c = BlackJackCard_p( 2, '♣' )
>>> two == two_c
False
We can see that the results aren't quite what we expect. We'll often need to override
the default implementation of __eq__().
__setitem__:
******************************************************************
__setitem__ method call  This allows us to write Python
 statements like myZipTree['Plymouth'] = 55446, just like a Python dictionary.
def __setitem__(self,k,v):
    self.insert_value(k,v)
	
__getitem__:
**********************************************************************
y implementing the __getitem__ method we can write a Python statement that looks just 
like we are accessing a dictionary, when in fact we are using a binary search tree, 
for example 

	z = myZipTree['Fargo']
	
__contains__:
**********************************************************************
implement the in operation by writing a __contains__ method for the
BinarySearchTree. The __contains__ method will simply call get and return
def __contains__(self,key):
    if self._get(key,self.root):
        return True
    else:
        return False
		
Example : 
	if 'test' in TreeNode:


***********************************************************

For a function dir output gives the same as above 
The main use is to  inherit the base class for our use suppose the dict is inherited for inheritance 
and update it for a Distict key

class DistinctDict(dict):
	def __setitem__(self,key,value):
		try:
			value_index = self.values.index(value)
			existing_key = self.keys()[value_index]
			if existing_key != key:
				raise DistictError("")
		except valueError:
			pass
		super(distinctdict, self).__setitem__(key, value)
		
 my = distinctdict()
 my['key'] = 'value'
 my['other_key'] = 'value'

Super :
=====================
The super is called by the sub classes to update the  base class calls 
class Mama(object):
	def says(self):
		"please Go and read"
class Sister(Mama):
	def says(self):
		super(Sister,self).says()
		
This shows thw usw of te supewr class 

But there are several places when super cannot and should not be used 
For diamond inheritancs 

class A:
 mtd m
 
class B(A):
	pass 

class C(A):
	mtd m
 
class D(B,C):

d=D  
d.m # calls the Cmthd


Explanation:
-------------------------------------
The crucial difference between resolution order for legacy vs new-style classes comes when the same ancestor class occurs more
than once in the "naive", depth-first approach -- e.g., consider a "diamond inheritance" case:

>>> class A: x = 'a'
... 
>>> class B(A): pass
... 
>>> class C(A): x = 'c'
... 
>>> class D(B, C): pass
... 
>>> D.x
'a'
here, legacy-style, the resolution order is D - B - A - C - A : so when looking up D.x, A is the first base in resolution order to solve it, 
thereby hiding the definition in C. 

While:

>>> class A(object): x = 'a'
... 
>>> class B(A): pass
... 
>>> class C(A): x = 'c'
... 
>>> class D(B, C): pass
... 
>>> D.x
'c'
>>> 
here, new-style, the order is:

>>> D.__mro__
(<class '__main__.D'>, <class '__main__.B'>, <class '__main__.C'>, 
    <class '__main__.A'>, <type 'object'>)

with A forced to come in resolution order only once and after all of its subclasses, so that overrides
(i.e., C's override of member x) actually work sensibly.

It's one of the reasons that old-style classes should be avoided: multiple inheritance with "diamond-like" patterns just
doesn't work sensibly with them, while it does with new-style


Heterogeneous Arguments:
------------------------------
Another issue with super usage is argument passing in initialization. How can a
class call its base class __init__ code if it doesn't have the same signature? This
leads to the following problem:
 class BaseBase(object):
    def __init__(self):
    print 'basebase'
    super(BaseBase, self).__init__()
   
 class Base1(BaseBase):
    def __init__(self):
    print 'base1'
    super(Base1, self).__init__()
   
 class Base2(BaseBase):
    def __init__(self, arg):
    print 'base2'
    super(Base2, self).__init__()
   
 class MyClass(Base1 , Base2):
    def __init__(self, arg):
    print 'my base'
    super(MyClass, self).__init__(arg)
   
m = MyClass(10)
my base
Traceback (most recent call last):
File "<stdin>", line 1, in <module>
File "<stdin>", line 4, in __init__
TypeError: __init__() takes exactly 1 argument (2 given)


getattr function :
+++++++++++++++++++++++++++++++++++++=+++++++
>>> class Person():
    	name = 'Victor'
    	def say(self, what):
       		print(self.name, what)

>>> getattr(Person, 'name')
'Victor'
>>> attr_name = 'name'
>>> person = Person()
>>> getattr(person, attr_name)
'Victor'
>>> getattr(person, 'say')('Hello')
Victor Hello

difference between __getattr__ and __getattribute__ 
----------------------------------------------------------
A key difference between __getattr__ and __getattribute__ is that __getattr__ is only invoked if the attribute 
wasn't found the usual ways. 
It's good for implementing a fallback for missing attributes, and is probably the one of two you want.

__getattribute__ is invoked before looking at the actual attributes on the object, and so can be tricky to
 implement correctly.
 You can end up in infinite recursions very easily.
class A:
    def __init__(self,t):
        self.test=t
    '''
    def __getattribute__(self, item):
        print('x y z')
        return item
    '''
    def __getattr__(self, item):
        print('jkl')
        return item

if __name__ == '__main__':
    a = A(45)

    print(a.test)
    print(a.nf)
>>>>>

45
jkl
nf

if getattribute is uncommented the _getattr wont get called at all for not present atributes 
also

---------------------------------------------------------
Order of referring the methods :

There are many ways to access/change the value of self.foo:

1) direct access a.foo
2) inner dict a.__dict__['foo']
3) get and set a.__get__ and a.__set__,of course there two are pre-defined methods.
4) getattribute a.__getattribute__
5) __getattr__ and __setattr__

Class attribute lookup:
---------------------------------------------

class Test(object):
	self.foo
	
	
The match will happen in the following order 
   		(Metaclass:__getatribute__('foo'))
                           |
            (yes)  ---  <foo in metaclass dict>  ---  (no)
              |                         		 |
              |			   <foo in class dict>		
        metaclass dict        (no)			(yes)
        has set n get           |                        |
              |              
              |                 |                        |
              |              <foo in metaclass dict>     foo in class dict has get
              |			|			yes			no
              |		Raise exception			|		 	|
	return metaclas					
	dict foo bar				return dict_foo_get		return Metaclass:__dict_foo

Magic method lookup
---------------------------------------------
For magic methods the lookup is done on the class, directly in the big struct with the slots: [12]

Does the object's class have a slot for that magic method (roughly object->ob_type->tp_<magicmethod> in C code)? 
If yes, use it. If it's NULL then the operation is not supported.


OOPS in python :
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Here is an example:


class Bug(object):
    legs = 0
    distance = 0

    def __init__(self, name, legs):
        self.name = name
        self.legs = legs

    def Walk(self):
        self.distance += 1

    def ToString(self):
        return self.name + " has " + str(self.legs) + " legs" + \
            " and taken " + str(self.distance) + " steps."

-----------------------------------------------------------------
Polymorphism:
The term polymorph means “many forms” or “many shapes”, so polymorphism is the ability to take many forms or shapes.
In the context of a class, this means we can use methods with many shapes—that is, many different sets of parameters.
In Python, we can use optional parameters to make a method more versatile. The constructor of our new Bug class can 
be transformed with the use of optional parameters like so:


    def __init__(self, name="Bug", legs=6):
        self.name = name
        self.legs = legs


--------------------------------------------------------------------------------------
Data Hiding (Encapsulation):

Private Variables:
class P:
   def __init__(self, name, alias):
      self.name = name       # public
      self.__alias = alias   # private

   def who(self):
      print('name  : ', self.name)
      print('alias : ', self.__alias)

>>> x = P()	  
>>> x.name
'Alex'
>>> x.alias
Traceback (most recent call last):
  File "", line 1, in 
AttributeError: P instance has no attribute 'alias'
	  
x.__alias
Traceback (most recent call last):
  File "", line 1, in 
AttributeError: P instance has no attribute '__alias'

But here is a magic want. One underscore('_') with the class name will do magic:

>>> x._P__alias
'amen'

access or change the distance variable (which we would assume is private, even though it isn’t):


    def GetDistance(self):
       return p_distance

    def SetDistance(self, value):
        p_distance = value

From a data hiding point of view, you could rename distance to p_distance (making it appear to be a private variable), 
and then access it using these two methods. That is, if data hiding is important in your program.

Implementing a Private Function (Workaround method)
-------------------------------------------------------------
import re
import inspect

class MyClass :

    def __init__(self) :
        pass

    def private_function ( self ) :
        try :
            function_call = inspect.stack()[1][4][0].strip()

            # See if the function_call has "self." in the begining
            # For the function called from the inside the class = self.private-function() 
            # if the function is called outside the class wd an object then it will be obj.private-function() 
            matched = re.match( '^self\.', function_call )
            if not matched :
                print 'This is Private Function, Go Away'
                return
        except :
            print 'This is Private Function, Go Away'
            return

        # This is the real Function, only accessible inside class #
        print 'Hey, Welcome in to function'

    def public_function ( self ) :
        # i can call private function from inside the class
        self.private_function()

### End ###
Implementing a reference like in C++ Style
---------------------------------------------
in order to do this, you'll need to keep a reference to the parent object around.
class PropertyRef:
    def __init__(self, obj, prop_name):
        klass = obj.__class__
        prop = getattr(klass, prop_name)
        if not hasattr(prop, 'fget'):
            raise TypeError('%s is not a property of %r' % (prop_name, klass))
        self.get = lambda: prop.fget(obj)
        if getattr(prop, 'fset'):
            self.set = lambda value: prop.fset(obj, value))

class Foo(object):
    @property
    def bar(self):
        return some_calculated_value

>>> foo_instance = Foo()
>>> ref = PropertyRef(foo_instance, 'bar')
>>> ref.get()  # Returns some_calculated_value
>>> ref.set(some_other_value)
>>> ref.get()  # Returns some_other_value
BTW, in the example you gave, the property is read-only (doesn't have a setter), so you wouldn't be able to set it anyway.

If this feels like an unnecessary amount of code, it probably is -- I'm almost certain you can find a better solution to your use case, whatever it is.


--------------------------------------------------------------------------------------
Inheritance:

Python supports inheritance of base classes. When a class is defined, the base class is included in parentheses:


class Car(Vehicle):

In addition, Python supports multiple inheritance; that is, more than one parent or base class can be
 inherited from
in a child class. For example:


class Car(Body,Engine,Suspension,Interior):

As long as the variables and methods in each parent class do not conflict with each other, the new child class can 
access them all without incident. But, if there are any conflicts, the conflicted variable or method is used from the 
parent that comes first in the inheritance ordering.


Use of super() to resolve any confuiosns in polymorphism
class Point():
    x = 0.0
    y = 0.0

    def __init__(self, x, y):
        self.x = x
        self.y = y
        print("Point constructor")



class Circle(Point):
    radius = 0.0

    def __init__(self, x, y, radius):
        super().__init__(x,y)
        self.radius = radius
 
 c = Circle(100,100,50)
 
 Output :
 
 Point constructor
Circle constructor
--------------------------------------------------------------------------------------
Polymorphism:

overloading :
1) Operator overlaoding 

class Book:
    title = ''
    pages = 0

    def __init__(self, title='', pages=0):
        self.title = title
        self.pages = pages

    def __str__(self):
        return self.title
        
      
   a1 = book(Mybook1,100)
   a2 = book(Mybook2,100)
   
   Mybook1 + Mybook2 = 200 
   
   + implementation:
   ====================
    So if the first item doesn’t know how to add itself to 0, Python fails. But before it fails, Python tries to do a revered add with the operators.
    
    def __add__(self, other):
    return self.pages + other.pages
    
    We have to override __radd__, or “reverse add”.

def __radd__(self, other):
    return self.pages + other.pages
    
    Hash overideing for making a object as dictionary key :
    =======================================================
   
****
Python often wonder why, while the language includes both a tuple and a list type,
 tuples are usable as a dictionary keys, while lists are not.
 This was a deliberate design decision, and can best be explained by first understanding
 how Python dictionaries work.

The implicit superclass – object:
----------------------------------
Each Python class definition has an implicit superclass: object.
class definition that simply extends object with a new name:
class X:
	pass
The following are some interactions with our class:
>>> X.__class__
<class 'type'>
>>> X.__class__.__base__
<class 'object'>

Intialising variables on the Fly :
----------------------------------
x = X()
x.item_a =1
Although this is allowed in python thats nota good design always 

Object declaration :
*****************************
A = Classname(22,22)

Calling the function is same as C++ 

A.fumc("dewded",12)

--------------------------------------------------------------------------------------
Atribute managment:
=========================
The class when created supports following four behaviors with respect
to attributes:
• To create a new attribute by setting its value
• To set the value of an existing attribute
• To get the value of an attribute
• To delete an attribute
class Generic:
	pass 
g = Generic()	
> g.atrib =1
> g.atrib  ## 1
> del g.atrib
> g.atrib ##No atribute is found 




Descriptors and properties:
===========================
For python as there is no private keyword in the classes the name managling comes handy 
Suppose we have a class with an atribute vale 

class Myclass(object):
	secret_value = 2
	
>> dir(MyClass)
['_MyClass__secret_value','__class__', '__delattr__',]
>> instance_of._MyClass__secret_value

This is provided to avoid name collision under inheritance, as the attribute is
renamed with the class name as a prefix. It is not a real lock, since the attribute can
be accessed through its composed name. This feature could be used to protect
the access of some attributes, but in practice, "__" should never be used.

_ in private 
----------------------------
When an attribute is not public, the convention to use is a "_" prefix. This does not call any
mangling algorithm, but just documents the attribute as being a private element of
the class and is the prevailing style.
Properties:
=======================
A property is
actually a method function and can process, rather than simply preserve, a reference
to another object.

2 ways to create properties :
-----------------------------
1) Using Property function :
2) Property Decorator : 
The decorator implementation is shown below 

class Parrot(object):
    def __init__(self):
        self._voltage = 100000

    @property
    def voltage(self):
        """Get the current voltage."""
        return self._voltage
        
A property object has getter, setter, and deleter methods usable as decorators that create a copy of the property
with the corresponding accessor function set to the decorated function
class C(object):
    def __init__(self):
        self._x = None

    @property
    def x(self):
        """I'm the 'x' property."""
        return self._x

    @x.setter
    def x(self, value):
        self._x = value

    @x.deleter
    def x(self):
        del self._x
        
 Take a look at two basic design patterns for properties       
• Eager calculation: In this design pattern, when we set a value via a property,
other attributes are also computed
• Lazy calculation: In this design pattern, calculations are deferred until
requested via a property

common features of the Hand object into an abstract superclass, as follows:
class Hand:
	pass
 Lazy calculation:
 -----------------------
The following is a subclass of Hand, where total is a lazy property that is computed
only when needed:
class Hand_Lazy(Hand):
	def __init__( self, dealer_card, *cards ):
		self.dealer_card= dealer_card
		self._cards= list(cards)
	@property  ##total is a lazy property
	def total( self ):
		delta_soft = max(c.soft-c.hard for c in self._cards)
		hard_total = sum(c.hard for c in self._cards)
			if hard_total+delta_soft <= 21: 
				return hard_total+delta_soft
		return hard_total
		
	@property
	def card( self ):
		return self._cards
		
	@card.setter
	def card( self, aCard ):
		self._cards.append( aCard )
		
	@card.deleter
	def card( self ):
		self._cards.pop(-1)
	
The Hand_Lazy class initializes a Hand object with a list of the Cards object.
The total property is a method that computes the total only when requested.
h= Hand_Lazy( d.pop(), d.pop(), d.pop() )
>>> h.total
19
>>> h.card= d.pop()
>>> h.total
29
Eagerly computed properties:
----------------------------
where total is a simple attribute that's computed eagerly as each card is added
 class Hand_Eager(Hand):
	_cards = 0
	def __init__( self, dealer_card, *cards ):
		self.dealer_card= dealer_card
		self.total= 0
		self._cards= list()
		for c in cards:
			self.card = c
			
	@property
	def card( self ):
		return self._cards
	@card.setter
	def card( self, aCard ):
		self._cards.append(aCard)
		self._delta_soft = max(aCard.soft-aCard.hard,self._delta_soft)
		self._hard_total += aCard.hard
		self._set_total() #The set total is calculated when this property is set 
	@card.deleter
	def card( self ):
		removed= self._cards.pop(-1)
		self._hard_total -= removed.hard
		# Issue: was this the only ace?
		self._delta_soft = max( c.soft-c.hard for c in self._cards)
		self._set_total() #The set total is calculated when this property is deleted
	def _set_total( self ):
		if self._hard_total+self._delta_soft <= 21:
			self.total= self._hard_total+self._delta_soft
		else:
			self.total= self._hard_total
In this case, each time a card is added, the total attribute is updated.
h1= Hand_Lazy( d.pop(), d.pop(), d.pop() )
print( h1.total )
h2= Hand_Eager( d.pop(), d.pop(), d.pop() )
print( h2.total )

This is main purpose of using the getter ad setter functions
special methods for attribute access:
--------------------------------------
• The __setattr__() method will create and set attributes.
• The __getattr__() method will do two things. Firstly, if an attribute
already has a value, __getattr__() is not used; the attribute value is
simply returned. Secondly, if the attribute does not have a value, then
__getattr__() is given a chance to return a meaningful value. If there is
no attribute, it must raise an AttributeError exception.
• The __delattr__() method deletes an attribute.
• The __dir__() method returns a list of attribute names.

The __getattribute__() method
------------------------------
An even lower level attribute processing is the __getattribute__() method.
The default implementation attempts to locate the value as an existing attribute
in the internal __dict__ (or __slots__). If the attribute is not found, it calls
__getattr__() as a fallback. If the value located is a descriptor (see in the following
Creating descriptors section), then it processes the descriptor. Otherwise, the value is
simply returned.

Descriptors:
=======================
A descriptor is a class that mediates attribute access.
Descriptor objects are built inside a class at class
definition time.
The descriptor design pattern has two parts: 
1> an owner class (uses one or more descriptors for its attributes)
2> descriptor itself (A descriptor class defines some combination of get,
 set, and delete methods)
 
Unlike other attributes, descriptors are created at the class level. They're not
created within the __init__() initialization they ca e set only at intilaisat
ion

To be recognized as a descriptor, a class must implement any combination of the
following three methods.
• Descriptor.__get__( self, instance, owner ) → object: 
-----------------------------------------------------------
 the instance parameter is the self variable of the object being
accessed. 
owner parameter is the owning class object. in a class context,  will get a None value.
Descriptor.__set__( self, instance, value ): 
-----------------------------------------------------------
In this method, the
instance parameter is the self variable of the object being accessed. The
value parameter is the new value that the descriptor needs to be set to.
Descriptor.__delete__( self, instance ):
-----------------------------------------------------------
 In this method, the
instance parameter is the self variable of the object being accessed. This
method of the descriptor must delete this attribute's va

2 species of Descriptors:
----------------------------------------
> A nondata descriptor: This kind of descriptor defines __set__() or __
delete__() or both. It cannot define __get__(). The nondata descriptor
object will often be used as part of some larger expression. It might be
a callable object, or it might have attributes or methods of its own. An
immutable nondata descriptor must implement __set__() but may simply
raise AttributeError.


> A data descriptor: This descriptor defines __get__() at a minimum. Usually,
it defines both __get__() and __set__() to create a mutable object. The
descriptor can't define any further attributes or methods of this object since
the descriptor will largely be invisible.

Lets Implenment each one of them 
class A(object):
# lets have a class with getter , setters 
    def __init__(self):
        self.value = ''

    def __get__(self, instance, owner):
        return self.value

    def __set__(self, instance, value):
         self.value = value

# Now we will use this class in another class as an Instance 
class Myclass(object):
    atr = A()

obj = Myclass()
#Set some value using the instance it will call the setter associated with the instance
obj.atr = '1st string'
# This will call the get method of the class
print (obj.atr)
>> '1st string'
# Add a new atribute on the fly with the instance
obj.new_atr = '2nd string'
print (obj.__dict__)
>> {'new_atr': '2nd string'}   # this will update the dict with new value
Another way to add an atribute 
obj.__dict__['new_atr'] = 'New Str'
print (obj.__dict__)
>>{'new_ano_atr': 'New Ano  Str', 'new_atr': 'New Str'}
	
Introspection Descriptor:
==============================
The introspection Descriptor is needed when a class is doing introspection over tehir atribute 
A property API class that render a redible documentation over the public methods of 


class API(object):
    def print_values(self,obj):
        def _print_values(mtdname):
            # Rejects all the  __XX__  mthds
            if mtdname.startswith('_'):
                return ''
            val = getattr(obj,mtdname)
            doc = val.__doc__
         #   print(doc)
            return '%s:%s'%(mtdname,doc)
        res = [_print_values(ent) for ent in dir(obj)]
        #print(res)
        return '\n'.join([ent for ent in res if ent != ''])

    def __get__(self, instance, owner):
        #Get gets the call from API
        if instance is not None:
            print('instnce')
            self.print_values(instance)
        else:
            print(self.print_values(owner))



class Myclass(object):
    # doc set as object of API
    __doc__ = API()

    def __init__(self):
        self.a =2

    def addby1(self):
        """Method is add by 1"""
        return self.a +1
    def addby2(self):
        """Method is add by 2"""
        return self.a +2

if __name__ == '__main__':
    #Lets call the Doc of the class
    Myclass.__doc__
   
>>
   addby1:Method is add by 1
addby2:Method is add by 2
	
	
Meta descriptor :
======================
perform a task.This can be useful, To lower the quantity of code needed to use a class
that provides steps.

Slots:
========================
It allows you to set a static attribute list for a given class with the __slots__ attribute,
and skip the creation of the __dict__ list in each instance of the class.
> If you intend to save memory space for classes with very atributes
> Creating immutable objects with __slots__
Example:
	If you cannot set an atribute or create a new one then its immutable 

class BlackJackCard:
	"""Abstract Superclass"""
	__slots__ = ( 'rank', 'suit', 'hard', 'soft' )
	def __init__( self, rank, suit, hard, soft ):
		super().__setattr__( 'rank', rank )
		super().__setattr__( 'suit', suit )
		super().__setattr__( 'hard', hard )
		super().__setattr__( 'soft', soft )
		def __str__( self ):
		return "{0.rank}{0.suit}".format( self )
	def __setattr__( self, name, value ):
		raise AttributeError( "'{__class__.__name__}' has no
		attribute '{name}'".format( __class__= self.__class__, name= name
		) )
We made three significant changes:
• We set __slots__ to the names of only the allowed attributes. This turns off
the internal __dict__ feature of the object and limits us to just the attributes
and no more.
• We defined __setattr__() to raise an exception rather than do
anything useful.
• We defined __init__() to use the superclass version of __setattr__()
so that values can be properly set in spite of the absence of a working
__setattr__() method in this class.


With some care, we can bypass the immutability feature if we work at it.
object.__setattr__(c, 'bad', 5)
The point of
an immutable object like this is to guarantee __hash__() returning a consistent value
and not to prevent people from writing rotten code.


Meta-programming:
=====================================================
The new-style classes brought the ability to change classes' and objects' definitions
on the fly, through two special methods: __new__ and __metaclass__.

Meta programing 
=============================

Meta-programming
The new-style classes brought the ability to change classes' and objects' definitions
on the fly, through two special methods: __new__ and __metaclass__.

> The  __new__ Method
------------------------
the new method will be invoked when the class is instantiated 
class MyClass(object):
	def __new__(cls):
		print '_new_called'
		return object.__new__(cls)## This _new_ mth will returm a object.new(Classname)
	##This will asign the memory for the class 
	
	
	def __init__(self):
		print "init mthd"
		self.a =1 
	#The intialisation will be taken care by the init methods 
	
	
>> a = MyClass()
_new_called'
init mthd
** So new can be used to make changes to class before the object /after object creation is taking place 
Now lets inherit the class 
class Newclass(Myclass0:
	pass 

a= Newclass()
_new_called'
init mthd
My other  class 
	

Uses:  Network socket or database initializations, for instance, should be controlled in
__new__ rather than in __init__. It tells us when the initialization must be done for
the class to work and when it might be derived.

__metaclass__ Method:
=================================
Metaclasses give the ability to interact when a class object is created in memory
through its factory. They act like __new__ but at the class level. The built-in type
type is the built-in base factory.


Creating classes dynamically
***********************************************************
Since classes are objects, you can create them on the fly, like any object.
First, you can create a class in a function using class:

   def choose_class(name):
          if name == 'foo':
              class Foo(object):
                  pass
              return Foo # return the class, not an instance
          else:
              class Bar(object):
                  pass
              return Bar
          
   MyClass = choose_class('foo') 
   print(MyClass) # the function returns a class, not an instance
  <class '__main__.Foo'>
   print(MyClass()) # you can create an object from this class
  <__main__.Foo object at 0x89c6d4c>


What are metaclasses?
************************
Metaclasses are the 'stuff' that creates classes.
metaclasses are what create these objects. They are the classes' classes, you can picture them this way:
  MyClass = MetaClass()
  MyObject = MyClass()
You've seen that type lets you do something like this:
Without the metaclass use 

def extra(self, arg): ...
	class Client1: ... # Client augments: too distributed
		if required():
			Client1.extra = extra
			
	class Client2: ...
		if required():
			Client2.extra = extra 


 
With the metaclass use 
This is exactly what metaclasses do—by declaring a metaclass, we tell Python to route
the creation of the class object to another class we provide:
def extra(self, arg): ...
class Extras(type):
	def __init__(Class, classname, superclasses, attributedict):
		if required():
			Class.extra = extra
			
class Client1(metaclass=Extras): ... # Metaclass declaration only (3.X form)
class Client2(metaclass=Extras): ... # Client class is instance of meta
class Client3(metaclass=Extras): ...
X = Client1() # X is instance of Client1
X.extra()


Because Python invokes the metaclass automatically at the end of the class statement
when the new class is created, it can augment, register, or otherwise manage the class
as needed.
 

Metaclasses Are Subclasses of Type
-------------------------------------
  MyClass = type('MyClass', (), {})
It's because the function type is in fact a metaclass. type is the metaclass Python uses to create all classes behind the scenes.
metaclasses can even be used to implement alternative
coding patterns such as aspect-oriented programming, object/relational mappers

It turns out that classes are instances of something, too:
• In Python 3.X, user-defined class objects are instances of the object named type,
which is itself a class.
• In Python 2.X, new-style classes inherit from object, which is a subclass of type;
classic classes are instances of type and are not created from a class.ORMs) for databases, and more.
__class___ Atribute :
**************************
Class gives the class name of the type is followed up with 
name = 'bob'
   name.__class__
  <type 'str'>
 
c = Foo()
c.__class__
>><class '__main__.Foo'>


now what is __class__.__class__ ie class of class which is metaclass
c.__class__.__class__

>><type 'type'>
type is a class that generates user-defined classes.
• Metaclasses are subclasses of the type class.
• Class objects are instances of the type class, or a subclass thereof.
• Instance objects are generated from a class.
In other words, to control the way classes are created and augment their behavior, all
we need to do is specify that a user-defined class be created from a user-defined metaclass
instead of the normal type class.

Class Statement Protocol (what python interpreter does wen it gets a class in code)
------------------------------------------------------------------------------------
when Python reaches a class statement, 
> it runs its nestedblock of code to create its attributes—all the names assigned at the top level 
of the nested code block generate attributes in the resulting class object. 
> These names are usually method functions created by nested defs, but they can also be 
arbitrary attributes assigned to create class data shared by all instances.
Python follows a standard protocol to make this happen: at the
end of a class statement, and after running all its nested code in a namespace dictionary
corresponding to the class’s local scope, Python calls the type object to create the
class object like this:
class = type(classname, superclasses, attributedict)


type object in turn defines a __call__ operator overloading method that runs two
def __call__():
	type.__new__(typeclass, classname, superclasses, attributedict)##Creates new obj
	type.__init__(class, classname, superclasses, attributedict) ## Intialises it
	
Example:
For example, given a class definition like the following for Spam:
class Eggs: ... # Inherited names here
class Spam(Eggs): # Inherits from Eggs
	data = 1 # Class data attribute
	def meth(self, arg): # Class method attribute
		return self.data + arg
		
Python will internally run the nested code block to create two attributes of the class
(data and meth), and then call the type object to generate the class object at the end of
the class statement:
Spam = type('Spam', (Eggs,), {'data': 1, 'meth': meth, '__module__': '__main__'})


The __metaclass__ attribute:
***************************************
Declaration in 3.X
	class Spam(metaclass=Meta): # 3.X version (only)
	class Spam(Eggs, metaclass=Meta): # Normal supers OK: must list first
Declaration in 2.X
	class Spam(object): # 2.X version (only), object optional?
		__metaclass__ = Meta
	class Spam(Eggs, object): # Normal supers OK: object suggested
		__metaclass__ = Meta
	
Metaclass Dispatch in Both 3.X and 2.X
---------------------------------------------
When a specific metaclass is declared per the prior sections’ syntax, the call to create
the class object run at the end of the class statement is modified to invoke the metaclass
instead of the type default:
class = Meta(classname, superclasses, attributedict)
And because the metaclass is a subclass of type, the type class’s __call__ delegates the
calls to create and initialize the new class object to the metaclass, if it defines custom
versions of these methods:
	Meta.__new__(Meta, classname, superclasses, attributedict)
	Meta.__init__(class, classname, superclasses, attributedict)
To demonstrate, here’s the prior section’s example again, augmented with a 3.X metaclass
specification:
	class Spam(Eggs, metaclass=Meta): # Inherits from Eggs, instance of Meta
		data = 1 # Class data attribute
		def meth(self, arg): # Class method attribute
			return self.data + arg
At the end of this class statement, Python internally runs the following to create the
class object—again, a call you could make manually too, but automatically run by
Python’s class machinery:
	Spam = Meta('Spam', (Eggs,), {'data': 1, 'meth': meth, '__module__': '__main__'})
If the metaclass defines its own versions of __new__ or __init__, they will be invoked
in turn during this call by the inherited type class’s __call__ method, to create and
initialize the new class. The net effect is to automatically run methods the metaclass
provides, as part of the class construction process.


A basic metaclass:

class MetaOne(type):
	def __new__(meta,classname,supers,classdict):
		print(meta,classname,supers,classdict,sep=\n)
		return type.__new__(meta,classname,supers,classdict)
	def __init__(Class, classname, supers, classdict):
		#print('In MetaOne.init:', classname, supers, classdict, sep='\n...')
		print('...init class object:', list(Class.__dict__.keys()))

print('making class')
class Spam(Eggs,metaclass=MetaOne):
	data =1
	def meth(self,arg):
		retun self.data + arg
		
print('making instance')
X = Spam()
print('data:', X.data, X.meth(3))

output:
---------		
making class
In MetaOne.new:
<class '__main__.MetaOne'>## meta name
	Spam	`##name of the classname
	(<class '__main__.Eggs'>,) ##supers 
	{'data': 1, 'meth': <function Spam.meth at 0x02A191E0>, '__module__': '__main__'}#classdict	
	
In Init.new:	
	
init class object: ['__qualname__', 'data', '__module__', 'meth', '__doc__']
making instance
data: 1 3

Metaclass creation Using simple factory functions
------------------------------------------------------
def MetaFunc(classname, supers, classdict):
	print('In MetaFunc: ', classname, supers, classdict, sep='\n...')
	return type(classname, supers, classdict)
class Eggs:
	pass
	print('making class')
class Spam(Eggs, metaclass=MetaFunc): # Run simple function at end
	data = 1 # Function returns class
def meth(self, arg):
	return self.data + arg
	
print('making instance')
X = Spam()
print('data:', X.data, X.meth(2))	
When run, the function is called at the end of the declaring class statement, and it
returns the expected new class object.

# A normal class instance can serve as a metaclass too
-----------------------------------------------------------
class MetaObj:
	def __call__(self, classname, supers, classdict):
		print('In MetaObj.call: ', classname, supers, classdict, sep='\n...')
		Class = self.__New__(classname, supers, classdict)
		self.__Init__(Class, classname, supers, classdict)
		return Class

		def __New__(self, classname, supers, classdict):
			print('In MetaObj.new: ', classname, supers, classdict, sep='\n...')
			return type(classname, supers, classdict)
			
		def __Init__(self, Class, classname, supers, classdict):
			print('In MetaObj.init:', classname, supers, classdict, sep='\n...')
			print('...init class object:', list(Class.__dict__.keys()))
class Eggs:
	pass
	print('making class')
	
class Spam(Eggs, metaclass=MetaObj()): # MetaObj is normal class instance
	data = 1 # Called at end of statement
	def meth(self, arg):
	return self.data + arg
	
	
adding more twiste we can use normal superclass inheritance to acquire the call interceptor in this
coding model—	
class MetaObjSuper:
	def __call__(self, classname, supers, classdict):
		print('In MetaObj.call: ', classname, supers, classdict, sep='\n...')
		Class = self.__New__(classname, supers, classdict)
		self.__Init__(Class, classname, supers, classdict)
		return Class

class SubMetaObj(SuperMetaObj):
	def __new__ / 
	__Init__
All r same
	
Inheritance and Instance:
------------------------------------------------------
(class using Metaclass are basically instance of Metaclass)
1. Metaclasses inherit from the type class (usually)
2. Metaclass declarations are inherited by subclasses
	So any subclass of Spam wil be having the MetaObj as metaclass
3. Metaclass attributes are not inherited by class instances
4. Metaclass attributes are acquired by classes.
	
def newmthd():
	pass
	
class MetaOne(type):
	def __new__(meta, classname, supers, classdict): # Redefine type method
		print('In MetaOne.new:', classname)
		classdict[mewmthd]= newmthd
		return type.__new__(meta, classname, supers, classdict)
	def toast(self):
		return 'toast'
	#Overding 
	def overd_me(self):
		return 'Meta'
		
	#OPerator Overloading 
	def __getitem__(cls, i): # Meta method for processing classes:
		return cls.data[i] # Built-ins skip class, use meta
						   # Explicit names search class + meta
		
class Super(metaclass=MetaOne): # Metaclass inherited by subs too
	data = 'Spam'
	
	def spam(self): # MetaOne run twice for two classes
		return 'spam'
	def overd_me(self):
		return 'Super' # Meta mthod is overidden
	

class Sub(Super): # Superclass: inheritance versus instance
	def eggs(self): # Classes inherit from superclasses
		return 'eggs' # But not from metaclasses
		
When this code is run (as a script or module), the metaclass handles construction of
both client classes, and instances inherit class attributes but not metaclass attributes:
>>>  # Runs class statements: metaclass run twice
	In MetaOne.new: Super
	In MetaOne.new: Sub
>>> X = Sub() # Normal instance of user-defined class
>>> X.eggs() # Inherited from Sub
	'eggs'
>>> X.spam() # Inherited from Super
	'spam'
>>> X.toast() # Not inherited from metaclass
	AttributeError: 'Sub' object has no attribute 'toast'
>>> Super.overd_me()
	Super
>>>Super.data[0]
	S
>>>>X.newmthd()

By contrast, classes both inherit names from their superclasses, and acquire names from
their metaclass (which in this example is itself inherited from a superclass):
>>> Sub.eggs(X) # Own method
	'eggs'
>>> Sub.spam(X) # Inherited from Super
	'spam'
>>> Sub.toast() # Acquired from metaclass
	'toast'
>>> Sub.toast(X) # Not a normal class method
	TypeError: toast() takes 1 positional argument but 2 were given

Metaclass Versus Superclass :
------------------------------
Metaclass:
		class A(type): attr = 1

		class B(metaclass=A): 
			pass # B is meta instance and acquires meta attr
			I = B() # I inherits from class but not meta!
			B.attr
		>> 1
>>> I.attr
AttributeError: 'B' object has no attribute 'attr'
>>> 'attr' in B.__dict__, 'attr' in A.__dict__
(False, True)
Superclass:
class A: attr = 1
class B(A): pass # I inherits from class and supers
I = B()
>>> B.attr
1
>>> I.attr
1
>>> 'attr' in B.__dict__, 'attr' in A.__dict__
(False, True)
Superclass:
class M(type): attr = 1
>>> class A: attr = 2
>>> class B(A, metaclass=M): pass # Supers have precedence over metas
>>> I = B()
>>> B.attr, I.attr
(2, 2)
>>> 'attr' in B.__dict__, 'attr' in A.__dict__, 'attr' in M.__dict__
(False, True, True)

 Differnce between class decorators and metaClass 
 ==================================================
 1) class decorators run after the decorated class has already been created. Thus, they are often used to 
	add logic to be run at instance creation time. When they do provide behavior for a class, it is typically
	through changes or proxies, instead of a more direct relationship.
	metaclasses, by contrast, run during class creation to make and return
	the new client class. Therefore, they are often used for managing or augmenting
	classes themselves, and can even provide methods to process the classes that are created
	from them, via a direct instance relationship
2) metaclasses also have a __new__ method used to create a class, which has
	no analogy in decorators; making a new class in a decorator would incur an extra step.
	Moreover, metaclasses may also provide behavior acquired by classes in the form of
	methods, which have no direct counterpart in decorators either; decorators must provide
	class behavior is less direct ways.
	
	
Uses of Metaclass:
-----------------------
1) metaclasses can be used to add decoration to all methods of classes automatically,
2) register all classes in use to an API, 
3) add user-interface logic to classes automatically,
4) create or extend classes from simplified specifications in text files,


variables and values are REFERENCES
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Everything is  reference of an Object except the variables user declares
f you're used to most traditional languages, you have a mental model of what happens in the following sequence:

a = 1
a = 2
You believe that a is a memory location that stores the value 1, then is updated to store the value 2. 
That's not how things work in Python. Rather, a starts as a reference to an object with the value 1,
then gets reassigned as a reference to an object with the value 2. Those two objects may continue to 
coexist even though a doesn't refer to the first one anymore; in fact they may be shared by any number
of other references within the program.

Value passing in a function is non mutable :
---------------------------------------------
When you call a function with a parameter, a new reference is created that refers to the object passed in. 
This is separate from the reference that was used in the function call, so there's no way to update that reference
 and make it refer to a new object. In your example:

    self.variable = 'Original'
    self.Change(self.variable)

def Change(self, var):
    var = 'Changed'
self.variable is a reference to the string object 'Original'. When you call Change you create a second reference
 var to the object. Inside the function you reassign the reference var to a different string object 'Changed',
 but the reference self.variable is separate and does not change.
 
 What is Happening here in terms of C++ terminology is
 ****************************************
 int& xRef = x;
	int& zRef = z;
	zRef = xRef; // Assigns values, not references
 ***************************************
 So over here var and 'changed' are two seperate references and now var value is equal to the 'changed' reference
 value.
 
The only way around this is to pass a mutable object. Because both references refer to the same object, any 
changes to the object are reflected in both places.

    self.variable = ['Original']
    self.Change(self.variable)

def Change(self, var):
    var[0] = 'Changed'

Now in this condition as var is a reference we are assigning the addresesss refered by var[0] to 'Changed' value 
So this willl refelect on self.variable also


Functions:
===================================================================================================
In python the functions are defined with def

def fun(a,b):
	c = a+b
	return c
d = fun(2,3)

default Function definition is here
	def printinfo( name, age = 35 ):

Variable-length arguments:
	def printinfo( arg1, *vartuple ):
		for var in vartuple:
      print var
	return;
	
More on Defining Function:
--------------------------

1> Default argumnents :
----------------------_
def fun(i,k=5):
	print k
this function can be called from the following ways 
fun(3) # 5 
fun(5) # 5 
So the default argument is evaluated only onece that is for the 1st call
Now for a function which is taking a mutable object he default argument is evaluated everytime
def foobar(arg_string="abc", arg_list = []): 
    print arg_string, arg_list 
    arg_string = arg_string + "xyz"
    arg_list.append("F")
 
for i in range(4): 
    foobar()
abc [] 
abc ['F'] 
abc ['F', 'F'] 
abc ['F', 'F', 'F']

What happens here is The objects that provide the default values are not created at the time that fun is called.
They are created at the time that the statement that defines the function is executed

1) A new function object is created, bound to the name fun, and stored in the namespace of foo_module.
2) Within the fun function object, for each argument with a default value, an object is created to hold
   the default object. In the case of fun, a string object containing “abc” is created as the default for 
   the arg_string argument, and an empty list object is created as the default for the arg_list argument.
3) whenever fun is called without arguments, arg_string will be bound to the default string object, and 
arg_list will be bound to the default list object. In such a case, arg_string will always be “abc”,
 but arg_list not be an empty list. 




2> Arbitary Arg List
--------------------
The least frequently used option is to specify that a function can be called with an arbitrary number
of arguments. These arguments will be wrapped up in a tuple

def write_multiple_items(file, separator, *args):
    file.write(separator.join(args))

3) multiple argument 
--------------------
def manyArgs(*arg):
  print "I was called with", len(arg), "arguments:", arg

 manyArgs(1)
I was called with 1 arguments: (1,)
 manyArgs(1, 2,3)
I was called with 3 arguments: (1, 2, 3)	

j_txt = _("jobs: ") 


Single Lone Underscore (_)
============================================
This is typically used in 3 cases:

In the interpreter: The _ name points to the result of the last executed statement in an interactive interpreter
 session.
 This was first done by the standard CPython interpreter, and others have followed too.

 _
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
NameError: name '_' is not defined
 42
 _
42
 'alright!' if _ else ':('
'alright!'
 _
'alright!'
As a name: This is somewhat related to the previous point. _ is used as a throw-away name. This will allow the 
next person reading your code to know that,
 by convention, a certain name is assigned but not intended to be used. 
For instance, you may not be interested in the actual value of a loop counter:

n = 42
for _ in range(n):
    do_something()

So overe here we dont care what number the loop is currently at we ajuts need to do something for 42 times and this saves 
n extra variable 
i18n: One may also see _ being used as a function. In that case, it is often the name used for the function that does internationalisation and 
localisation string translation lookups. This seems to have originated from and follow the corresponding C convention. For instance, as seen in
 the Django documentation for translation, you may have:

from django.utils.translation import ugettext as _
from django.http import HttpResponse

def my_view(request):
    output = _("Welcome to my site.")
    return HttpResponse(output)
The second and third purposes can conflict, so one should avoid using _ as a throw-away name in any code block that also uses it for i18n lookup and translation.

Difference between _, __ and __xx__ in Python
==================================================
When learning Python many people don't really understand why so much underlines in the beginning of the methods, sometimes even in the end like __this__! 
I've already had to explain it so many times, it's time to document it.

One underline in the beginning
Python doesn't have real private methods, so one underline in the beginning of a method or attribute means you shouldn't access this method, because
 it's not part of the API. It's very common when using properties:

class BaseForm(StrAndUnicode):
     def _get_errors(self):
        "Returns an ErrorDict for the data provided for the form"
        if self._errors is None:
            self.full_clean()
        return self._errors
    
    errors = property(_get_errors)
This snippet was taken from django source code (django/forms/forms.py). This means errors is a property, and it's part of the API,
 but the method this property calls, _get_errors, is "private", so you shouldn't access it.

Two underlines in the beginning
This one causes a lot of confusion. It should not be used to mark a method as private, the goal here is to avoid your method to be 
overridden by a subclass. Let's see an example:

class A(object):
    def __method(self):
        print "I'm a method in A"
    
    def method(self):
        self.__method()
     
a = A()
a.method()
The output here is

$ python example.py 
I'm a method in A
Fine, as we expected. Now let's subclass A and customize __method

class B(A):
    def __method(self):
        print "I'm a method in B"

b = B()
b.method()
and now the output is   

$ python example.py
I'm a method in A
as you can see, A.method() didn't call B.__method() as we could expect. Actually this is the correct behavior for __.
 So when you create a method starting with __ you're saying that you don't want anybody to override it, it will be accessible just from inside the own class.

How python does it? Simple, it just renames the method. Take a look:

a = A()
a._A__method()  # never use this!! please!
$ python example.py
I'm a method in A
If you try to access a.__method() it won't work either, as I said, __method is just accessible inside the class itself.

Two underlines in the beginning and in the end
When you see a method like __this__, the rule is simple: don't call it. Why? Because it means it's a method python calls,
 not you. Take a look:

 name = "igor"
 name.__len__()
4
 len(name)
4

 number = 10
 number.__add__(20)
30
 number + 20
30
There is always an operator or native function that calls these magic methods. The idea here is to give you the ability 
to override operators in your own classes. Sometimes it's just a hook python calls in specific situations. __init__(), 
for example, is called when the object is created so you can initialize it. __new__() is called to build the instance, and so on   

Here's an example:

class CrazyNumber(object):
    
    def __init__(self, n):
        self.n = n
    
    def __add__(self, other):
        return self.n - other.n
    
    def __sub__(self, other):
        return self.n + other.n
    
    def __str__(self):
        return str(self.n)


num = CrazyNumber(10)
print num           # 10
print num + 5       # 5
print num - 20      # 30
Another example:

class Room(object):

    def __init__(self):
        self.people = []

    def add(self, person):
        self.people.append(person)

    def __len__(self):
        return len(self.people)

room = Room()
room.add("Igor")
print len(room)     # 1
The documentation covers all these special methods.

Conclusion:
--------------------
Use _one_underline to mark you methods as not part of the API. Use __two_underlines__ when you're creating 
objects to look like native python objects or you wan't to customize behavior in specific situations. And 
don't use __just_to_underlines, unless you really know what you're doing!

	


Lambda Functions
===================================================================================================
Lamda functions are not bound to a name they just use a construct called lambda.
* Lamda function dont have  a return statment it will always have a line which is returned.
* Lamda can directly used without the use assigning it to a variable.

lambda x,y: x + y

this is a sum function 

lambda Functions:
==================================================
Python supports the creation of anonymous functions (i.e. functions that are not bound to a name) at runtime, 
diffrence between two functions
 def f (x): return x**2
    
 print f(8)
64
 
 g = lambda x: x**2
 
 print g(8)
64


 def make_incrementor (n): return lambda x: x + n
 
 f = make_incrementor(2)
 g = make_incrementor(6)
 
 print f(42), g(42)
44 48
 
 print make_incrementor(22)(33)
55
The above code defines a function "make_inrementor" that creates an anonymous function 
on the fly and returns it. The returned function increments its argument by the value that
was specified when it was created.



No Multiline Lambda in Python: Why not?
===================================================
Look at the following:

map(multilambda x:
      y=x+1
      return y
   , [1,2,3])
Is this a lambda returning (y, [1,2,3]) (thus map only gets one parameter, resulting in an error)?
 Or does it return y? Or is it a syntax error, because the comma on the new line is misplaced? 
 How would Python know what you want?

Within the parens, indentation doesn't matter to python, so you can't unambiguously work with multilines.

This is just a simple one, there's probably more examples.



This is mostly used as argument of other functional concepts in python on a list of values 
g = [2, 18, 9, 22, 17, 24, 8, 12, 27]

1. filter

Filter seperates the list passed by the lamda then elements in a  new array 
 filter(lambda x: x % 3 == 0,g)
 
 >> [18, 9, 24, 12, 27]
 
 
 
2. map

map will do a set of operations on each of the elements and output it

 map(lambda x: x * 2 + 10, foo)

>> [14, 46, 28, 54, 44, 58, 26, 34, 64]

3. reduce

reduce will do a operation in all the elements at a once and output a result

reduce(lambda x, y: x + y, foo)
>> 139
the functools has the reduce now  

Modules:
=================================
Creating a module is as simple as creating a file ,and Import it to use it 

Creating a folder Module will include a __init__.py file to make it a Module 


Importing and using of Module methods:
-----------------------------------------
File_mod/File_handler.py    A folder containing a file  having a class test

1) To import this file we 

from File_mod import File_handler

		or
import File_mod.File_handler

2) To Import test class inside the file 

 import File_mod.File_handler.test


Use of if (__name__) to check the Running module is Main or Imported:
--------------------------------------------------------------------
When your script is run by passing it as a command to the Python interpreter,

python myscript.py
all of the code that is at indentation level 0 gets executed.
__name__ is a built-in variable which evaluate to the name of the current module. However, if a module is being 
run directly (as in myscript.py above), then __name__ instead is set to the string "__main__". 
Thus, you can test whether your script is being run directly or being imported by something else by testing

# file one.py
def func():
    print("func() in one.py")

print("top-level in one.py")

if __name__ == "__main__":
    print("one.py is being run directly")
else:
    print("one.py is being imported into another module")

# file two.py
import one

print("top-level in two.py")
one.func()

if __name__ == "__main__":
    print("two.py is being run directly")
else:
    print("two.py is being imported into another module")
Now, if you invoke the interpreter as

python one.py
The output will be

top-level in one.py
one.py is being run directly
If you run two.py instead:

python two.py
You get

top-level in one.py
one.py is being imported into another module
top-level in two.py
func() in one.py
two.py is being run directly
Thus, when module one gets loaded, its __name__ equals "one" instead of __main__.


Global Variables:
=================================
Global vs. Local variables:
Variables that are defined inside a function body have a local scope, and those defined outside have a global scope.

This means that local variables can be accessed only inside the function in which they are declared, whereas global variables can be accessed 
throughout the program body by all functions. When you call a function, the variables declared inside it are brought into scope.
 Following is a simple example:

#!/usr/bin/python

total = 0; # This is global variable.
# Function definition is here
def sum( arg1, arg2 ):
   # Add both the parameters and return them."
   total = arg1 + arg2; # Here total is local variable.
   print "Inside the function local total : ", total
   return total;

# Now you can call sum function
sum( 10, 20 );
print "Outside the function global total : ", total 
When the above code is executed, it produces the following result:

Inside the function local total :  30
Outside the function global total :  0
Another way to that is by using 

Third global is only useful to change or create global variables in a local context, although creating global variables is seldom considered a good solution.

def bob():
    me = "locally defined"    # Defined only in local context
    print me

bob()
print me     # Asking for a global variable
The above will give you:

locally defined
Traceback (most recent call last):
  File "file.py", line 9, in <module>
    print me
NameError: name 'me' is not defined
While if you use the global statement, the variable will become available "outside" the scope of the function, 
effectively becoming a global variable.

def bob():
    global me
    me = "locally defined"   # Defined locally but declared as global
    print me

bob()
print me     # Asking for a global variable
So the above code will give you:

locally defined
locally defined


class and objects:
=====================================
Class declaration in python

class a:
new way of class declaration in Python

class a(object):
	// calling the instance of the python
	x = a()
now if we print the object 

print a
<__main__.ObjectCreator object at 0x8974f2c>

Classes are objects too.As soon as you use the keyword class, Python executes it and creates an OBJECT
This object (the class) is itself capable of creating objects (the instances), and this is why it's a class.

But still, it's an object, and therefore:
i)you can assign it to a variable
	b =a 
ii)you can copy it
	c(a)

iii)you can add attributes to it
	a.var = 'test'

iv)you can pass it as a function parameter
	print a

OOSP pythonic Way :
====================

virtual clases:
---------------

When abstractmethod() is applied in combination with other method descriptors, it should be applied as the
 innermost decorator,
 as shown in the following usage examples:

import abc

class Shape(object):
    __metaclass__ = abc.ABCMeta

    @abc.abstractmethod
    def method_to_implement(self, input):
        """Method documentation"""
        return

    @property
    @abc.abstractmethod
    def my_abstract_property(self):
           
    @my_abstract_property.setter
    @abc.abstractmethod
    def my_abstract_property(self, val):
           

    @abc.abstractmethod
    def _get_x(self):
           
    @abc.abstractmethod
    def _set_x(self, val):
           
    x = property(_get_x, _set_x)
If your class is already using a metaclass, derive it from ABCMeta rather than type and you can continue to use your own metaclass.

Alternative of Abstarct methods:
--------------------------------
A cheap alternative (and the best practice before the abc module was introduced) would be to have all your abstract methods just
raise an exception (NotImplementedError is a good one) so that classes derived from it would have to override that method to be useful.
Before this te duck typing concept was der 
Accessing members of both sequence type objects and mapping type objects is done by using the __getitem__ method of these objects.

a = [0,1 2,3]
print a[0]
0
b = {'a': 0, 'b': 1}
print b['a']
0
is exactly the same as :

a =  [0,1 2, 3]
print list.__getitem__(a, 0)
0
b = {'a': 0, 'b': 1}
print dict._getitem__(b, 'a')
0
In the first example we use normal Python syntax. In the second example we do what the first example is doing under the hood. In order to set
 members we would use the __setitem__ method instead of __getitem__.
 

What is Duck typing ?

The principle of duck typing says that you shouldn't care what type of object you have - just whether or not you can do
the required action with your object.
 
Thus, a dynamic type system as Python's always uses duck typing:
Example 1:
------------------------------
def f(x):
    x.Quak()
If f gets an x supporting a Quak(), everything is fine, if not, it will crash at runtime.

But duck typing doesn't imply dynamic typing at all - in fact, there is a very popular but completely static duck typing approach that doesn't give any requirements too:

template <typename T>
void f(T x) { x.Quak(); } 
The function doesn't tell in any way that it wants some x that can Quak, so instead it just tries at compile time and if everything works, it's fine.

Example 2: 
------------------------------
class Animal:
    def __init__(self,name,legs):
        self.name = name
        self.legs = legs

    def getLegs(self):
        return "{0} has {1} legs".format(self.name, self.legs)

    def says(self):
        return "I am an unknown animal"

class Dog(Animal): # <Dog inherits from Animal here (all methods as well)

    def says(self): # <Called instead of Animal says method
        return "I am a dog named {0}".format(self.name)


 For this reason the isinstance keyword is frowned upon.To safegaurd a process and not throw the 
 exceptions 
    def somethingOnlyADogCanDo(self):
        return "be loyal"

formless = Animal("Animal", 0)
rover = Dog("Rover", 4) #<calls initialization method from animal

print(formless.says()) # <calls animal say method

print(rover.says()) #<calls Dog says method
print(rover.getLegs()) #<calls getLegs method from animal class
Results should be:

I am an unknown animal
I am a dog named Rover
Rover has 4 legs


Static method :
--------------------------------------

class MyClass(object):
    @staticmethod
    def the_static_method(x):
        print x

MyClass.the_static_method(2) # outputs 2

Class vs static methods in Python:
===================================
class to do so but that will spread the code related to class, to out of the class. This can cause a future code

def get_no_of_instances(cls_obj):
    return cls_obj.no_inst
 
class Kls(object):
    no_inst = 0
 
    def __init__(self):
        Kls.no_inst = Kls.no_inst + 1
 
ik1 = Kls()
ik2 = Kls()
 
print(get_no_of_instances(Kls))
Gives us the following output:

1
2

we can create a method in a class, using @classmethod
-------------------------------------------------------------------

class Kls(object):
    no_inst = 0
 
    def __init__(self):
        Kls.no_inst = Kls.no_inst + 1
 
    @classmethod
    def get_no_of_instance(cls_obj):
        return cls_obj.no_inst
 
ik1 = Kls()
ik2 = Kls()
 
print ik1.get_no_of_instance()
print Kls.get_no_of_instance()
We get the following output:
2
2

Python @staticmethod
----------------------------------------------------------------------
Often there is some functionality that relates to the class, but does not need the class or any instance(s) to do some work.
 Perhaps something like setting environmental variables, changing an attribute in another class, etc. In these situation we
 can also use a function, however doing so also spreads the interrelated code .
 
  use a @staticmethod, we can place all code in the relevant place.
  
IND = 'ON'
 
class Kls(object):
    def __init__(self, data):
        self.data = data
 
    @staticmethod
    def checkind():
        return (IND == 'ON')  ## this is a global variable value which is intialised over here 
 
    def do_reset(self):
        if self.checkind():
            print('Reset done for:', self.data)
 
    def set_db(self):
        if self.checkind():
            self.db = 'New db connection'
        print('DB connection made for: ', self.data)
 
ik1 = Kls(12)
ik1.do_reset()
ik1.set_db()

Reset done for: 12
DB connection made for: 12



How @staticmethod and @classmethod are different.
class Kls(object):
    def __init__(self, data):
        self.data = data
 
    def printd(self):
        print(self.data)
 
    @staticmethod
        def smethod(*arg):
            print('Static:', arg)
 
    @classmethod
        def cmethod(*arg):
            print('Class:', arg)
>>> ik = Kls(23)
>>> ik.printd()
23
>>> ik.smethod()
Static: ()
>>> ik.cmethod()
Class: (<class '__main__.Kls'>,)
>>> Kls.printd()
TypeError: unbound method printd() must be called with Kls instance as first argument (got nothing instead)
>>> Kls.smethod()
Static: ()
>>> Kls.cmethod()
 
 
 
Magical methods :
===============================================================================

http://www.rafekettler.com/magicmethods.html#intro

Yeild Keyword :
====================================================================================================================
Iterables

When you create a list, you can read its items one by one, and it's called iteration:

 mylist = [1, 2, 3]
 for i in mylist:
       print(i)
1
2
3
Mylist is an iterable. When you use a list comprehension, you create a list, and so an iterable:

 mylist = [x*x for x in range(3)]
 for i in mylist:
       print(i)
0
1
4
Everything you can use "for    in..." on is an iterable: lists, strings, files   
 These iterables are handy because you can read them as much as you wish, but you store all the values in memory and 
 it's not always what you want when you have a lot of values.

Generators

Generators are iterators, but you can only iterate over them once. It's because they do not store all the values in memory,
they generate the values on the fly:

 mygenerator = (x*x for x in range(3))
 for i in mygenerator:
       print(i)
0
1
4
It is just the same except you used () instead of []. BUT, you can not perform for i in mygenerator a second time since generators can only
be used once: they calculate 0, then forget about it and calculate 1, and end calculating 4, one by one.

for i in mygenerator:
       print(i)
Actually this outputs nothing	   
	   
Yield
=============
Yield is a keyword that is used like return, except the function will return a generator.

 def createGenerator():
       mylist = range(3)
       for i in mylist:
           yield i*i
   
 mygenerator = createGenerator() # create a generator
 print(mygenerator) # mygenerator is an object!
<generator object createGenerator at 0xb7555c34>
 for i in mygenerator:
        print(i)
0
1
4
Here it's a useless example, but it's handy when you know your function will return a huge set of values that
 you will only need to read once.

To master yield, you must understand that when you call the function, the code you have written in the function
 body does not run. The function only returns the generator object, this is a bit tricky :-)

Then, your code will be run each time the for uses the generator.

Now the hard part:

The first time the for calls the generator object created from your function, it will run the code in your function 
from the beginning until it hits yield, then it'll return the first value of the loop. Then, each other call will
 run the loop you have written in the function one more time, and return the next value, until there is no value to return.

The generator is considered empty once the function runs but does not hit yield anymore. It can be because the loop 
had come to an end, or because you do not satisfy a "if/else" anymore.

Iterators:
-----------------------------------------------------------
An iterator is nothing more than a container object that implements the iterator
protocol

i = iter([1,2,3,4])
>> i.next()
1
>> i.next()
2
    like that 
the way this is implemented is by using two methods 
a) next, which returns the next item of the container
b) __iter__, which returns the iterator itself

Now we can write our own iterators in the same way just have to implement the __iter__ method 
and next() this next name can be different its not a mandate

class myiter(object):
	def __init__(self,step):
		self.step=step
	def next(self):
		if step ==0:
			raise Exception("Ended")
		step -=1
		return self.step
	def __iter__(self):
		"""return the the iteratoe itself"""
		return self
		
>>i =  myiter(5)
>> for el in MyIterator(4):
    print el

	
vars:
=========	
vars() returns a dictionary to the local variables that is indexed by the name of the variable
as a string. The scope is local.
>>> x = 5
>>> dir()
['__builtins__', '__doc__', '__name__', 'x']
>>> vars()[4] = 'An integer'
>>> dir()
[4, '__builtins__', '__doc__', '__name__', 'x']
>>> x
5
>>> 4S
4
>>> vars()[4]
'An integer'
	
Generators :
-------------------------------------------------------------
generators provide an elegant way to write simple and efficient
code for functions that return a list of elements. Based on the yield directive, they
allow you to pause a function and return an intermediate result. The function saves
its execution context and can be resumed later if necessary.

Ex:
def fibonacci():
	a, b = 0, 1
	while True:
		yield b
		a, b = b, a + b
fib = fibonacci()
[fib.next() for i in range(10)]
[3, 5, 8, 13, 21, 34, 55, 89, 144, 233]

This function returns a generator object, a special iterator, which knows how to
save the execution context. It can be called indefinitely, yielding the next element of
the suite each time.

*** when to use 
generators should be considered every time you deal with a
function that returns a sequence or works in a loop. Returning the elements one at a
time can improve the overall performance, when they are passed to another function
for further work.

One step up again it looks like we will be using yeild as expression and 

but makes yield return the value passed. The function can,
therefore, change its behavior depending on the client code. Two other functions
were added to complete this behavior: throw and close. They raise an error into
the generator:
throw allows the client code to send any kind of exception to be raised.
close acts in the same way, but raises a specific exception: GeneratorExit.
In that case, the generator function must raise GeneratorExit again, or
StopIteration.
Therefore, a typical template for a generator would look like the following:
 def my_generator():
    try:
		yield 'something'
		except ValueError:
		yield 'dealing with the exception'
    finally:
		print "ok let's clean"
   
 gen = my_generator()
 gen.next()
'something'
 gen.throw(ValueError('mean mean mean'))
'dealing with the exception'
 gen.close()
ok let's clean
 gen.next()
Traceback (most recent

Greedy vs. Non-Greedy:
================================

Exception Handling:
==============================
Type of Errors:

 10 * (1/0)
Traceback (most recent call last):
  File "<stdin>", line 1, in ?
ZeroDivisionError: integer division or modulo by zero
 4 + spam*3
Traceback (most recent call last):
  File "<stdin>", line 1, in ?
NameError: name 'spam' is not defined
 '2' + 2
Traceback (most recent call last):
  File "<stdin>", line 1, in ?
TypeError: cannot concatenate 'str' and 'int' objects

How the for statement works?
Any time you use for to iterate over an iterable (basically, all sequence types and anything that defines __iter__() or __getitem__()),
 it needs to know when to stop iterating. Take a look at the code below:

words = ['exceptions', 'are', 'useful']
for word in words:
    print(word)
How does for know when it's reached the last element in words and should stop trying to get more items? The answer may surprise you:
 the list raises a StopIteration exception.
In fact, all iterables follow this pattern. When a for statement is first evaluated, it calls iter() on the object being iterated over.\
 This creates an iterator for the object, capable of returning the contents of the object in sequence. For the call to iter() to succeed,
 the object must either support the iteration protocol (by defining __iter__()) or the sequence protocol (by defining __getitem__()).

As it happens, both the __iter__() and __getitem__() functions are required to raise an exception when the items to iterate over are exhausted.
 __iter__() raises the StopIteration exception, as discussed earlier, and __getitem__() raises the IndexError exception. This is how for knows when to stop
 
 

Raising the exceptions :
-------------------------------------------------------------------------------
def _check_parents_equal(self, other):
        if not self._parents_equal(other):
            raise TypeError(
                'This operation is valid only for enum values of the same type')
                
                
catching it exception passing it for another try:

def __getitem__(cls, other):
        try:
            _check_parents_equal(self, other)
        except TypeError:
            pass
        try:
            return cls._enums_by_str[name]
        except KeyError:
            return cls._enums_by_int[name]

			
The try statement has another optional clause which is intended to define clean-up actions that must be executed under all circumstances. For example:

>>>
>>> try:
...     raise KeyboardInterrupt
... finally:
...     print 'Goodbye, world!'
...
Goodbye, world!
KeyboardInterrupt
Traceback (most recent call last):
  File "<stdin>", line 2, in ?
A finally clause is always executed before leaving the try statement, whether an exception has occurred or not. When an exception has occurred in the try clause and has not been handled by an except clause (or it has occurred in a except or else clause), it is re-raised after the finally clause has been executed. The finally clause is also executed “on the way out” when any other clause of the try statement is left via a break, continue or return statement. A more complicated example (having except and finally clauses in the same try statement works as of Python 2.5):

>>>
>>> def divide(x, y):
...     try:
...         result = x / y
...     except ZeroDivisionError:
...         print "division by zero!"
...     else:
...         print "result is", result
...     finally:
...         print "executing finally clause"
...
>>> divide(2, 1)
result is 2
executing finally clause
>>> divide(2, 0)
division by zero!
executing finally clause
>>> divide("2", "1")
executing finally clause
Traceback (most recent call last):
  File "<stdin>", line 1, in ?
  File "<stdin>", line 3, in divide
TypeError: unsupported operand type(s) for /: 'str' and 'str'
As you can see, the finally clause is executed in any event. The TypeError raised by dividing two strings is not handled by the except clause and therefore re-raised after the finally clause has been executed.

In real world applications, the finally clause is useful for releasing external resources (such as files or network connections), regardless of whether the use of the resource was successful
C
Catching Exception of any Type :
---------------------------------
try:
    a = 2/0
except Exception as e:
    print e.__doc__
    print e.message

This wil catch andy exception and print it .


User-defined Exceptions:
---------------------------
Programs may name their own exceptions by creating a new exception class (see Classes for more about Python classes). Exceptions should typically be derived from the Exception class, either directly or indirectly. For example:

>>>
>>> class MyError(Exception):
...     def __init__(self, value):
...         self.value = value
...     def __str__(self):
...         return repr(self.value)
...
>>> try:
...     raise MyError(2*2)
... except MyError as e:
...     print 'My exception occurred, value:', e.value
...
My exception occurred, value: 4
>>> raise MyError('oops!')
Traceback (most recent call last):
  File "<stdin>", line 1, in ?
__main__.MyError: 'oops!'
 Memory managment in Python:
 ===================================================================================
 
 private heap
-----------------------------------------
| all Python objects and data structures |
------------------------------------------
				|
		(Python memory manager) # this controls the heap`
				
				
Steps for Allocations:
1. At the lowest Level a raw memory allocator ensures enough room in the private heap 
for storing all Python-related data by interacting with the memory manager od OS.
2. several object-specific allocators operate on the same heap and implement distinct memory management(lke for int / dict etc)
The default raw memory block allocator uses the following functions: malloc(), calloc(), realloc() and free(); call malloc(1)

class Resource:
    def __init__(self, name):
        self.handle = allocate_resource(name)
    def __del__(self):
        if self.handle:
            self.close()
    def close(self):
        release_resource(self.handle)
        self.handle = None
    ...

for name in big_list:
    x = Resource(name)
    do something with x
In current releases of CPython, each new assignment to x inside the loop will release the previously allocated resource. 
Using GC, this is not guaranteed. If you want to write code that will work with any Python implementation, you should explicitly
close the resource; this will work regardless of GC:

for name in big_list:
    x = Resource()
    do something with x
    x.close()


Python heap is performed by the interpreter itself and that the user has no control over it, even if she regularly manipulates
object pointers to memory blocks inside that heap.
			The allocation of heap space for Python objects and other internal buffers is performed on demand by the Python memory manager through
			
Decorators:
=================================
Python's functions are objects
To understand decorators, you must first understand that functions are objects in Python. 
This has important consequences. 

    
Functions references:
===========================
functions are objects and therefore can be assigned to a variable can be defined in another function.
Well, that means that a function can return another function :-) Have a look:

def getTalk(type="shout"):

    # We define functions on the fly
    def shout(word="yes"):
        return word.capitalize()+"!"

    def whisper(word="yes") :
        return word.lower()+"...";

    # Then we return one of them
    if type == "shout":
        # We don't use "()", we are not calling the function,
        # we are returning the function object
        return shout  
    else:
        return whisper

# How do you use this strange beast?

# Get the function and assign it to a variable
	talk = getTalk()      

# You can see that "talk" is here a function object:
	print talk
#outputs : <function shout at 0xb7ea817c>

# The object is the one returned by the function:
	print talk()
#outputs : Yes!

# And you can even use it directly if you feel wild:
	print getTalk("whisper")()
#outputs : yes   
But wait, there is more. If you can return a function, then you can pass one as a parameter:

def doSomethingBefore(func): 
    print "I do something before then I call the function you gave me"
    print func()

	doSomethingBefore(scream)
#outputs: 
#I do something before then I call the function you gave me
#Yes!
Well, you just have everything needed to understand decorators. You see, decorators are wrappers which means that they let you execute code before and after the function they decorate without the need to modify the function itself.


Passing Multiple Arguments to a Function at a shot :
==============================================================================================
def test_var_args_call(arg1, arg2, arg3):
    print "arg1:", arg1
    print "arg2:", arg2
    print "arg3:", arg3


Passing as a Tuple: Just create a tuple with the arguments and Pass it in the definations 

args = ("two", 3)
test_var_args_call(1, *args)

passing as a Dictionary:  will Similarly just keep the names same of the keys with the args in the function 

kwargs = {"arg3": 3, "arg2": "two"}
test_var_args_call(1, **kwargs)


Handcrafted decorators
==================================================================================================
How you would do it manually:

# A decorator is a function that expects ANOTHER function as parameter
def my_shiny_new_decorator(a_function_to_decorate):

    # Inside, the decorator defines a function on the fly: the wrapper.
    # This function is going to be wrapped around the original function
    
    def the_wrapper_around_the_original_function():

        print "Before the function runs"

        # Call the function here (using parentheses) Actual function call
        a_function_to_decorate()

        print "After the function runs"

    # At this point, "a_function_to_decorate" HAS NEVER BEEN EXECUTED.
    # We return the wrapper function we have just created.
    return the_wrapper_around_the_original_function

# Now imagine you create a function you don't want to ever touch again.
def a_stand_alone_function():
    print "I am a stand alone function, don't you dare modify me"

a_stand_alone_function() 
#outputs: I am a stand alone function, don't you dare modify me

Well, you can decorate it to extend its behavior. Just pass it to the decorator, it will wrap it dynamically in 
 any code you want and return you a new function ready to be used:

a_stand_alone_function_decorated = my_shiny_new_decorator(a_stand_alone_function)
a_stand_alone_function_decorated()


#outputs:
 Before the function runs
 I am a stand alone function, don't you dare modify me
 After the function runs
Now, you probably want that every time you call a_stand_alone_function,
a_stand_alone_function_decorated is called instead. That's easy, just overwrite a_stand_alone_function with 
the function returned by my_shiny_new_decorator:


Decorators demystified
The previous example, using the decorator syntax Now to reuse the above decorator created above and 

@my_shiny_new_decorator
def another_stand_alone_function():
    print "Leave me alone"

another_stand_alone_function()  
#outputs:  
 Before the function runs
 Leave me alone
 After the function runs
 
Yes, that's all, it's that simple. @decorator is just a shortcut to:

another_stand_alone_function = my_shiny_new_decorator(another_stand_alone_function)
Decorators are just a pythonic variant of the decorator design pattern. 
There are several classic design patterns embedded in Python to ease development, like iterators.
Of course, you can cumulate decorators:

def bread(func):
    def wrapper():
        print "</''''''\>"
        func()
        print "<\______/>"
    return wrapper

def ingredients(func):
    def wrapper():
        print "#tomatoes#"
        func()
        print "~salad~"
    return wrapper

def sandwich(food="--ham--"):
    print food

sandwich()
#outputs: --ham--
sandwich = bread(ingredients(sandwich))
sandwich()
#outputs:
#</''''''\>
# #tomatoes#
# --ham--
# ~salad~
#<\______/>
Using the Python decorator syntax:

@bread
@ingredients
def sandwich(food="--ham--"):
    print food

sandwich()
#outputs:
#</''''''\>
# #tomatoes#
# --ham--
# ~salad~
#<\______/>
The order you set the decorators MATTERS:

@ingredients
@bread
def strange_sandwich(food="--ham--"):
    print food

strange_sandwich()
#outputs:
##tomatoes#
#</''''''\>
# --ham--
#<\______/>
# ~salad~
Eventually answering the question
As a conclusion, you can easily see how to answer the question:

# The decorator to make it bold
def makebold(fn):
    # The new function the decorator returns
    def wrapper():
        # Insertion of some code before and after
        return "<b>" + fn() + "</b>"
    return wrapper

# The decorator to make it italic
def makeitalic(fn):
    # The new function the decorator returns
    def wrapper():
        # Insertion of some code before and after
        return "<i>" + fn() + "</i>"
    return wrapper

@makebold
@makeitalic
def say():
    return "hello"

print say() 
#outputs: <b><i>hello</i></b>

# This is the exact equivalent to 
def say():
    return "hello"
say = makebold(makeitalic(say))

print say() 
#outputs: <b><i>hello</i></b>
You can now just leave happy, or burn your brain a little bit more and see advanced uses of decorators.

Passing arguments to the decorated function
# It's not black magic, you just have to let the wrapper 
# pass the argument:

def a_decorator_passing_arguments(function_to_decorate):
    def a_wrapper_accepting_arguments(arg1, arg2):
        print "I got args! Look:", arg1, arg2
        function_to_decorate(arg1, arg2)
    return a_wrapper_accepting_arguments

# Since when you are calling the function returned by the decorator, you are
# calling the wrapper, passing arguments to the wrapper will let it pass them to 
# the decorated function

@a_decorator_passing_arguments
def print_full_name(first_name, last_name):
    print "My name is", first_name, last_name

print_full_name("Peter", "Venkman")
# outputs:
#I got args! Look: Peter Venkman
#My name is Peter Venkman
Decorating methods
What's great with Python is that methods and functions are really the same,
except methods expect their first parameter to be a reference to the current object (self).
It means you can build a decorator for methods the same way, just remember to take self in consideration:

def method_friendly_decorator(method_to_decorate):
    def wrapper(self, lie):
        lie = lie - 3 # very friendly, decrease age even more :-)
        return method_to_decorate(self, lie)
    return wrapper


class Lucy(object):

    def __init__(self):
        self.age = 32

    @method_friendly_decorator
    def sayYourAge(self, lie):
        print "I am %s, what did you think?" % (self.age + lie)

l = Lucy()
l.sayYourAge(-3)
#outputs: I am 26, what did you think?
Of course, if you make a very general decorator and want to apply it to any function or method, no matter
its arguments, then just use *args, **kwargs:

def a_decorator_passing_arbitrary_arguments(function_to_decorate):
    # The wrapper accepts any arguments
    def a_wrapper_accepting_arbitrary_arguments(*args, **kwargs):
        print "Do I have args?:"
        print args
        print kwargs
        # Then you unpack the arguments, here *args, **kwargs
        # If you are not familiar with unpacking, check:
        # http://www.saltycrane.com/blog/2008/01/how-to-use-args-and-kwargs-in-python/
        function_to_decorate(*args, **kwargs)
    return a_wrapper_accepting_arbitrary_arguments

@a_decorator_passing_arbitrary_arguments
def function_with_no_argument():
    print "Python is cool, no argument here."

function_with_no_argument()
#outputs
#Do I have args?:
#()
#{}
#Python is cool, no argument here.

@a_decorator_passing_arbitrary_arguments
def function_with_arguments(a, b, c):
    print a, b, c

function_with_arguments(1,2,3)
#outputs
#Do I have args?:
#(1, 2, 3)
#{}
#1 2 3 

@a_decorator_passing_arbitrary_arguments
def function_with_named_arguments(a, b, c, platypus="Why not ?"):
    print "Do %s, %s and %s like platypus? %s" %\
    (a, b, c, platypus)

function_with_named_arguments("Bill", "Linus", "Steve", platypus="Indeed!")
#outputs
#Do I have args ? :
#('Bill', 'Linus', 'Steve')
#{'platypus': 'Indeed!'}
#Do Bill, Linus and Steve like platypus? Indeed!

class Mary(object):

    def __init__(self):
        self.age = 31

    @a_decorator_passing_arbitrary_arguments
    def sayYourAge(self, lie=-3): # You can now add a default value
        print "I am %s, what did you think ?" % (self.age + lie)

m = Mary()
m.sayYourAge()
#outputs
# Do I have args?:
#(<__main__.Mary object at 0xb7d303ac>,)
#{}
#I am 28, what did you think?
Passing arguments to the decorator
Great, now what would you say about passing arguments to the decorator itself? Well this is a bit twisted because
a decorator must accept a function as an argument and therefore, you cannot pass the decorated function arguments 
directly to the decorator.

Before rushing to the solution, let's write a little reminder:

# Decorators are ORDINARY functions
def my_decorator(func):
    print "I am a ordinary function"
    def wrapper():
        print "I am function returned by the decorator"
        func()
    return wrapper

# Therefore, you can call it without any "@"

def lazy_function():
    print "zzzzzzzz"

decorated_function = my_decorator(lazy_function)
#outputs: I am a ordinary function 

# It outputs "I am a ordinary function", because that's just what you do:
# calling a function. Nothing magic.

@my_decorator
def lazy_function():
    print "zzzzzzzz"

#outputs: I am a ordinary function
It's exactly the same. "my_decorator" is called. So when you @my_decorator, you are telling Python to call 
the function 'labeled by the variable "my_decorator"'. It's important, because the label you give can point directly to the decorator 
or not! Let's start to be evil!

def decorator_maker():

    print "I make decorators! I am executed only once: "+\
          "when you make me create a decorator."

    def my_decorator(func):

        print "I am a decorator! I am executed only when you decorate a function."

        def wrapped():
            print ("I am the wrapper around the decorated function. "
                  "I am called when you call the decorated function. "
                  "As the wrapper, I return the RESULT of the decorated function.")
            return func()

        print "As the decorator, I return the wrapped function."

        return wrapped

    print "As a decorator maker, I return a decorator"
    return my_decorator

# Let's create a decorator. It's just a new function after all.
new_decorator = decorator_maker()       
#outputs:
#I make decorators! I am executed only once: when you make me create a decorator.
#As a decorator maker, I return a decorator

# Then we decorate the function

def decorated_function():
    print "I am the decorated function."

decorated_function = new_decorator(decorated_function)
#outputs:
#I am a decorator! I am executed only when you decorate a function.
#As the decorator, I return the wrapped function

# Let's call the function:
decorated_function()
#outputs:
#I am the wrapper around the decorated function. I am called when you call the decorated function.
#As the wrapper, I return the RESULT of the decorated function.
#I am the decorated function.
No surprise here. Let's do EXACTLY the same thing, but skipping intermediate variables:

def decorated_function():
    print "I am the decorated function."
decorated_function = decorator_maker()(decorated_function)
#outputs:
#I make decorators! I am executed only once: when you make me create a decorator.
#As a decorator maker, I return a decorator
#I am a decorator! I am executed only when you decorate a function.
#As the decorator, I return the wrapped function.

# Finally:
decorated_function()    
#outputs:
#I am the wrapper around the decorated function. I am called when you call the decorated function.
#As the wrapper, I return the RESULT of the decorated function.
#I am the decorated function.
Let's make it AGAIN, even shorter:

@decorator_maker()
def decorated_function():
    print "I am the decorated function."
#outputs:
#I make decorators! I am executed only once: when you make me create a decorator.
#As a decorator maker, I return a decorator
#I am a decorator! I am executed only when you decorate a function.
#As the decorator, I return the wrapped function.

#Eventually: 
decorated_function()    
#outputs:
#I am the wrapper around the decorated function. I am called when you call the decorated function.
#As the wrapper, I return the RESULT of the decorated function.
#I am the decorated function.
Hey, did you see that? We used a function call with the "@" syntax :-)

So back to decorators with arguments. If we can use functions to generate the decorator on the fly, we can pass arguments to that function, right?

def decorator_maker_with_arguments(decorator_arg1, decorator_arg2):

    print "I make decorators! And I accept arguments:", decorator_arg1, decorator_arg2

    def my_decorator(func):
        # The ability to pass arguments here is a gift from closures.
        print "I am the decorator. Somehow you passed me arguments:", decorator_arg1, decorator_arg2

        # Don't confuse decorator arguments and function arguments!
        def wrapped(function_arg1, function_arg2) :
            print ("I am the wrapper around the decorated function.\n"
                  "I can access all the variables\n"
                  "\t- from the decorator: {0} {1}\n"
                  "\t- from the function call: {2} {3}\n"
                  "Then I can pass them to the decorated function"
                  .format(decorator_arg1, decorator_arg2,
                          function_arg1, function_arg2))
            return func(function_arg1, function_arg2)

        return wrapped

    return my_decorator

@decorator_maker_with_arguments("Leonard", "Sheldon")
def decorated_function_with_arguments(function_arg1, function_arg2):
    print ("I am the decorated function and only knows about my arguments: {0}"
           " {1}".format(function_arg1, function_arg2))

decorated_function_with_arguments("Rajesh", "Howard")
#outputs:
#I make decorators! And I accept arguments: Leonard Sheldon
#I am the decorator. Somehow you passed me arguments: Leonard Sheldon
#I am the wrapper around the decorated function. 
#I can access all the variables 
#   - from the decorator: Leonard Sheldon 
#   - from the function call: Rajesh Howard 
#Then I can pass them to the decorated function
#I am the decorated function and only knows about my arguments: Rajesh Howard

A decorator to decorate a decorator OK, as a bonus, 

def decorator_with_args(decorator_to_enhance):
    """ 
    This function is supposed to be used as a decorator.
    It must decorate an other function, that is intended to be used as a decorator.
    Take a cup of coffee.
    It will allow any decorator to accept an arbitrary number of arguments,
    saving you the headache to remember how to do that every time.
    """

    # We use the same trick we did to pass arguments
    def decorator_maker(*args, **kwargs):

        # We create on the fly a decorator that accepts only a function
        # but keeps the passed arguments from the maker.
        def decorator_wrapper(func):

            # We return the result of the original decorator, which, after all, 
            # IS JUST AN ORDINARY FUNCTION (which returns a function).
            # Only pitfall: the decorator must have this specific signature or it won't work:
            return decorator_to_enhance(func, *args, **kwargs)

        return decorator_wrapper

    return decorator_maker
It can be used as follows:

# You create the function you will use as a decorator. And stick a decorator on it :-)
# Don't forget, the signature is "decorator(func, *args, **kwargs)"
@decorator_with_args 
def decorated_decorator(func, *args, **kwargs): 
    def wrapper(function_arg1, function_arg2):
        print "Decorated with", args, kwargs
        return func(function_arg1, function_arg2)
    return wrapper

# Then you decorate the functions you wish with your brand new decorated decorator.

@decorated_decorator(42, 404, 1024)
def decorated_function(function_arg1, function_arg2):
    print "Hello", function_arg1, function_arg2

decorated_function("Universe and", "everything")
#outputs:
#Decorated with (42, 404, 1024) {}
#Hello Universe and everything

# Whoooot!
I know, the last time you had this feeling, it was after listening a guy saying: "before understanding recursion, you must first understand recursion". But now, don't you feel good about mastering this?

Best practices while using decorators
====================================================================================
They are new as of Python 2.4, so be sure that's what your code is running on.
Decorators slow down the function call. Keep that in mind.
You can not un-decorate a function. There are hacks to create decorators that can be removed but nobody uses them.
 So once a function is decorated, it's done. For all the code.
Decorators wrap functions, which can make them hard to debug.
Python 2.5 solves this last issue by providing the functools module including functools.wraps that copies the name, module and docstring
of any wrapped function to it's wrapper. Func fact, functools.wraps is a decorator :-)

 # "functools" can help for that

import functools

def bar(func):
    # We say that "wrapper", is wrapping "func"
    # and the magic begins
    @functools.wraps(func)i
    def wrapper():
        print "bar"
        return func()
    return wrapper

@bar
def foo():
    print "foo"

print foo.__name__
#outputs: foo
How can the decorators be useful?
===========================================
Now the big question: what can I use decorators for? Seem cool and powerful, but a practical example would be great. Well, there are 1000 possibilities.
Classic uses are extending a function behavior from an external lib (you can't modify it) or for a debug purpose (you don't want to modify it because
 it's temporary). You can use them to extends several functions with the same code without rewriting it every time, for DRY's sake. E.g.:

 Checking Execution time:
 ***************************
def benchmark(func):
    """
    A decorator that prints the time a function takes
    to execute.
    """
    import time
    def wrapper(*args, **kwargs):
        t = time.clock()
        res = func(*args, **kwargs)
        print func.__name__, time.clock()-t
        return res
    return wrapper


def logging(func):
    """
    A decorator that logs the activity of the script.
    (it actually just prints it, but it could be logging!)
    """
    def wrapper(*args, **kwargs):
        res = func(*args, **kwargs)
        print func.__name__, args, kwargs
        return res
    return wrapper


def counter(func):
    """
    A decorator that counts and prints the number of times a function has been executed
    """
    def wrapper(*args, **kwargs):
        wrapper.count = wrapper.count + 1
        res = func(*args, **kwargs)
        print "{0} has been used: {1}x".format(func.__name__, wrapper.count)
        return res
    wrapper.count = 0
    return wrapper

@counter
@benchmark
@logging
def reverse_string(string):
    return str(reversed(string))

print reverse_string("Able was I ere I saw Elba")
print reverse_string("A man, a plan, a canoe, pasta, heros, rajahs, a coloratura, maps, snipe, percale, macaroni, a gag, a banana bag, a tan, a tag, a banana bag again (or a camel), a crepe, pins, Spam, a rut, a Rolo, cash, a jar, sore hats, a peon, a canal: Panama!")

#outputs:
#reverse_string ('Able was I ere I saw Elba',) {}
#wrapper 0.0
#wrapper has been used: 1x 
#ablE was I ere I saw elbA
#reverse_string ('A man, a plan, a canoe, pasta, heros, rajahs, a coloratura, maps, snipe, percale, macaroni, a gag, a banana bag, a tan, a tag, a banana bag again (or a camel), a crepe, pins, Spam, a rut, a Rolo, cash, a jar, sore hats, a peon, a canal: Panama!',) {}
#wrapper 0.0
#wrapper has been used: 2x
#!amanaP :lanac a ,noep a ,stah eros ,raj a ,hsac ,oloR a ,tur a ,mapS ,snip ,eperc a ,)lemac a ro( niaga gab ananab a ,gat a ,nat a ,gab ananab a ,gag a ,inoracam ,elacrep ,epins ,spam ,arutaroloc a ,shajar ,soreh ,atsap ,eonac a ,nalp a ,nam A
Of course the good thing with decorators is that you can use them right away on almost anything without rewriting. DRY, I said:

@counter
@benchmark
@logging
def get_random_futurama_quote():
    import httplib
    conn = httplib.HTTPConnection("slashdot.org:80")
    conn.request("HEAD", "/index.html")
    for key, value in conn.getresponse().getheaders():
        if key.startswith("x-b") or key.startswith("x-f"):
            return value
    return "No, I'm     doesn't!"

print get_random_futurama_quote()
print get_random_futurama_quote()

#outputs:
#get_random_futurama_quote () {}
#wrapper 0.02
#wrapper has been used: 1x
#The laws of science be a harsh mistress.
#get_random_futurama_quote () {}
#wrapper 0.01
#wrapper has been used: 2x
#Curse you, merciful Poseidon!

Python itself provides several decorators: property, staticmethod, etc. Django use decorators to manage caching and view permissions.
Twisted to fake inlining asynchronous functions calls. This really is a large playground.

EDIT: given the success of this answer, and people asking me to do the same with metaclasses, I did.
The common patterns for decorators are:
Argument checking
Caching
Proxy
Context provider

1. Argument checking
Checking the arguments that a function receives or returns can be useful when it is
executed in a specific context. For example, if a function is to be called through
XML-RPC, Python will not be able to directly provide its full signature as in
the statically-typed languages.(to check the datatypes of argument wen pass/ret to a function)

A decorator can be used to fixed the allowed type of user defined types fixed a class which implemnets 
a decorator
# Dict to store the 
rpc_info = {} # dictionary of function name and tuple of Arguments 
def xmlrpc(in=(),out=(type(None)): #Wrapper Bread wd default arg tuple as in/out
	#register the Signature allowed 
	func_name= inspect.stack()[1][3] # Get the function name which is calling this decorator
	rpc_info[func_name]=(in,out) # the Global dict is Update Nameand args
	
	def _check_types(elements, types):
		"""Subfunction that checks the types."""
		if len(elements) != len(types):    #
			raise TypeError('argument count is wrong')
		typed = enumerate(izip(elements, types))#(int,string)(int,string)=(int,int)(string,string)
		
		for index, couple in typed:
			arg, of_the_right_type = couple
			if isinstance(arg, of_the_right_type):
				continue
			raise TypeError('arg #%d should be %s' % \(index, of_the_right_type)
	
	#Ingredeints  wrapper sits above sandvich 
	def __xmlrpc(*args): #args = (arg1,arg2)
		#Extracting the Args input
		input_args = arg[1:]
		_check_types(input_args,in) # Lets current arg list wd the fixed type 
		# Now free to call our Sandvich function with arguments
		res = function(*args) ## The function call is the name taken from above
		
		#Check the output as  a list or Tuple 
		if not Type(res) in [list,tuple]
			chck_res= (res,)
		else:
			chck_res=  res
		_check_type(chck_res)
		return res
	return __xmlrpc
return _xmlrpc

Lets write a class that uses this decorator 
class RPC(object):
	@xmlrpc((int,int))
	def meth1(int,int):
		print "received 2 ints as arg n ret type"

	@xmlrpc((int,),(int,))
	def meth2(int):
		print "received 1 ints as arg n ret type as int"
			
lets use the class 
my =RPC()
my.meth1(1,2)  # Runs OK 
my.meth2(1,2) #Throws an error 

2. Caching :
A simple decorator but focuses on those functions whose internal state does not affect the output. Each set of
arguments can be linked to a unique result. So is a good decorator to provide on CPU intensive functions,
the idea over here is to generate a key with args and preserve it for a particular duration 

cache = {} # this maintains a dict of the keys 
def is_obsolute(entry,duration):
	# thsi shuold return the cur time diffrence wd the func entry time 
	return (time.time() - entry['time']) > duration
	
def compute_key(function,args,kw):
	key = pickle.dumps((function.func_name,args,kw))
	return hashlib.sha1(key).hexdigest()
	
def memorize(dur=10):
	def _memorize(function):
		def __memorize(*args,**kw):
			key = compute_key(function,args,kw)
			# check the key exixts or not 
			if (key in cache) and not is_obsolute(cache[key],duration):
				return cache[key]['value']
			# computing 
			result = function(*args,**kw)
			#storing the result 
			cache[key]={'value': result, 'time': time.time()}
			return result
		return __memorize
	return _memorize

@memorize()
def add_it(a,b):
	return a + b
	
add_it(2,2) # 4 

Update the dur @memorize(2)

3. Proxy :
Proxy decorators are used to tag and register functions with a global mechanism,
For the use case we try to put a 

UserPerev = {}
class User(object):
	def __init__(self,user,role)
		self.role = role
		self.user = user
		UserPerev[self.user]=self.role
		
def protect(role):
	def _protect(object):
		def __protect(*args,**krgs):
			if role is None or @protect('admin')='Admin'
					throw an exception
			return (func(*args))
		return __protect
	return _protect		
	
	
Ram = User(Ram,'admin')

class Myprojcts():
		@protect('admin')
		def waffle_recipe(self):
			print 'use tons of butter!'
			
>>user = ram
>>these_are = Myprojcts()

Class Decorators:
------------------------------------
while similar in concept to function decorators, class decorators
are applied to classes instead—they may be used either to manage classes themselves,
or to intercept instance creation calls in order to manage instances.
The basic class decorator is shown up with an example of singleton which interceptsa
class during objecty creation 

here the singleton decorator will be used whenever
any class is decorated using the decorator 
A singleton decorator function for classes :
---------------------------------------------
instances = {}
def singleton(aClass): # On @ decoration
	def onCall(*args, **kwargs): # On instance creation
		if aClass not in instances: # One dict entry per class
			instances[aClass] = aClass(*args, **kwargs)
		return instances[aClass]
	return onCall
	
decorate the classes for which you want to enforce a single-instance model
(for reference, all the code in this section is in the file singletons.py):

@singleton # Person = singleton(Person)
class Person: # Rebinds Person to onCall
	def __init__(self, name): # onCall remembers Person
		self.name = name
Use:
	bob = Person('Bob', 40, 10) # Really calls onCall
	print(bob.name, bob.pay())
	
	sue = Person('Sue', 50, 20) # Same, single object
	print(sue.name, sue.pay())
wrapping up Multipleclass methods by a Class:
------------------------------------------------
This technique is used by ocerlaoding __getattr__ insdide the class
class Wrapper:
	def __init__(self, object):
		self.wrapped = object # Save object
	def __getattr__(self, attrname):
		print('Trace:', attrname) # Trace fetch
		return getattr(self.wrapped, attrname) # Delegate fetch
>>> x = Wrapper([1,2,3]) # Wrap a list
>>> x.append(4) # Delegate to list method
Trace: append
>>> x.wrapped ## [1, 2, 3, 4]

Doing the same using a class decorator:
-----------------------------------------
The init of the Wrapper will 
def Tracer(aClass): # On @ decorator
	class Wrapper:
		def __init__(self, *args, **kargs): # On instance creation
			self.fetches = 0
			self.wrapped = aClass(*args, **kargs) # Use enclosing scope name
		def __getattr__(self, attrname):
			print('Trace: ' + attrname) # Catches all but own attrs
			self.fetches += 1
			return getattr(self.wrapped, attrname) # Delegate to wrapped obj
		return Wrapper
Uses 
-----		
@Tracer
class Spam: # Spam = Tracer(Spam)
	def display(self): # Spam is rebound to Wrapper
	print('Spam!' * 8)
##
food = Spam() # Triggers Wrapper()
food.display() # Triggers __getattr__
print([food.fetches]) ## The instance can fetch a init deco variable
the class decorator here allows us to trace an entire
object interface—that is, accesses to any of the instance’s attributes.

"""
File access1.py (3.X + 2.X)
Privacy for attributes fetched from class instances.
See self-test code at end of file for a usage example.
Decorator same as: Doubler = Private('data', 'size')(Doubler).
Private returns onDecorator, onDecorator returns onInstance,
and each onInstance instance embeds a Doubler instance.
"""
traceMe = False
def trace(*args):
	if traceMe: print('[' + ' '.join(map(str, args)) + ']')
def Private(*privates): # privates in enclosing scope
	def onDecorator(aClass): # aClass in enclosing scope
		class onInstance: # wrapped in instance attribute
			def __init__(self, *args, **kargs):
				self.wrapped = aClass(*args, **kargs)
			def __getattr__(self, attr): # My attrs don't call getattr
				trace('get:', attr) # Others assumed in wrapped
				if attr in privates:
					raise TypeError('private attribute fetch: ' + attr)
				else:
					return getattr(self.wrapped, attr)
			def __setattr__(self, attr, value): # Outside accesses
				trace('set:', attr, value) # Others run normally
				if attr == 'wrapped': # Allow my attrs
					self.__dict__[attr] = value # Avoid looping
				elif attr in privates:
					raise TypeError('private attribute change: ' + attr)
				else:
					setattr(self.wrapped, attr, value) # Wrapped obj attrs
		return onInstance # Or use __dict__
	return onDecorator
if __name__ == '__main__':
	traceMe = True
	@Private('data', 'size') # Doubler = Private(...)(Doubler)
	class Doubler:
Example
def __init__(self, label, start):
	self.label = label # Accesses inside the subject class
	self.data = start # Not intercepted: run normally
def size(self):
	return len(self.data) # Methods run with no checking
def double(self): # Because privacy not inherited
	for i in range(self.size()):
		self.data[i] = self.data[i] * 2
	X = Doubler('X is', [1, 2, 3])
	Y = Doubler('Y is', [-10, −20, −30])
# The following all succeed
	print(X.label) # Accesses outside subject class
	X.display(); X.double(); X.display() # Intercepted: validated, delegated
	print(Y.label)





4. Context Provider
------------------------------------

Context Manager :
=======================================================================
the context manager is used to provide the Threading module in python 
Now to know more about the context manager the lets know the 'with'

Python "with" Statement by Example
 The classic example is opening a file, manipulating the file, then closing it:

with open('output.txt', 'w') as f:
    f.write('Hi there!')
The above with statement will automatically close the file after the nested block of code.
The advantage of using a with statement is that it is guaranteed to close the file no matter
 how the nested block exits. If an exception occurs before the end of the block, it will close
 the file before the exception is caught by an outer exception handler. If the nested block
 were to contain a return statement, or a continue or break statement, the with statement would
 automatically close the file in those cases, too.
 
 
 lets go for an example 
 to send multiple number of udp packets suppose 6 paqckets 
 
 send_udp.buffer()
 for i in xrange(6)
	send_udp.open()
	send_udp.connect()
	send_udp.write()
	send_udp.close()
	
	
now this will become problematic to update if the open and the close is not mapped properlly 
for more number of the sockets 
So this bahaviour can be ensued by writing the Context manager and there are Two ways to
 do that 
1. Implementing the Context Manager as a Class
***************************************************
Here’s the first approach. To implement a context manager, we define a class containing an
 __enter__ and __exit__ method. The class below accepts a send_udp context, cr, in its constructor:

class Socket():
    def __init__(self, send_udp):
        self.send_udp = send_udp
    def __enter__(self):
        self.send_udp.open()
        return self.send_udp
    def __exit__(self, type, value, traceback):
		
        self.send_udp.close()
		
Thanks to those two methods, it’s valid to instantiate a Socket object and use it in a with statement.
 The Socket object is considered to be the context manager.


with Socket(send_udp):
	send_udp.connect()
	send_udp.write()
        
Here are the exact steps taken by the Python interpreter when it reaches the with statement:
--------------------------------------------------------------------------------------------
1. The with statement stores the Socket object in a temporary, hidden variable, since it’ll be needed later.
 (Actually, it only stores the bound __exit__ method, but that’s a detail.)
2. The with statement calls __enter__ on the Socket object, giving the context manager a chance to do its job.
	The __enter__ method calls open on the send_udp context.
	The __enter__ method returns the send_udp context, but as you can see, we have not
	specified the optional "as" 
	target part of the with statement. Therefore, the return value is not Socket anywhere.
	We don’t need it; we know it’s the same send_udp context that we passed in.

3. The nested block of code is executed. connect and write
4. At the end of the nested block, the with statement calls the Socket object’s __exit__ method, passing the arguments 
	(None, None, None) to indicate that no exception occured.
5. The __exit__ method calls close on the send_udp context.
Once we understand what the Python interpreter is doing, we can make better sense of the example at the beginning
 of this blog post, where we opened a file in the with statement: File objects expose their own __enter__ and __exit__
 methods, and can therefore act as their own context managers. 
 
 Steps when an Exception is occured during send_udp.connect()/send_udp.write()
 --------------------------------------------------------------------------------------------
1. The connect method raises a TypeError exception: “Context.connect() takes exactly 4 arguments.”
2. The with statement catches this exception.
3. The with statement calls __exit__ on the Socket object. It passes information about the exception 
 in four arguments:  __exit__(self, exc_type, exc_val, exc_tb): – the same values you’d get by calling sys.exc_info.
 
 This tells the __exit__ method everything it could possibly need to know about the
 exception(exc_type,exc_val) that occurred.
4.In this case, our __exit__ method doesn’t particularly care. It calls restore on the  context anyway,
 and returns None.(In Python, when no return statement is specified, the function actually returns None.)
5.The with statement checks to see whether this return value is true. Since it isn’t, the with statement 
 re-raises the TypeError exception to be handled by someone else.
In this manner, we can guarantee that restore will always be called on the  context, whether an exception occurs or not.


 
2. Implementing the Context Manager as a Generator
***************************************************
 we can write a generator function. Here’s a simplified example of such a generator function. 
 
from contextlib import contextmanager
@contextmanager
def Socket(send_udp):
    send_udp.open()
    yield send_udp
    send_udp.close()

To make this generator work, two entities from contextlib, a standard Python module, are required: the contextmanager 
function, and an internal class named GeneratorContextManager. The source code, contextlib.py, is a bit hairy, but at
 least it’s short. I’ll simply describe what happens, and you are free to refer to the source code, and any other
 supplementary materials, as needed.
 
 
1. The Python interpreter recognizes the yield statement in the middle of the function definition.
 As a result, the def statement does not create a normal function; it creates a generator function.
2. Because of the presence of the @contextmanager decorator, contextmanager is called with the generator function as its argument.
3. The contextmanager function returns a “factory” function, which creates GeneratorContextManager objects wrapped around the provided generator. 

Finally, the factory function is assigned to Socket. From this point on, when we call Socket, we’ll actually be calling the factory function.
The other conetxts usd are 
1. decimal context
2. 

What is Multithreading?
=============================================================================
A thread is short for a thread of execution. A programmer can split his or her work
into threads that run simultaneously and share the same memory context. 
Multi-threading will benefit from a multiprocessor or multi-core
machine and will parallelize each thread execution on each CPU, thus making the
program faster.

How Python Deals with Threads
=============================
Python uses multiple kernel-level threads that can each run any of the interpreter-level 
threads. However, all threads accessing Python objects are serialized by one global lock. 
This is done because much of the interpreter code as well as third-party C code is not
thread-safe and need to be protected.
This mechanism is called the Global Interpreter Lock (GIL)

When threads contain only pure Python code, there is no point in using threads to
speed up the program since the GIL will serialize it. However, multiple threads can
do IO operations or execute C code in certain third-party extensions parallelly.

Use of threads can be really useful in some cases. They can help in:
Building responsive interfaces
Delegating work
Building multi-user applications

Building Responsive Interfaces
Let's say you ask your system to copy files from a folder to another through a
graphical user interface. The task will possibly be pushed into the background
and the windows will be constantly refreshed by the main thread, so you get live
feedback on the operation. You will also be able to cancel the operation.

Delegating Work
If your process depends on third-party resources, threads might really speed up
everything.Let's take the case of a function that indexes files in a folder and pushes the built
indexes into a database. Depending on the type of file, the function calls a different
external program.

Starting a New Thread
=============================================================================
import thread
import time

# Define a function for the thread
def print_time( threadName, delay):
   count = 0
   while count < 5:
      time.sleep(delay)
      count += 1
      print "%s: %s" % ( threadName, time.ctime(time.time()) )

# Create two threads as follows
try:
   thread.start_new_thread( print_time, ("Thread-1", 2, ) )
   thread.start_new_thread( print_time, ("Thread-2", 4, ) )
except:
   print "Error: unable to start thread"

while 1:
   pass
   
When the above code is executed, it produces the following result −

Thread-1: Thu Jan 22 15:42:17 2009
Thread-1: Thu Jan 22 15:42:19 2009
Thread-2: Thu Jan 22 15:42:19 2009
Thread-1: Thu Jan 22 15:42:21 2009
Thread-2: Thu Jan 22 15:42:23 2009
Thread-1: Thu Jan 22 15:42:23 2009
Thread-1: Thu Jan 22 15:42:25 2009

Creating Thread Using Threading Module
=============================================================================
To implement a new thread using the threading module, you have to do the following −

Define a new subclass of the Thread class.

Override the __init__(self [,args]) method to add additional arguments.

Then, override the run(self [,args]) method to implement what the thread should do when started.

Once you have created the new Thread subclass, you can create an instance of it and then start a new thread by
invoking the start(), which in turn calls run() method.

Example
#!/usr/bin/python

import threading
import time

exitFlag = 0

class myThread (threading.Thread):
    def __init__(self, threadID, name, counter):
        threading.Thread.__init__(self)
        self.threadID = threadID
        self.name = name
        self.counter = counter
    def run(self):
        print "Starting " + self.name
        print_time(self.name, self.counter, 5)
        print "Exiting " + self.name

def print_time(threadName, delay, counter):
    while counter:
        if exitFlag:
            thread.exit()
        time.sleep(delay)
        print "%s: %s" % (threadName, time.ctime(time.time()))
        counter -= 1

# Create new threads
thread1 = myThread(1, "Thread-1", 1)
thread2 = myThread(2, "Thread-2", 2)

# Start new Threads
thread1.start()
thread2.start()

print "Exiting Main Thread"
When the above code is executed, it produces the following result −

Starting Thread-1
Starting Thread-2
Exiting Main Thread
Thread-1: Thu Mar 21 09:10:03 2013
Thread-1: Thu Mar 21 09:10:04 2013
Thread-2: Thu Mar 21 09:10:04 2013
Thread-1: Thu Mar 21 09:10:05 2013
Thread-1: Thu Mar 21 09:10:06 2013
Thread-2: Thu Mar 21 09:10:06 2013
Thread-1: Thu Mar 21 09:10:07 2013
Exiting Thread-1
Thread-2: Thu Mar 21 09:10:08 2013
Thread-2: Thu Mar 21 09:10:10 2013
Thread-2: Thu Mar 21 09:10:12 2013
Exiting Thread-2


Synchronizing Threads:
=============================================================================
The threading module provided with Python includes a simple-to-implement locking mechanism that allows you to synchronize threads.
A new lock is created by calling the Lock() method, which returns the new lock.

The acquire(blocking) method of the new lock object is used to force threads to run synchronously. 
The optional blocking parameter enables you to control whether the thread waits to acquire the lock.

If blocking is set to 0, the thread returns immediately with a 0 value if the lock cannot be acquired and with a 1 if the lock was acquired.
 If blocking is set to 1, the thread blocks and wait for the lock to be released.

The release() method of the new lock object is used to release the lock when it is no longer required.

Example
#!/usr/bin/python

import threading
import time

class myThread (threading.Thread):
    def __init__(self, threadID, name, counter):
        threading.Thread.__init__(self)
        self.threadID = threadID
        self.name = name
        self.counter = counter
    def run(self):
        print "Starting " + self.name
        # Get lock to synchronize threads
        threadLock.acquire()
        print_time(self.name, self.counter, 3)
        # Free lock to release next thread
        threadLock.release()

def print_time(threadName, delay, counter):
    while counter:
        time.sleep(delay)
        print "%s: %s" % (threadName, time.ctime(time.time()))
        counter -= 1

threadLock = threading.Lock()
threads = []

# Create new threads
thread1 = myThread(1, "Thread-1", 1)
thread2 = myThread(2, "Thread-2", 2)

# Start new Threads
thread1.start()
thread2.start()

# Add threads to thread list
threads.append(thread1)
threads.append(thread2)

# Wait for all threads to complete
for t in threads:
    t.join()
print "Exiting Main Thread"

When the above code is executed, it produces the following result −

Starting Thread-1
Starting Thread-2
Thread-1: Thu Mar 21 09:11:28 2013
Thread-1: Thu Mar 21 09:11:29 2013
Thread-1: Thu Mar 21 09:11:30 2013
Thread-2: Thu Mar 21 09:11:32 2013
Thread-2: Thu Mar 21 09:11:34 2013
Thread-2: Thu Mar 21 09:11:36 2013
Exiting Main Thread
Multithreaded Priority Queue
=============================================================================
The Queue module allows you to create a new queue object that can hold a specific number of items. There are following methods to control the Queue −

get(): The get() removes and returns an item from the queue.

put(): The put adds item to a queue.

qsize() : The qsize() returns the number of items that are currently in the queue.

empty(): The empty( ) returns True if queue is empty; otherwise, False.

full(): the full() returns True if queue is full; otherwise, False.

Example
#!/usr/bin/python

import Queue
import threading
import time

exitFlag = 0

class myThread (threading.Thread):
    def __init__(self, threadID, name, q):
        threading.Thread.__init__(self)
        self.threadID = threadID
        self.name = name
        self.q = q
    def run(self):
        print "Starting " + self.name
        process_data(self.name, self.q)
        print "Exiting " + self.name

def process_data(threadName, q):
    while not exitFlag:
        queueLock.acquire()
        if not workQueue.empty():
            data = q.get()
            queueLock.release()
            print "%s processing %s" % (threadName, data)
        else:
            queueLock.release()
        time.sleep(1)

threadList = ["Thread-1", "Thread-2", "Thread-3"]
nameList = ["One", "Two", "Three", "Four", "Five"]
queueLock = threading.Lock()
workQueue = Queue.Queue(10)
threads = []
threadID = 1

# Create new threads
for tName in threadList:
    thread = myThread(threadID, tName, workQueue)
    thread.start()
    threads.append(thread)
    threadID += 1

# Fill the queue
queueLock.acquire()
for word in nameList:
    workQueue.put(word)
queueLock.release()

# Wait for queue to empty
while not workQueue.empty():
    pass

# Notify threads it's time to exit
exitFlag = 1

# Wait for all threads to complete
for t in threads:
    t.join()
print "Exiting Main Thread"
When the above code is executed, it produces the following result −

Starting Thread-1
Starting Thread-2
Starting Thread-3
Thread-1 processing One
Thread-2 processing Two
Thread-3 processing Three
Thread-1 processing Four
Thread-2 processing Five
Exiting Thread-3
Exiting Thread-1
Exiting Thread-2
Exiting Main Thread

Simple Example
=============================================================================
Let's take a small example of an application that recursively scans a directory to
process text files. Each text file is opened and processed by an external converter.
Using threads will possibly make it faster because the indexation work can be done
simultaneously on several files.
The external converter is a small Python program that does some complex work:
#!/usr/bin/python
	for i in range(100000):
	i = str(i) + "y"*10000


In a multi-threaded solution, the main program deals with a pool of threads. Each
thread takes its work from a queue. In this use case, threads are called workers. The
queue is the shared resource where the main program adds files it has found walking
in the directory.It provides a multi-consumer, multiproducer 
FIFO queue that internally uses a deque instance and is thread-safe.
So if we want to process files that are in that queue, we just use the get method
together with task_done, which lets the Queue instance know that the task has be
finished for join to work:
   
The worker function, which will be called through a thread, takes file names from
the queue and processes them by calling the index_file function. The sleep call
simulates the process done by an external program, and makes the thread wait for
the results, and therefore unlock the GIL.
The main program can then launch workers, scan for files to be processed, and feed
the queue with them.
This is done by creating Thread instances with the worker method. The setDaemon
is necessary so that the threads automatically get shut down when the program exits.
Otherwise, the program would hang forever waiting for them to exit. This can be
manually managed but it is not useful here.
At the end of the index_files function, the join method will wait for the queue to
be fully processed.
Let's create a full script called indexer.py that runs a multithreaded version, and a
single thread to index a directory structure containing text files:


from threading import Thread
import os
import subprocess
from Queue import Queue
import logging
import time
import sys
from pbp.scripts.profiler import profile, print_stats
dirname = os.path.realpath(os.path.dirname(__file__))
CONVERTER = os.path.join(dirname, 'converter.py')
q = Queue()

def index_file(filename):
	f = open(filename)
	try:
		content = f.read()
		# process is here
		subprocess.call([CONVERTER])
		finally:
		f.close()
		
def worker():
	while True:
		index_file(q.get())
		q.task_done()
		
def index_files(files, num_workers):
	for i in range(num_workers):
		t = Thread(target=worker)
		t.setDaemon(True)
		t.start()
		for file in files:
			q.put(file)
			q.join()
			
def get_text_files(dirname):
	for root, dirs, files in os.walk(dirname):
		for file in files:
			if os.path.splitext(file)[-1] != '.txt':
				continue
			yield os.path.join(root, file)
			
@profile('process')
def process(dirname, numthreads):
	dirname = os.path.realpath(dirname)
	if numthreads > 1:
		index_files(get_text_files(dirname), numthreads)
	else:
		for f in get_text_files(dirname):
			index_file(f)
			
if __name__ == '__main__':
	process(sys.argv[1], int(sys.argv[2]))
	print_stats()
	
	
This script can be used with any directory as long as it contains text files. It takes two
parameters:
1. The name of the directory
2. The number of threads
When name of the directory is usedalone, no threads are launched and the directory
is processed in the main thread.Let's run it directory containing 36 files with 19 text files.
The directory is composed of a structure of 6 directories:
$ python2 indexer.py zc.buildout-1.0.6-py2.5.egg 1
process : 301.83 kstones, 6.821 secondes, 396 bytes
$ python indexer.py zc.buildout-1.0.6-py2.5.egg 2
---------------------------------------------------------------------------------------------


Pyprocessing
=============
pyprocessing provides a portable way to work with processes as if they
were threads.


Caching
========
The result of a function or a method that is expensive to run can be cached as long as:
The function is deterministic and results have the same value every time,
given the same input.
The return value of the function continues to be useful and valid for some
period of time (non-deterministic).
Good candidates for caching are usually:
Results from callables that query databases
Results from callables that render static values, like file content, web
requests, or PDF rendering
Results from deterministic callables that perform complex calculations
Global mappings that keep track of values with expiration times, like web
session objects
Some data that needs to be accessed often and quickly
Deterministic Caching:
=====================
A simple example of deterministic caching is a function that calculates a square.
Keeping track of the results allows you to speed it up:
 import random, timeit
 from pbp.scripts.profiler import profile, print_stats
 cache = {}
 def square(n):
    return n * n
   
 @profile(‘not cached’)
    def factory_calls():
    for i in xrange(100):
    square(random.randint(1, 10))
   
 def cached_factory(n):
    if n not in cache:
    cache[n] = square(n)
    return cache[n]
   
 @profile(‘cached’)
    def cached_factory_calls():
    n = [random.randint(1, 10) for i in range(100)]
    ns = [cached_factory(i) for i in n]
   
 factory_calls(); cached_factory_calls();
 print_stats()
not cached : 20.51 kstones, 0.340 secondes, 396 bytes
cached : 6.07 kstones, 0.142 secondes, 480 bytes


OPTIMISATION OF APPLICATION S
	
Time Checks for Modules :
-------------------------------------------
Write a Speed Test

def test_speed():
   import time
   start = time.time()
   #The function to be tested 
   func()
   interval =time.time() - start
   assert interval > 10
Profiling CPU Usage:
=============================================================================
Macro-profiling: Profiles the whole program while it is being used, and
generates statistics.
Micro-profiling: Measures a precise part of the program by instrumenting
it manually.

Macro-profiling:
=======================
python -m cProfile myapp.py
1212 function calls in 10.120 CPU seconds
Ordered by: standard name
ncalls tottime cumtime percall file
1 0.000 10.117 10.117 myapp.py:16(main)
400 0.004 4.077 0.010 myapp.py:3(lighter)
200 0.002 2.035 0.010 myapp.py:6(light)
2 0.005 10.117 5.058 myapp.py:9(heavy)
3 0.000 0.000 0.000 {range}
602 10.106 10.106 0.017 {time.sleep}

Micro-Profiling:
=======================
When the slow function is found, it is sometimes necessary to do more profiling
work that tests just a part of the program. This is done by manually instrumenting a
part of the code in a speed test.
For instance, the cProfile module can be used from a decorator:
 import tempfile, os, cProfile, pstats
 def profile(column='time', list=5):
	 def _profile(function):
		def __profile(*args, **kw):
			s = tempfile.mktemp()
			profiler = cProfile.Profile
			profiler.runcall(function, *args, **kw)
			profiler.dump_stats(s)
			p = pstats.Stats(s)
			p.sort_stats(column).print_stats(list)
		return __profile
 	return _profile

 from myapp import main
 @profile('time', 6)
    def main_profiled():


Profiling Memory Usage:
=======================
Another problems is memory consumption. If a program that runs starts to eat so
much memory that the system swaps, there is probably a place in your application
where too many objects are created. This is often easy to detect through classical
profiling because consuming enough memory to make a system swap involves a lot
of CPU work that can be detected.

So objects that remain in memory are:
Global objects
Objects that are still referenced in some way

Profiling Memory:
=======================
Knowing how many objects are controlled by the garbage collector and their real
size is a bit tricky.



Profiling Network Usage:
=======================
As I said earlier, an application that communicates with third-party programs such
as a database or an LDAP server can be slowed down when those applications are
slow. This can be tracked with a regular code profiling method on the application
side. But if the third-party software works fine on its own, the culprit is probably
the network.
Unit testing in python 
===============================
The unittest module provides a rich set of tools for constructing and running tests. This section demonstrates 
that a small subset of the tools suffice to meet the needs of most users.The unittest.Testcase is inherited 
and the new class is written with the test case

Here is a short script to tests which is run as a suite and then suite is been run 

class Tests(unittest.TestCase):
    def setUp(self):
        print('Gonna Run the test cases')
	def tearDown(self):
        print('Over test cases')
    def runTest3(self):
        print('test1')
        self.assertEqual({'acad'},lcs('academy', 'abracadabra'))
    def runTest2(self):
        print('test2')
        self.assertEqual({'abc','aba'},lcs('ababc', 'abcdaba'))
	
	
    @unittest.skipUnless(sys.platform.startswith("win"), "requires Windows")
    def test_windows_support(self):
        # windows specific testing code
        pass

 



def suite():
    test_suite = unittest.TestSuite()
    test_suite.addTest(Tests('runTest2'))
    test_suite.addTest(Tests('runTest3'))
	test_suite.addTest(Tests('test_windows_support'))
    return test_suite



if __name__ == "__main__":

   unittest.TextTestRunner(verbosity=2).run(suite())
The output of the above is 
Gonna Run the test cases
runTest2 (__main__.Tests) ... ok
test2
runTest3 (__main__.Tests) ... ok
test_windows_support (__main__.Tests) ... skipped 'requires Windows'

----------------------------------------------------------------------
Ran 3 tests in 0.000s

OK (skipped=1)
Gonna Run the test cases
test2
Over test cases
Gonna Run the test cases
test1
Over test cases

xml handling :
=================================
file handling:
=================================
f = open('workfile', 'w')
f.read()
>> 'This is the entire file.\n'
 f.readline()
'This is the first line of the file.\n'
 f.readline()
'Second line of the file\n'
 f.readline()
''
 f = open('workfile', 'r+')
 f.write('0123456789abcdef')
 f.seek(5)     # Go to the 6th byte in the file
 f.read(1)
'5'
 f.seek(-3, 2) # Go to the 3rd byte before the end
 f.read(1)
'd'

The Singleton
==================================
Possibly the simplest design pattern is the singleton, which is a way to provide one and only one object of a particular type. To accomplish this, 
you must take control
 of object creation out of the hands of the programmer. One convenient way to do this is to delegate to a single instance of a private nested inner class:

# Singleton/SingletonPattern.py

class OnlyOne:
    class __OnlyOne:
         def __init__(self, arg):
            self.val = arg
        def __str__(self):
            return repr(self) + self.val
    instance = None
    def __init__(self, arg):
        if not OnlyOne.instance:
            OnlyOne.instance = OnlyOne.__OnlyOne(arg)
        else:
            OnlyOne.instance.val = arg
    def __getattr__(self, name):
        return getattr(self.instance, name)

x = OnlyOne('sausage')
print(x)
y = OnlyOne('eggs')
print(y)
z = OnlyOne('spam')
print(z)
print(x)
print(y)
print(`x`)
print(`y`)
print(`z`)
output = '''
<__main__.__OnlyOne instance at 0076B7AC>sausage
<__main__.__OnlyOne instance at 0076B7AC>eggs
<__main__.__OnlyOne instance at 0076B7AC>spam
<__main__.__OnlyOne instance at 0076B7AC>spam




things to practice:
+++++++++++++++++++++++++++++++++

1. Tuple of list 
==================
first_lst = [('-2.50', 0.49, 0.52), ('-2.00', 0.52, 0.50)]
this is not the correct way and will throw a generator error
>> print first_lst 
>> <generator object <genexpr> at 0x02261288>

convert the tuple into all floats 
first_lst=[ tuple(float(y) for y in x) for x in first_lst ]

Now the other way (x for x in y) where y is a tuple is all allowed over here 


2. lamdas list and fetching Values 
====================================
from functools import partial
lambdas = [(lambda : i for i in range(3)]
 lambdas[0]()
2
 lambdas[1]()
2

Sol: 

The important thing to be understood here is, the functions are created during the evaluation of the list 
comprehension, but the value of i will be evaluated only during the execution of the functions.

So, in your case, you have created three functions and all of them refer
 i. When those functions are
 invoked at runtime, i will have the value ..
 
 2, because that was the last value bound to i in the last
 iteration of the for loop.
 
  lambdas = [lambda i=i: i for i in range(3)]
  Use a parameter with default value to bind the current value of i to a local variable. When the lambda
  gets called without an argument, the local variable i is assigned the default value
  
  
  
3. use of Arg and karg best expalined :
==============================================
Soln: 
class Foo():
    def __init__(self, data = False, *args, **kwargs):
        print kwargs, args, data

Foo()
Foo(data = True)
Foo(data = True, data1 = "Welcome")
Foo(True, 1, data1 = "Welcome")
# Foo(True, data1 = "Welcome", 1) # This will throw an error
Output

{} () False
{} () True
{'data1': 'Welcome'} () True
{'data1': 'Welcome'} (1,) True
In this example,

We don't pass a value to data, so default value (False) is assumed
We explicitly pass a value to data, so that value is taken
We explicitly pass a value to data and another keyword argument.
We explicitly pass a value to data, positional parameter and a keyword argument.
This will throw an error because no positional parameter should occur after a keyword argument. Order is very important

4. generator(tuple) vs list comprehension
==================================================
[myClass().Function(things) for things in biggerThing]
Function is a method, and it builds a list. The method itself doesn't return anything, but lists get manipulated within.

Now when I change it to a generator ,

(myClass().Function(things) for things in biggerThing)

Wont work 
Soln:

1). map(myClass().Function, biggerThing) 
map(function, iterable, ...)

Apply function to every item of iterable and return a list of the results. If additional iterable arguments are passed, function must take that many arguments and is applied to the items from all iterables in parallel. If one iterable is shorter than another it is assumed to be extended with None items. If function is None, the identity function is assumed; if there are multiple arguments, map() returns a list consisting of tuples containing the corresponding items from all iterables (a kind of transpose operation). The iterable arguments may be a sequence or any iterable object; the result is always a list.


2). 
Generators are evaluated on the fly, as they are consumed. So if you never iterate over a generator, its elements are never evaluated.
So, if you did:

for _ in (myClass().Function(things) for things in biggerThing):
    pass


5.create a dictionary with list comprehension
============================================================

Soln:

d = {key: value for (key, value) in iterable}
Of course, you can use the iterable in any way you want (tuples and lists literals, generator comprehensions, list
 comprehensions, 
generator functions, functional composition    feel creative) as long as each element is an iterable itself of 
two elements:

d = {value: foo(value) for value in sequence if bar(value)}

def key_value_gen(k):
   yield chr(k+65)
   yield chr((k+13)%26+65)
d = dict(map(key_value_gen, range(26)))

6.sort a list of dictionaries by values of the dictionary in Python?
===================================================================
[{'name':'Homer', 'age':39}, {'name':'Bart', 'age':10}]
sorted by name, should become

[{'name':'Bart', 'age':10}, {'name':'Homer', 'age':39}]


sol:
newlist = sorted(list_to_be_sorted, key=lambda k: k['name']) 

7.  list to make a dictionary
===================================
a = [1,2,3,4] and I want d = {1:0, 2:0, 3:0, 4:0}

sol:
zip can be usedinsid a dict to make a key value pair
a = [1,2,3,4] and I want d = {1:0, 2:0, 3:0, 4:0}


dict(zip(x ,[0 for x in range(0,len(x))]))


8. Merging two Dictionary :
==================================================================================
def merge_two_dicts(x, y):
    '''Given two dicts, merge them into a new dict as a shallow copy.'''
    z = x.copy()
    z.update(y)
    return z
x = {'a': 1, 'b': 2}
y = {'b': 3, 'c': 4}
y will come second and its values will replace x's values, thus 'b' will point to 3 in our final result.

9.Dict of Dicts :
==============================================================================================================
class AutoVivification(dict):
    """Implementation of perl's autovivification feature."""
    def __getitem__(self, item):
        try:
            return dict.__getitem__(self, item)
        except KeyError:
            value = self[item] = type(self)()
            return value
Testing:

a = AutoVivification()

a[1][2][3] = 4
a[1][3][3] = 5
a[1][2]['test'] = 6

print a
Output:

{1: {2: {'test': 6, 3: 4}, 3: {3: 5}}}

10. Best way to check a key in a dict:
==============================================================================================================
if 'key1' in dict:
  print "blah"
else:
  print "boo"
  
  the other methods will check for trhe whole dict 
   key = i % 10
    d[key] = d.get(key, 0) + 1
<<<<<<< HEAD
	
11. counting keyword in a strring 
================================================================
def name_freq(review_str,Names_list ):

    total = 0
    start = 0
    for keyword in Names_list:
       # print(keyword)
        a= re.search(keyword,review_str,re.IGNORECASE)
        if a is not None:
            matches = re.compile(keyword,re.IGNORECASE)
            while True:
                mo = matches.search(review_str,start)
              #  print(mo)
                if mo is None:
                    break
                start = 1 + mo.start()
                total = total +1
            print(a.group(),total)
        else :
            print('No matches found for '+ keyword)

    
11. Select the string split 
==============================================================================================================
 line = 'asdf fjdk; afed, fjek,asdf,      foo'
 import re
 re.split(r'[;,\s]\s*', line)
['asdf', 'fjdk', 'afed', 'fjek', 'asdf', 'foo']


>> fields = re.split(r'(;|,|\s)\s*', line)
 fields
['asdf', ' ', 'fjdk', ';', 'afed', ',', 'fjek', ',', 'asdf', ',', 'foo']
>>>>>>> b54797adae915e70034bfb835fe66a5c02bf182f

12. Map:
==============================================================================================
map executes the function given as the first argument on all the elements of the iterable given as the second argument.
 If the function given takes in more than 1 arguments, then many iterables are given.  #Follow the link to know more similar functions
For eg:

>>>a='ayush'
>>>map(ord,a)
....  [97, 121, 117, 115, 104]
>>> print map(lambda x, y: x*y**2, [1, 2, 3], [2, 4, 1])
....  [4, 32, 3]
<<<<<<< HEAD
12. Tuples of Lists:
==================================================================================================
actual_clients_emails, actual_client_names = zip(
    *[(partner.email, partner.name) for partner in op_client])

13 . generator Comprehensions
==============================================================================================
If so, a generator expression is like a list comprehension, but instead of finding all the items you're interested and packing them into list, it waits, and yields each item out of the expression, one by one.

>>> my_list = [1, 3, 5, 9, 2, 6]
>>> filtered_list = [item for item in my_list if item > 3]
>>> print filtered_list
[5, 9, 6]
>>> len(filtered_list)
3
>>> # compare to generator expression
... 
>>> filtered_gen = (item for item in my_list if item > 3)
>>> print filtered_gen  # notice it's a generator object
<generator object at 0xb7d5e02c>
>>> len(filtered_gen) # So technically, it has no length
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
TypeError: object of type 'generator' has no len()
>>> # We extract each item out individually. We'll do it manually first.
... 
>>> filtered_gen.next()
5
>>> filtered_gen.next()
9
>>> filtered_gen.next()
6
>>> filtered_gen.next() # Should be all out of items and give an error
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
StopIteration
>>> # Yup, the generator is spent. No values for you!
... 
>>> # Let's prove it gives the same results as our list comprehension
... 
>>> filtered_gen = (item for item in my_list if item > 3)
>>> gen_to_list = list(filtered_gen)
>>> print gen_to_list
[5, 9, 6]
>>> filtered_list == gen_to_list
True
>>>>>>> origin/master
12. sorting by the first item in each sub-list:
 ==========================================================
>>> def getKey(item):
...     return item[0]
>>> l = [[2, 3], [6, 7], [3, 34], [24, 64], [1, 43]]
>>> sorted(l, key=getKey)
[[1, 43], [2, 3], [3, 34], [6, 7], [24, 64]]

Lsts do it in descending style
sorted(l, reverse=True)

Sorting a List (or Tuple) of Custom Python Objects:
=========     =================    =================
class Custom(object):
    def __init__(self, name, number):
        self.name = name
        self.number = number
#*** This repr is exclusively written to how th e out put of the class in our specific format 		
	def __repr__(self):
       return '{}: {} {}'.format(self.__class__.__name__, self.name, self.number)

Lets enter our List 
customlist = [ Custom('object', 99), Custom('michael', 1),Custom('theodore the great', 59),Custom('life', 42)]

def getKey(custom):
    return custom.number

>>> sorted(customlist, key=getKey)
[Custom: michael 1, Custom: life 42,
 Custom: theodore the great 59, Custom: object 99]	
 
 Now again lets generalise the sorter by removing the key just definee the __cmp__ function 
 def __cmp__(self, other):
        if hasattr(other, 'number'):
            return self.number.__cmp__(other.number)
			
>>> sorted(customlist, key=getKey)
[Custom: michael 1, Custom: life 42,x Custom: theodore the great 59, Custom: object 99]	

If the comparision is heterogenos then we have to use the get the getkey in all classea and we should compare 
the get key 
def __cmp__(self, other):
        if hasattr(other, 'getKey'):
            return self.getKey().__cmp__(other.getKey())
 
def getKey(self):
    return self.age
----------------------------------------------------------------------------------------


13. The Difference Between xrange and range in Python
==========================================================
Before we get started, let's talk about what makes xrange and range different.

For the most part, xrange and range are the exact same in terms of functionality. They both provide a way to generate a 
list of integers for you to use, however you please. The only difference is that range returns a Python list object and
 xrange returns an xrange object.
What does that mean? Good question! It means that xrange doesn't actually generate a static list at run-time like range does.
 It creates the values as you need them with a s     pecial technique called yielding. This technique is used with a type of object
 known as generators. If you want to read more in depth about generators and the yield keyword

 Python's xrange and range with Odd Numbers

Say we wanted only odd numbers. Here's what we could do:
>>> for i in xrange(1, 10, 2):
...     print(i)
...
1
3
5
7
9
We told Python that we would like the first element to be one, the last element to be one less than ten, 
and that we'd like each element to go up by two. Really simple stuff.

Python's xrange and range with Negative Numbers

Alright, so what about if we want a negative list?Simple. Here's an example:
>>> for i in xrange(-1, -10, -1):
...     print(i)
...
-1
-2
-3
-4
-5
-6
-7
-8
-9


Using various python modules convert the list a to generate the output ‘one, two, three’

a = ['one', 'two', 'three']
Ans:   ", ".join(a)

What would the following code yield?
word = 'abcdefghij'
print word[:3] + word[3:]

Ans: ‘abcdefghij’ will be printed.
This is called string slicing. Since here the indices of the two slices are colliding, the string slices 
are ‘abc’ and ‘defghij’. The ‘+’ operator on strings concatenates them. Thus, the two slices formed are concatenated
to give the answer ‘abcdefghij’.

 exchanging numbers is as easy as:
a,b = b,a

Iterate over a list of words and use a dictionary to keep track of the frequency(count) of each word. for example

{‘one’:2, ‘two’:2, ‘three’:2}

Ans:

def dic(words):
  a = {}
  for i in words:
    try:
      a[i] += 1
    except KeyError: ## the famous pythonic way:
      a[i] = 1       ## Halt and catch fire
  return a
  
  
Print the sum of digits of numbers starting from 1 to 100 (inclusive of both)

Ans:

print sum(range(1,101))


one more example of even numbers between 100 and 120:
>>> for i in xrange(100, 120, 2):
...     print(i)
...
100
102
104
106
108
110

